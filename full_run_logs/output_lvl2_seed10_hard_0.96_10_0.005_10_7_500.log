==> Playing in 11_vs_11_hard_stochastic.
==>Level 2
==>OTs in this level are dict_keys(['charge_goal', 'just_shoot', 'maintain_ball_possession', 'defend_'])
==>Currently learning attack to choose from above OTs.
==>using device cuda
==>critic has 7 layers and 500 hidden units.
goal_identified
goal_identified
=== ep: 0, time 27.043179750442505, eps 0.9, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
goal_identified
=== ep: 1, time 25.5335373878479, eps 0.8561552526261419, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
goal_identified
=== ep: 2, time 25.38989543914795, eps 0.8144488388143276, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
goal_identified
=== ep: 3, time 25.73197913169861, eps 0.774776470806127, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
goal_identified
=== ep: 4, time 27.231654405593872, eps 0.7370389470171057, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
=== ep: 5, time 25.846898317337036, eps 0.701141903981193, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
goal_identified
goal_identified
goal_identified
=== ep: 6, time 25.908387422561646, eps 0.6669955803928644, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
goal_identified
=== ep: 7, time 26.106475114822388, eps 0.6345145926571234, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
goal_identified
goal_identified
goal_identified
=== ep: 8, time 25.988529920578003, eps 0.6036177213860398, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
goal_identified
=== ep: 9, time 30.34828782081604, eps 0.5742277083079742, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 5
goal_identified
=== ep: 10, time 26.19188666343689, eps 0.5462710630816575, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 0
=== ep: 11, time 27.793800830841064, eps 0.5196778795320575, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 4
goal_identified
goal_identified
=== ep: 12, time 26.238980531692505, eps 0.49438166084852986, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1
=== ep: 13, time 26.353426456451416, eps 0.47031915330815344, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3
goal_identified
=== ep: 14, time 26.506611108779907, eps 0.4474301881084772, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 7
goal_identified
=== ep: 15, time 26.06463313102722, eps 0.42565753091417224, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 9
=== ep: 16, time 26.39708709716797, eps 0.4049467387413822, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 10
goal_identified
=== ep: 17, time 26.074666023254395, eps 0.3852460238219053, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 11
=== ep: 18, time 26.48793315887451, eps 0.3665061241067986, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 13
goal_identified
=== ep: 19, time 29.18135380744934, eps 0.3486801800855966, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 15
goal_identified
=== ep: 20, time 26.20267152786255, eps 0.3317236176131267, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
/home/ksridhar/GRF/scripts/policies.py:456: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 11 > 10.0 and we are deleting ep 16
=== ep: 21, time 26.07371687889099, eps 0.31559403645092865, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 21
goal_identified
goal_identified
=== ep: 22, time 26.33250880241394, eps 0.3002511042445735, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 17
=== ep: 23, time 26.01314640045166, eps 0.2856564556717689, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 23
=== ep: 24, time 26.402289152145386, eps 0.27177359650906974, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 18
goal_identified
=== ep: 25, time 26.300095558166504, eps 0.2585678123773109, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 19
goal_identified
=== ep: 26, time 26.35857129096985, eps 0.24600608193757734, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 20
goal_identified
=== ep: 27, time 26.08130121231079, eps 0.23405699432065646, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 24
goal_identified
=== ep: 28, time 26.232115268707275, eps 0.22269067058350425, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2
=== ep: 29, time 31.060702085494995, eps 0.2118786889963241, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 29
goal_identified
=== ep: 30, time 26.435487747192383, eps 0.2015940139734384, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 30
goal_identified
=== ep: 31, time 26.371656894683838, eps 0.191810928470242, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 31
goal_identified
goal_identified
=== ep: 32, time 26.806746006011963, eps 0.1825049696771952, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 12
goal_identified
=== ep: 33, time 26.42314839363098, eps 0.17365286785005798, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 33
goal_identified
=== ep: 34, time 26.27957248687744, eps 0.16523248812340846, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 34
goal_identified
=== ep: 35, time 25.833192110061646, eps 0.15722277516195018, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 35
=== ep: 36, time 26.250190258026123, eps 0.1496037005112063, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 36
goal_identified
goal_identified
=== ep: 37, time 25.715262413024902, eps 0.14235621251595124, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 37
goal_identified
goal_identified
=== ep: 38, time 26.277940034866333, eps 0.13546218868114893, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 14
goal_identified
=== ep: 39, time 30.069820642471313, eps 0.1289043903562757, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 22
goal_identified
=== ep: 40, time 26.68177628517151, eps 0.12266641962971482, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 40
goal_identified
=== ep: 41, time 26.517180919647217, eps 0.116732678325436, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 41
goal_identified
goal_identified
=== ep: 42, time 25.86041235923767, eps 0.11108832899943073, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 25
goal_identified
=== ep: 43, time 26.431273460388184, eps 0.10571925783837377, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 43
goal_identified
=== ep: 44, time 26.443148612976074, eps 0.10061203936773815, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 26
goal_identified
=== ep: 45, time 26.687668085098267, eps 0.09575390288111604, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 45
goal_identified
goal_identified
=== ep: 46, time 26.22799038887024, eps 0.09113270050680057, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 27
goal_identified
=== ep: 47, time 26.603731632232666, eps 0.08673687683177911, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 47
goal_identified
goal_identified
=== ep: 48, time 26.14878535270691, eps 0.08255544000718185, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 28
goal_identified
goal_identified
=== ep: 49, time 30.279609441757202, eps 0.07857793426293408, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 32
goal_identified
=== ep: 50, time 26.46188235282898, eps 0.07479441376288502, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 39
goal_identified
goal_identified
=== ep: 51, time 26.31022882461548, eps 0.0711954177350367, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 44
goal_identified
goal_identified
=== ep: 52, time 26.411086559295654, eps 0.06777194681468615, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 50
goal_identified
=== ep: 53, time 26.161712169647217, eps 0.06451544054132621, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 53
goal_identified
goal_identified
=== ep: 54, time 26.612362146377563, eps 0.06141775595303503, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 6
=== ep: 55, time 26.346927642822266, eps 0.05847114722483011, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 55
goal_identified
=== ep: 56, time 26.611478805541992, eps 0.05566824630007096, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 56
=== ep: 57, time 26.659137964248657, eps 0.05300204446647978, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 57
goal_identified
goal_identified
=== ep: 58, time 26.698140144348145, eps 0.050465874830710106, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 58
goal_identified
=== ep: 59, time 30.97439432144165, eps 0.04805339564764071, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 59
goal_identified
goal_identified
goal_identified
=== ep: 60, time 26.301692485809326, eps 0.045758574462709686, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 8
goal_identified
=== ep: 61, time 26.240739345550537, eps 0.043575673027635695, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 61
goal_identified
goal_identified
=== ep: 62, time 26.397641897201538, eps 0.04149923295180846, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 62
=== ep: 63, time 26.09489107131958, eps 0.03952406205346913, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 63
goal_identified
=== ep: 64, time 26.519814491271973, eps 0.03764522137655123, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 64
goal_identified
=== ep: 65, time 26.820613622665405, eps 0.03585801284071809, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 65
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 66, time 26.435871124267578, eps 0.034157967493714775, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 38
=== ep: 67, time 26.405022144317627, eps 0.03254083433665968, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 67
goal_identified
=== ep: 68, time 26.693400621414185, eps 0.031002569694333147, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 68
goal_identified
goal_identified
goal_identified
=== ep: 69, time 32.86790442466736, eps 0.02953932710388308, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 42
goal_identified
=== ep: 70, time 26.50365114212036, eps 0.028147447696664333, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 70
goal_identified
=== ep: 71, time 26.62233281135559, eps 0.026823451049161253, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 71
goal_identified
goal_identified
goal_identified
=== ep: 72, time 26.374378204345703, eps 0.025564026480116013, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 46
goal_identified
=== ep: 73, time 26.482818365097046, eps 0.02436602477210106, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 73
goal_identified
=== ep: 74, time 26.378185749053955, eps 0.02322645029683511, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 74
goal_identified
goal_identified
=== ep: 75, time 26.485783100128174, eps 0.02214245352455219, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 75
goal_identified
goal_identified
goal_identified
=== ep: 76, time 26.63937735557556, eps 0.02111132389869288, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 48
goal_identified
=== ep: 77, time 26.312915802001953, eps 0.020130483058101077, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 77
goal_identified
=== ep: 78, time 26.598976135253906, eps 0.019197478389778148, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 78
goal_identified
=== ep: 79, time 29.970679998397827, eps 0.018309976896072843, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 79
=== ep: 80, time 26.187392950057983, eps 0.017465759360972027, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 80
=== ep: 81, time 26.5776047706604, eps 0.01666271480090467, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 81
=== ep: 82, time 26.488260507583618, eps 0.015898835186183367, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 82
goal_identified
goal_identified
=== ep: 83, time 26.580763339996338, eps 0.015172210419884185, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 83
=== ep: 84, time 26.155224084854126, eps 0.014481023561609456, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 84
goal_identified
goal_identified
=== ep: 85, time 26.58145761489868, eps 0.01382354628419033, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 85
goal_identified
goal_identified
goal_identified
=== ep: 86, time 26.510847568511963, eps 0.013198134551968641, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 49
=== ep: 87, time 26.892449617385864, eps 0.012603224509851407, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 87
goal_identified
=== ep: 88, time 26.46056866645813, eps 0.012037328572858524, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 88
goal_identified
goal_identified
=== ep: 89, time 33.04541873931885, eps 0.011499031706385502, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 89
goal_identified
goal_identified
=== ep: 90, time 26.453078746795654, eps 0.010986987887879832, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 90
goal_identified
=== ep: 91, time 26.38346219062805, eps 0.010499916741083536, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 91
goal_identified
goal_identified
=== ep: 92, time 26.454941511154175, eps 0.010036600334425595, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 92
goal_identified
goal_identified
=== ep: 93, time 26.648172855377197, eps 0.00959588013555861, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 51
goal_identified
=== ep: 94, time 26.602142572402954, eps 0.009176654114424539, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 94
goal_identified
=== ep: 95, time 26.45392680168152, eps 0.00877787398760545, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 95
=== ep: 96, time 26.33911371231079, eps 0.008398542597069007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 96
goal_identified
goal_identified
goal_identified
=== ep: 97, time 26.447077989578247, eps 0.008037711416753971, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 52
goal_identified
goal_identified
=== ep: 98, time 26.31865930557251, eps 0.00769447818076098, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 98
goal_identified
goal_identified
=== ep: 99, time 33.43596792221069, eps 0.007367984627217855, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 99
goal_identified
=== ep: 100, time 26.509506940841675, eps 0.007057414352177835, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 100
goal_identified
=== ep: 101, time 26.615201473236084, eps 0.006761990768184489, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 101
goal_identified
goal_identified
=== ep: 102, time 26.859761238098145, eps 0.006480975162398559, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 102
=== ep: 103, time 26.766958475112915, eps 0.006213664849431085, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 103
goal_identified
goal_identified
goal_identified
=== ep: 104, time 26.419947385787964, eps 0.005959391414263934, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 54
goal_identified
goal_identified
goal_identified
=== ep: 105, time 26.77058458328247, eps 0.005717519040864065, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 72
goal_identified
goal_identified
goal_identified
=== ep: 106, time 26.742608785629272, eps 0.005487442922312285, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 76
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 107, time 26.27878427505493, eps 0.005268587748470919, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 86
=== ep: 108, time 26.181217670440674, eps 0.005060406267408787, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 108
goal_identified
=== ep: 109, time 33.76414155960083, eps 0.004862377916986354, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 109
goal_identified
=== ep: 110, time 26.210264921188354, eps 0.004674007523179196, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 110
goal_identified
=== ep: 111, time 26.383843421936035, eps 0.004494824061885041, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 111
goal_identified
goal_identified
=== ep: 112, time 26.66310691833496, eps 0.0043243794811181555, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 93
goal_identified
goal_identified
=== ep: 113, time 26.814380645751953, eps 0.0041622475806460035, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 104
goal_identified
=== ep: 114, time 26.477821111679077, eps 0.0040080229462666735, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 114
goal_identified
goal_identified
goal_identified
=== ep: 115, time 26.944465398788452, eps 0.0038613199360621906, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 105
goal_identified
=== ep: 116, time 27.04965591430664, eps 0.003721771716092858, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 116
goal_identified
=== ep: 117, time 27.071460962295532, eps 0.0035890293431213305, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 117
goal_identified
=== ep: 118, time 26.826026678085327, eps 0.0034627608920727634, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 118
=== ep: 119, time 32.50259184837341, eps 0.00334265062604924, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 119
=== ep: 120, time 26.572763919830322, eps 0.0032283982068230565, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 120
=== ep: 121, time 26.565792083740234, eps 0.0031197179438347193, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 121
goal_identified
goal_identified
=== ep: 122, time 26.643798351287842, eps 0.0030163380798177374, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 122
goal_identified
=== ep: 123, time 26.530038833618164, eps 0.0029180001112638996, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 123
goal_identified
=== ep: 124, time 26.61385989189148, eps 0.002824458142029865, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 124
goal_identified
goal_identified
=== ep: 125, time 26.56140398979187, eps 0.0027354782684687108, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 106
goal_identified
=== ep: 126, time 26.67762589454651, eps 0.0026508379945489875, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 126
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 127, time 26.680047750473022, eps 0.0025703256754987464, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 112
goal_identified
goal_identified
=== ep: 128, time 26.46071743965149, eps 0.0024937399885833667, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 113
goal_identified
=== ep: 129, time 33.06904339790344, eps 0.0024208894296938593, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 129
goal_identified
goal_identified
=== ep: 130, time 27.04997968673706, eps 0.0023515918344868374, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 130
goal_identified
=== ep: 131, time 26.961133241653442, eps 0.002285673922878779, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 131
goal_identified
=== ep: 132, time 26.500771045684814, eps 0.0022229708657555565, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 132
goal_identified
goal_identified
goal_identified
=== ep: 133, time 26.732505321502686, eps 0.0021633258728137976, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 115
goal_identified
=== ep: 134, time 26.55512261390686, eps 0.0021065898005034594, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 134
=== ep: 135, time 26.933727264404297, eps 0.002052620779091266, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 135
=== ep: 136, time 26.897719860076904, eps 0.0020012838579124784, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 136
goal_identified
=== ep: 137, time 26.612363576889038, eps 0.0019524506679239415, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 137
goal_identified
goal_identified
=== ep: 138, time 26.679155588150024, eps 0.001905999100714611, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 138
goal_identified
goal_identified
=== ep: 139, time 32.407734394073486, eps 0.001861813003170924, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 139
goal_identified
goal_identified
goal_identified
=== ep: 140, time 27.113081216812134, eps 0.0018197818870335101, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 125
goal_identified
goal_identified
=== ep: 141, time 26.753262281417847, eps 0.0017798006526189953, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 141
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 142, time 27.17311668395996, eps 0.0017417693260160481, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 60
goal_identified
goal_identified
goal_identified
=== ep: 143, time 26.521300792694092, eps 0.0017055928090985275, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 143
goal_identified
=== ep: 144, time 26.690573930740356, eps 0.0016711806417306348, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 144
goal_identified
goal_identified
=== ep: 145, time 26.68081569671631, eps 0.0016384467755694515, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 145
goal_identified
goal_identified
goal_identified
=== ep: 146, time 26.860766649246216, eps 0.0016073093588992661, sum reward: 3, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 146
=== ep: 147, time 26.703935623168945, eps 0.0015776905319596466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 147
goal_identified
=== ep: 148, time 26.378787517547607, eps 0.0015495162322554856, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 148
=== ep: 149, time 33.565807819366455, eps 0.0015227160093621863, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 149
goal_identified
goal_identified
=== ep: 150, time 27.02009630203247, eps 0.0014972228487629025, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 150
=== ep: 151, time 27.378584623336792, eps 0.0014729730042773413, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 151
=== ep: 152, time 26.53236174583435, eps 0.001449905838663109, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 152
goal_identified
goal_identified
=== ep: 153, time 26.483003616333008, eps 0.00142796367199102, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 153
goal_identified
goal_identified
=== ep: 154, time 26.384294748306274, eps 0.0014070916374152305, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 154
goal_identified
goal_identified
goal_identified
=== ep: 155, time 26.804931163787842, eps 0.001387237543977543, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 155
goal_identified
goal_identified
=== ep: 156, time 26.583012342453003, eps 0.0013683517461028282, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 156
=== ep: 157, time 27.349895238876343, eps 0.0013503870194592265, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 157
goal_identified
goal_identified
=== ep: 158, time 26.72028160095215, eps 0.0013332984428727204, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 158
goal_identified
=== ep: 159, time 32.881813049316406, eps 0.001317043286000802, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 159
goal_identified
goal_identified
=== ep: 160, time 26.57341742515564, eps 0.0013015809024843582, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 160
goal_identified
=== ep: 161, time 26.611979961395264, eps 0.0012868726283106018, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 161
goal_identified
goal_identified
=== ep: 162, time 26.5775363445282, eps 0.0012728816851329014, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 162
goal_identified
goal_identified
=== ep: 163, time 26.88533043861389, eps 0.0012595730883057546, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 163
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 164, time 26.64077925682068, eps 0.001246913559404956, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 164
goal_identified
=== ep: 165, time 26.706509590148926, eps 0.0012348714430141991, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 165
goal_identified
=== ep: 166, time 26.62258243560791, eps 0.0012234166275700486, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 166
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 167, time 26.554694652557373, eps 0.001212520470067348, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 167
goal_identified
=== ep: 168, time 26.82998824119568, eps 0.0012021557244367845, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 168
goal_identified
=== ep: 169, time 34.32245326042175, eps 0.0011922964734155277, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 169
goal_identified
goal_identified
=== ep: 170, time 26.527068614959717, eps 0.001182918063740569, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 170
goal_identified
goal_identified
=== ep: 171, time 26.846961736679077, eps 0.0011739970445027263, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 171
goal_identified
goal_identified
=== ep: 172, time 26.80672264099121, eps 0.0011655111085071537, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 172
goal_identified
=== ep: 173, time 26.324292182922363, eps 0.001157439036493735, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 173
goal_identified
goal_identified
=== ep: 174, time 26.886934995651245, eps 0.0011497606440778825, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 174
=== ep: 175, time 26.870103120803833, eps 0.0011424567312790603, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 175
goal_identified
=== ep: 176, time 26.525399208068848, eps 0.0011355090345108335, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 176
=== ep: 177, time 26.77600598335266, eps 0.0011289001809123877, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 177
goal_identified
goal_identified
goal_identified
=== ep: 178, time 26.798154830932617, eps 0.0011226136449073282, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 69
goal_identified
goal_identified
goal_identified
=== ep: 179, time 34.183090925216675, eps 0.001116633706881133, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 97
goal_identified
=== ep: 180, time 26.779143571853638, eps 0.001110945413873925, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 180
=== ep: 181, time 26.94904065132141, eps 0.001105534542190287, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 181
goal_identified
goal_identified
=== ep: 182, time 28.76515793800354, eps 0.0011003875618326132, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 182
goal_identified
=== ep: 183, time 26.558451414108276, eps 0.0010954916026690664, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 183
goal_identified
=== ep: 184, time 26.89600419998169, eps 0.001090834422251547, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 184
=== ep: 185, time 26.81937074661255, eps 0.0010864043752031938, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 185
goal_identified
=== ep: 186, time 26.924659490585327, eps 0.0010821903840988777, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 186
goal_identified
=== ep: 187, time 27.196381092071533, eps 0.0010781819117658682, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 187
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 188, time 27.003782987594604, eps 0.0010743689349354123, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 127
=== ep: 189, time 35.04963207244873, eps 0.0010707419191793434, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 189
goal_identified
=== ep: 190, time 26.630210399627686, eps 0.0010672917950690429, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 190
goal_identified
=== ep: 191, time 26.71735382080078, eps 0.0010640099354971456, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 191
goal_identified
goal_identified
=== ep: 192, time 26.662672758102417, eps 0.0010608881341052777, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 192
goal_identified
=== ep: 193, time 26.75749182701111, eps 0.0010579185847638855, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 193
goal_identified
=== ep: 194, time 26.895756721496582, eps 0.0010550938620528466, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 194
goal_identified
goal_identified
=== ep: 195, time 26.77399253845215, eps 0.001052406902694051, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 195
goal_identified
goal_identified
=== ep: 196, time 26.798524379730225, eps 0.001049850987889527, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 196
goal_identified
goal_identified
=== ep: 197, time 26.78450036048889, eps 0.0010474197265209469, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 197
=== ep: 198, time 28.33354926109314, eps 0.0010451070391685015, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 198
goal_identified
goal_identified
=== ep: 199, time 37.115588665008545, eps 0.001042907142909185, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 199
goal_identified
=== ep: 200, time 26.871411323547363, eps 0.001040814536856474, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 200
goal_identified
goal_identified
=== ep: 201, time 26.565242528915405, eps 0.0010388239884052469, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 201
=== ep: 202, time 27.061848640441895, eps 0.0010369305201475454, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 202
=== ep: 203, time 26.94934892654419, eps 0.0010351293974264616, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 203
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 204, time 26.512549877166748, eps 0.00103341611649703, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 128
goal_identified
goal_identified
=== ep: 205, time 26.745834589004517, eps 0.0010317863932645186, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 205
goal_identified
goal_identified
=== ep: 206, time 26.874119758605957, eps 0.0010302361525719613, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 206
=== ep: 207, time 26.81110954284668, eps 0.0010287615180101426, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 207
goal_identified
=== ep: 208, time 26.658891201019287, eps 0.001027358802224555, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 208
goal_identified
=== ep: 209, time 38.709216356277466, eps 0.0010260244976950921, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 209
goal_identified
=== ep: 210, time 27.5595920085907, eps 0.0010247552679654227, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 210
goal_identified
goal_identified
goal_identified
=== ep: 211, time 26.863797187805176, eps 0.00102354793930011, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 133
goal_identified
=== ep: 212, time 27.39091396331787, eps 0.0010223994927486214, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 212
goal_identified
=== ep: 213, time 27.216165781021118, eps 0.001021307056596379, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 213
=== ep: 214, time 27.18272829055786, eps 0.0010202678991839778, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 214
=== ep: 215, time 27.026060581207275, eps 0.0010192794220766138, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 215
goal_identified
=== ep: 216, time 26.880763053894043, eps 0.0010183391535666436, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 216
goal_identified
goal_identified
=== ep: 217, time 26.79714584350586, eps 0.0010174447424930286, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 217
goal_identified
=== ep: 218, time 26.99344801902771, eps 0.0010165939523622068, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 218
=== ep: 219, time 36.113276958465576, eps 0.0010157846557556941, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 219
goal_identified
=== ep: 220, time 27.540935516357422, eps 0.001015014829010431, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 220
goal_identified
goal_identified
goal_identified
=== ep: 221, time 27.073764324188232, eps 0.0010142825471585687, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 221
goal_identified
goal_identified
goal_identified
=== ep: 222, time 26.96962833404541, eps 0.0010135859791140496, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 222
goal_identified
goal_identified
=== ep: 223, time 26.973641633987427, eps 0.0010129233830939361, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 223
goal_identified
=== ep: 224, time 27.303117036819458, eps 0.0010122931022630473, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 224
goal_identified
goal_identified
=== ep: 225, time 27.12795662879944, eps 0.001011693560591007, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 225
goal_identified
=== ep: 226, time 27.172089099884033, eps 0.0010111232589113477, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 226
goal_identified
goal_identified
goal_identified
=== ep: 227, time 27.28021264076233, eps 0.0010105807711728136, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 227
goal_identified
goal_identified
=== ep: 228, time 26.86385440826416, eps 0.0010100647408734893, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 228
goal_identified
=== ep: 229, time 33.688506841659546, eps 0.001009573877668838, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 229
goal_identified
goal_identified
=== ep: 230, time 26.91555666923523, eps 0.001009106954145169, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 230
=== ep: 231, time 27.07909083366394, eps 0.0010086628027504636, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 231
goal_identified
goal_identified
=== ep: 232, time 26.938213109970093, eps 0.0010082403128748867, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 232
=== ep: 233, time 27.053716897964478, eps 0.0010078384280736842, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 233
goal_identified
goal_identified
=== ep: 234, time 26.7157883644104, eps 0.001007456143425521, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 234
goal_identified
goal_identified
=== ep: 235, time 26.952131032943726, eps 0.001007092503019653, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 235
goal_identified
=== ep: 236, time 27.10217523574829, eps 0.001006746597565654, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 236
goal_identified
=== ep: 237, time 26.839131832122803, eps 0.001006417562119715, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 237
goal_identified
goal_identified
goal_identified
=== ep: 238, time 26.821155786514282, eps 0.0010061045739218342, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 238
goal_identified
=== ep: 239, time 36.241241455078125, eps 0.0010058068503384884, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 239
goal_identified
goal_identified
=== ep: 240, time 27.039249658584595, eps 0.001005523646905642, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 240
goal_identified
goal_identified
goal_identified
=== ep: 241, time 27.20380449295044, eps 0.001005254255467199, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 241
goal_identified
=== ep: 242, time 26.77791666984558, eps 0.0010049980024042435, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 242
goal_identified
=== ep: 243, time 27.090164184570312, eps 0.0010047542469506416, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 243
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 244, time 26.714725017547607, eps 0.0010045223795907931, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 140
goal_identified
goal_identified
goal_identified
=== ep: 245, time 26.979331493377686, eps 0.001004301820535524, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 245
goal_identified
goal_identified
=== ep: 246, time 26.895882844924927, eps 0.0010040920182723119, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 246
goal_identified
=== ep: 247, time 26.72457456588745, eps 0.0010038924481862177, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 247
goal_identified
goal_identified
=== ep: 248, time 26.61629009246826, eps 0.0010037026112480747, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 248
goal_identified
goal_identified
=== ep: 249, time 35.23264002799988, eps 0.0010035220327666559, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 249
goal_identified
goal_identified
goal_identified
=== ep: 250, time 26.63762927055359, eps 0.0010033502612016988, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 178
goal_identified
goal_identified
goal_identified
=== ep: 251, time 27.097452402114868, eps 0.001003186867034819, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 251
=== ep: 252, time 26.939409732818604, eps 0.001003031441695491, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 252
goal_identified
goal_identified
=== ep: 253, time 26.89583683013916, eps 0.0010028835965394094, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 253
goal_identified
=== ep: 254, time 27.249116897583008, eps 0.0010027429618766747, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 254
=== ep: 255, time 27.03512930870056, eps 0.0010026091860473767, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 255
goal_identified
=== ep: 256, time 27.27163290977478, eps 0.0010024819345422614, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 256
=== ep: 257, time 27.057896614074707, eps 0.0010023608891662839, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 257
=== ep: 258, time 27.35511565208435, eps 0.001002245747242954, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 258
=== ep: 259, time 33.70673751831055, eps 0.0010021362208574892, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 259
goal_identified
goal_identified
=== ep: 260, time 27.013044595718384, eps 0.001002032036136876, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 260
goal_identified
=== ep: 261, time 27.09173583984375, eps 0.0010019329325650452, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 261
goal_identified
goal_identified
goal_identified
=== ep: 262, time 27.304272651672363, eps 0.0010018386623314465, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 262
goal_identified
goal_identified
=== ep: 263, time 27.135369777679443, eps 0.0010017489897113931, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 263
=== ep: 264, time 26.862912893295288, eps 0.0010016636904766263, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 264
goal_identified
goal_identified
goal_identified
=== ep: 265, time 28.49786138534546, eps 0.0010015825513346283, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 265
goal_identified
=== ep: 266, time 27.097936630249023, eps 0.0010015053693952815, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 266
goal_identified
=== ep: 267, time 27.365708827972412, eps 0.0010014319516635345, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 267
goal_identified
goal_identified
=== ep: 268, time 26.939807176589966, eps 0.0010013621145568167, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 268
goal_identified
=== ep: 269, time 33.82571601867676, eps 0.0010012956834459848, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 269
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 270, time 27.006112813949585, eps 0.0010012324922186594, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 179
goal_identified
=== ep: 271, time 27.498347282409668, eps 0.001001172382863857, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 271
goal_identified
goal_identified
=== ep: 272, time 27.08045220375061, eps 0.0010011152050768812, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 272
goal_identified
=== ep: 273, time 27.338435888290405, eps 0.0010010608158834819, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 273
goal_identified
goal_identified
=== ep: 274, time 27.17918062210083, eps 0.0010010090792823456, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 274
goal_identified
goal_identified
goal_identified
=== ep: 275, time 27.23355507850647, eps 0.0010009598659050213, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 275
goal_identified
goal_identified
goal_identified
=== ep: 276, time 26.90172815322876, eps 0.0010009130526924313, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 211
=== ep: 277, time 27.448246479034424, eps 0.0010008685225871602, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 277
=== ep: 278, time 27.0793559551239, eps 0.0010008261642407504, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 278
=== ep: 279, time 36.17984104156494, eps 0.001000785871735272, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 279
goal_identified
goal_identified
=== ep: 280, time 26.83887815475464, eps 0.0010007475443184742, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 280
=== ep: 281, time 27.258497953414917, eps 0.001000711086151851, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 281
=== ep: 282, time 27.02732515335083, eps 0.0010006764060709957, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 282
=== ep: 283, time 27.147924423217773, eps 0.001000643417357642, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 283
=== ep: 284, time 27.287053108215332, eps 0.0010006120375228235, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 284
goal_identified
goal_identified
=== ep: 285, time 26.97482204437256, eps 0.0010005821881006083, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 285
goal_identified
=== ep: 286, time 26.768065214157104, eps 0.0010005537944518927, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 286
goal_identified
goal_identified
=== ep: 287, time 27.20400881767273, eps 0.0010005267855777657, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 287
goal_identified
goal_identified
=== ep: 288, time 26.990525722503662, eps 0.0010005010939419733, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 288
goal_identified
goal_identified
=== ep: 289, time 33.87051606178284, eps 0.001000476655302044, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 289
goal_identified
goal_identified
=== ep: 290, time 26.74750518798828, eps 0.0010004534085486486, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 290
goal_identified
=== ep: 291, time 27.123563528060913, eps 0.0010004312955527947, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 291
goal_identified
goal_identified
=== ep: 292, time 27.3671772480011, eps 0.0010004102610204745, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 292
goal_identified
goal_identified
=== ep: 293, time 27.618183374404907, eps 0.0010003902523544011, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 293
goal_identified
goal_identified
=== ep: 294, time 27.376928329467773, eps 0.0010003712195224871, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 294
=== ep: 295, time 27.267529487609863, eps 0.0010003531149327387, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 295
goal_identified
=== ep: 296, time 27.10202670097351, eps 0.0010003358933142518, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 296
goal_identified
=== ep: 297, time 27.075366497039795, eps 0.0010003195116040093, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 297
goal_identified
goal_identified
goal_identified
=== ep: 298, time 27.17456865310669, eps 0.0010003039288392032, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 298
goal_identified
goal_identified
=== ep: 299, time 34.818543910980225, eps 0.0010002891060548044, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 299
goal_identified
=== ep: 300, time 27.139961004257202, eps 0.0010002750061861312, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 300
goal_identified
goal_identified
=== ep: 301, time 26.95315384864807, eps 0.0010002615939761676, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 301
goal_identified
goal_identified
=== ep: 302, time 27.044790506362915, eps 0.001000248835887403, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 302
goal_identified
=== ep: 303, time 26.531421184539795, eps 0.0010002367000179694, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 303
goal_identified
goal_identified
=== ep: 304, time 27.301398754119873, eps 0.0010002251560218723, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 304
goal_identified
=== ep: 305, time 27.14168930053711, eps 0.0010002141750331084, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 305
goal_identified
goal_identified
=== ep: 306, time 27.488370180130005, eps 0.0010002037295934862, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 306
goal_identified
=== ep: 307, time 27.455272912979126, eps 0.0010001937935839656, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 307
=== ep: 308, time 27.317394256591797, eps 0.0010001843421593476, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 308
goal_identified
=== ep: 309, time 33.59710907936096, eps 0.0010001753516861473, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 309
=== ep: 310, time 27.121881246566772, eps 0.0010001667996834991, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 310
=== ep: 311, time 26.96718192100525, eps 0.001000158664766942, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 311
goal_identified
=== ep: 312, time 27.179258108139038, eps 0.0010001509265949466, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 312
=== ep: 313, time 26.99169373512268, eps 0.001000143565818053, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 313
goal_identified
goal_identified
=== ep: 314, time 27.064367055892944, eps 0.0010001365640304844, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 314
goal_identified
goal_identified
=== ep: 315, time 27.065537691116333, eps 0.0010001299037241253, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 315
=== ep: 316, time 27.466787815093994, eps 0.0010001235682447402, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 316
goal_identified
goal_identified
goal_identified
=== ep: 317, time 27.030571937561035, eps 0.0010001175417503308, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 317
=== ep: 318, time 26.830649852752686, eps 0.0010001118091715218, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 318
goal_identified
=== ep: 319, time 34.65336799621582, eps 0.0010001063561738807, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 319
goal_identified
goal_identified
=== ep: 320, time 27.45776891708374, eps 0.0010001011691220727, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 320
goal_identified
=== ep: 321, time 27.166280031204224, eps 0.0010000962350457665, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 321
goal_identified
goal_identified
goal_identified
=== ep: 322, time 27.21616268157959, eps 0.0010000915416072012, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 244
goal_identified
goal_identified
=== ep: 323, time 27.268822193145752, eps 0.0010000870770703358, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 323
goal_identified
goal_identified
=== ep: 324, time 27.28507113456726, eps 0.0010000828302715028, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 324
goal_identified
goal_identified
=== ep: 325, time 26.89830446243286, eps 0.0010000787905914928, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 325
goal_identified
=== ep: 326, time 27.34809970855713, eps 0.0010000749479290019, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 326
goal_identified
goal_identified
=== ep: 327, time 26.947869777679443, eps 0.001000071292675372, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 327
goal_identified
goal_identified
=== ep: 328, time 26.954966068267822, eps 0.001000067815690565, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 250
goal_identified
goal_identified
=== ep: 329, time 34.94302201271057, eps 0.0010000645082803084, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 329
goal_identified
=== ep: 330, time 27.682791471481323, eps 0.0010000613621743532, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 330
goal_identified
goal_identified
goal_identified
=== ep: 331, time 27.173478841781616, eps 0.0010000583695057963, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 276
goal_identified
goal_identified
=== ep: 332, time 27.21419358253479, eps 0.0010000555227914069, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 332
goal_identified
goal_identified
goal_identified
=== ep: 333, time 27.323951959609985, eps 0.0010000528149129166, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 322
=== ep: 334, time 27.34895944595337, eps 0.0010000502390992187, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 334
=== ep: 335, time 26.749691247940063, eps 0.0010000477889094373, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 335
goal_identified
=== ep: 336, time 27.691275119781494, eps 0.0010000454582168217, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 336
goal_identified
goal_identified
=== ep: 337, time 29.583887100219727, eps 0.001000043241193426, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 337
goal_identified
=== ep: 338, time 26.774269104003906, eps 0.0010000411322955373, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 338
goal_identified
goal_identified
=== ep: 339, time 32.15927004814148, eps 0.0010000391262498123, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 339
goal_identified
=== ep: 340, time 27.19768190383911, eps 0.001000037218040092, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 340
goal_identified
goal_identified
goal_identified
=== ep: 341, time 27.058003902435303, eps 0.0010000354028948577, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 328
goal_identified
=== ep: 342, time 27.164619207382202, eps 0.0010000336762753012, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 342
=== ep: 343, time 27.500110387802124, eps 0.001000032033863974, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 343
goal_identified
goal_identified
=== ep: 344, time 27.32964515686035, eps 0.0010000304715539925, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 344
goal_identified
goal_identified
goal_identified
=== ep: 345, time 27.27463459968567, eps 0.001000028985438768, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 345
goal_identified
goal_identified
goal_identified
=== ep: 346, time 27.12979221343994, eps 0.001000027571802238, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 346
goal_identified
goal_identified
goal_identified
=== ep: 347, time 27.457038164138794, eps 0.0010000262271095755, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 347
goal_identified
=== ep: 348, time 27.332191467285156, eps 0.0010000249479983478, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 348
goal_identified
=== ep: 349, time 34.20993685722351, eps 0.0010000237312701107, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 349
goal_identified
goal_identified
=== ep: 350, time 27.313772678375244, eps 0.00100002257388241, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 350
goal_identified
=== ep: 351, time 27.347299098968506, eps 0.0010000214729411737, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 351
=== ep: 352, time 27.03865933418274, eps 0.0010000204256934752, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 352
=== ep: 353, time 26.948240041732788, eps 0.0010000194295206493, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 353
=== ep: 354, time 27.13433337211609, eps 0.0010000184819317455, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 354
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 355, time 27.25350856781006, eps 0.001000017580557298, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 331
=== ep: 356, time 26.974753379821777, eps 0.001000016723143401, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 356
goal_identified
=== ep: 357, time 27.13083839416504, eps 0.0010000159075460732, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 357
goal_identified
goal_identified
=== ep: 358, time 27.038334846496582, eps 0.0010000151317258964, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 358
=== ep: 359, time 33.49336099624634, eps 0.0010000143937429161, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 359
goal_identified
=== ep: 360, time 27.36106038093567, eps 0.0010000136917517905, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 360
goal_identified
goal_identified
=== ep: 361, time 27.52182126045227, eps 0.001000013023997176, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 361
goal_identified
goal_identified
=== ep: 362, time 27.19441509246826, eps 0.0010000123888093385, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 362
=== ep: 363, time 27.27862572669983, eps 0.0010000117845999773, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 363
goal_identified
goal_identified
=== ep: 364, time 27.246334075927734, eps 0.0010000112098582543, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 364
goal_identified
=== ep: 365, time 26.64684295654297, eps 0.001000010663147016, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 365
goal_identified
goal_identified
goal_identified
=== ep: 366, time 27.00701928138733, eps 0.0010000101430991996, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 333
goal_identified
=== ep: 367, time 27.193530321121216, eps 0.0010000096484144142, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 367
goal_identified
=== ep: 368, time 27.368690490722656, eps 0.0010000091778556905, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 368
goal_identified
=== ep: 369, time 31.68994402885437, eps 0.0010000087302463867, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 369
goal_identified
goal_identified
=== ep: 370, time 27.31248164176941, eps 0.001000008304467246, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 370
=== ep: 371, time 27.07425308227539, eps 0.0010000078994535993, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 371
goal_identified
=== ep: 372, time 27.13446545600891, eps 0.0010000075141927012, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 372
goal_identified
=== ep: 373, time 27.585551977157593, eps 0.0010000071477211988, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 373
=== ep: 374, time 27.210224628448486, eps 0.0010000067991227223, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 374
goal_identified
goal_identified
goal_identified
=== ep: 375, time 27.184637784957886, eps 0.0010000064675255943, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 375
goal_identified
=== ep: 376, time 26.454058408737183, eps 0.001000006152100649, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 376
=== ep: 377, time 26.877683639526367, eps 0.0010000058520591598, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 377
goal_identified
goal_identified
=== ep: 378, time 27.447730779647827, eps 0.0010000055666508666, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 378
goal_identified
=== ep: 379, time 34.591363191604614, eps 0.0010000052951621003, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 379
goal_identified
goal_identified
=== ep: 380, time 27.443405389785767, eps 0.0010000050369139975, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 341
goal_identified
=== ep: 381, time 27.415008544921875, eps 0.001000004791260803, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 381
goal_identified
=== ep: 382, time 26.884111166000366, eps 0.0010000045575882562, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 382
goal_identified
=== ep: 383, time 27.174588918685913, eps 0.001000004335312054, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 383
goal_identified
goal_identified
=== ep: 384, time 27.182380437850952, eps 0.0010000041238763903, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 384
goal_identified
goal_identified
=== ep: 385, time 27.29725456237793, eps 0.0010000039227525655, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 385
=== ep: 386, time 27.417025327682495, eps 0.0010000037314376652, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 386
goal_identified
=== ep: 387, time 27.236311674118042, eps 0.001000003549453303, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 387
goal_identified
goal_identified
=== ep: 388, time 27.142959117889404, eps 0.0010000033763444226, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 388
goal_identified
=== ep: 389, time 34.77076196670532, eps 0.001000003211678162, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 389
goal_identified
goal_identified
=== ep: 390, time 27.300257205963135, eps 0.0010000030550427698, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 390
goal_identified
=== ep: 391, time 27.530969619750977, eps 0.0010000029060465757, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 391
=== ep: 392, time 27.62604784965515, eps 0.0010000027643170119, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 392
=== ep: 393, time 27.519257307052612, eps 0.0010000026294996803, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 393
goal_identified
=== ep: 394, time 27.410290002822876, eps 0.0010000025012574677, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 394
=== ep: 395, time 27.014918088912964, eps 0.0010000023792697014, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 395
goal_identified
goal_identified
=== ep: 396, time 27.028486490249634, eps 0.0010000022632313489, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 396
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 397, time 27.165085077285767, eps 0.0010000021528522535, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 355
=== ep: 398, time 27.419363975524902, eps 0.00100000204785641, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 398
=== ep: 399, time 32.388535022735596, eps 0.0010000019479812744, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 399
=== ep: 400, time 27.733913898468018, eps 0.0010000018529771066, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 400
goal_identified
=== ep: 401, time 27.806175470352173, eps 0.0010000017626063467, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 401
goal_identified
goal_identified
=== ep: 402, time 27.57001757621765, eps 0.0010000016766430208, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 402
goal_identified
goal_identified
=== ep: 403, time 27.265373945236206, eps 0.0010000015948721758, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 403
goal_identified
goal_identified
goal_identified
=== ep: 404, time 27.418938875198364, eps 0.001000001517089342, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 380
goal_identified
=== ep: 405, time 27.283950567245483, eps 0.0010000014431000217, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 405
goal_identified
=== ep: 406, time 27.366021394729614, eps 0.001000001372719203, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 406
goal_identified
=== ep: 407, time 27.036918878555298, eps 0.0010000013057708975, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 407
goal_identified
=== ep: 408, time 27.116878986358643, eps 0.0010000012420876994, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 408
goal_identified
goal_identified
=== ep: 409, time 33.77547907829285, eps 0.0010000011815103674, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 404
goal_identified
=== ep: 410, time 27.15226674079895, eps 0.001000001123887427, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 410
goal_identified
=== ep: 411, time 27.460054636001587, eps 0.0010000010690747903, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 411
=== ep: 412, time 27.297051191329956, eps 0.0010000010169353975, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 412
goal_identified
=== ep: 413, time 27.14337468147278, eps 0.0010000009673388729, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 413
goal_identified
goal_identified
=== ep: 414, time 27.15754508972168, eps 0.0010000009201611994, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 414
goal_identified
=== ep: 415, time 27.313581943511963, eps 0.0010000008752844081, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 415
=== ep: 416, time 27.297241926193237, eps 0.0010000008325962838, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 416
goal_identified
goal_identified
=== ep: 417, time 27.303626537322998, eps 0.001000000791990084, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 417
goal_identified
goal_identified
=== ep: 418, time 27.165882349014282, eps 0.0010000007533642718, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 418
goal_identified
goal_identified
goal_identified
=== ep: 419, time 33.64226222038269, eps 0.0010000007166222626, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 419
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 420, time 27.25646424293518, eps 0.0010000006816721825, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 409
=== ep: 421, time 27.78059148788452, eps 0.001000000648426638, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 421
goal_identified
=== ep: 422, time 27.430495262145996, eps 0.0010000006168024976, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 422
=== ep: 423, time 27.28895401954651, eps 0.0010000005867206849, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 423
=== ep: 424, time 27.301106452941895, eps 0.0010000005581059794, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 424
goal_identified
=== ep: 425, time 27.082396984100342, eps 0.0010000005308868295, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 425
goal_identified
goal_identified
=== ep: 426, time 27.081735372543335, eps 0.0010000005049951733, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 426
goal_identified
=== ep: 427, time 27.150384426116943, eps 0.001000000480366268, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 427
=== ep: 428, time 27.403948307037354, eps 0.0010000004569385287, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 428
=== ep: 429, time 35.78937363624573, eps 0.0010000004346533736, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 429
goal_identified
=== ep: 430, time 26.931474208831787, eps 0.0010000004134550786, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 430
=== ep: 431, time 27.331124305725098, eps 0.0010000003932906364, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 431
goal_identified
=== ep: 432, time 27.701761722564697, eps 0.0010000003741096257, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 432
=== ep: 433, time 26.88419246673584, eps 0.001000000355864084, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 120/120)
== current size of memory is eps 11 > 10.0 and we are deleting ep 433
goal_identified
=== ep: 434, time 27.357321977615356, eps 0.0010000003385083878, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 434
goal_identified
goal_identified
=== ep: 435, time 26.995530366897583, eps 0.001000000321999139, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 435
goal_identified
=== ep: 436, time 26.823949575424194, eps 0.0010000003062950555, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 436
=== ep: 437, time 27.108530282974243, eps 0.0010000002913568694, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 437
goal_identified
goal_identified
=== ep: 438, time 27.355769157409668, eps 0.0010000002771472273, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 438
=== ep: 439, time 34.60367012023926, eps 0.0010000002636305976, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 439
goal_identified
goal_identified
=== ep: 440, time 27.42124032974243, eps 0.0010000002507731815, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 440
=== ep: 441, time 26.57284688949585, eps 0.0010000002385428292, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 441
goal_identified
=== ep: 442, time 27.408775329589844, eps 0.0010000002269089582, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 442
goal_identified
=== ep: 443, time 27.43323040008545, eps 0.0010000002158424776, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 443
goal_identified
=== ep: 444, time 27.29159641265869, eps 0.0010000002053157158, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 444
=== ep: 445, time 27.396714210510254, eps 0.0010000001953023503, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 445
goal_identified
goal_identified
=== ep: 446, time 27.279664039611816, eps 0.001000000185777342, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 446
goal_identified
=== ep: 447, time 27.428478479385376, eps 0.0010000001767168742, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 447
goal_identified
goal_identified
=== ep: 448, time 27.4522123336792, eps 0.0010000001680982905, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 448
goal_identified
=== ep: 449, time 35.50092077255249, eps 0.0010000001599000403, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 449
goal_identified
=== ep: 450, time 27.637254238128662, eps 0.0010000001521016232, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 450
goal_identified
goal_identified
goal_identified
=== ep: 451, time 27.3438823223114, eps 0.0010000001446835395, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 451
goal_identified
goal_identified
goal_identified
=== ep: 452, time 27.186901569366455, eps 0.0010000001376272401, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 452
goal_identified
=== ep: 453, time 27.07095742225647, eps 0.0010000001309150804, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 453
goal_identified
=== ep: 454, time 27.15633535385132, eps 0.0010000001245302765, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 454
goal_identified
goal_identified
=== ep: 455, time 27.333168268203735, eps 0.0010000001184568633, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 455
goal_identified
goal_identified
=== ep: 456, time 27.60715889930725, eps 0.0010000001126796538, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 456
goal_identified
=== ep: 457, time 27.346487045288086, eps 0.0010000001071842023, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 457
goal_identified
goal_identified
goal_identified
=== ep: 458, time 27.12227463722229, eps 0.001000000101956767, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 458
goal_identified
=== ep: 459, time 33.90247416496277, eps 0.001000000096984277, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 459
goal_identified
goal_identified
=== ep: 460, time 27.053863525390625, eps 0.001000000092254298, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 460
goal_identified
goal_identified
=== ep: 461, time 27.30988311767578, eps 0.0010000000877550027, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 461
goal_identified
=== ep: 462, time 27.137467861175537, eps 0.0010000000834751407, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 462
goal_identified
goal_identified
=== ep: 463, time 27.556737899780273, eps 0.00100000007940401, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 463
goal_identified
=== ep: 464, time 26.968389749526978, eps 0.0010000000755314307, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 464
goal_identified
=== ep: 465, time 27.233991146087646, eps 0.0010000000718477194, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 465
goal_identified
goal_identified
=== ep: 466, time 26.965562343597412, eps 0.0010000000683436647, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 466
goal_identified
goal_identified
=== ep: 467, time 26.760697841644287, eps 0.001000000065010505, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 467
=== ep: 468, time 27.76436448097229, eps 0.0010000000618399052, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 468
goal_identified
goal_identified
goal_identified
=== ep: 469, time 35.4943106174469, eps 0.0010000000588239375, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 469
goal_identified
=== ep: 470, time 27.082528591156006, eps 0.0010000000559550603, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 470
goal_identified
=== ep: 471, time 27.567332983016968, eps 0.0010000000532260998, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 471
goal_identified
=== ep: 472, time 27.157546043395996, eps 0.0010000000506302322, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 472
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 473, time 27.21442985534668, eps 0.0010000000481609666, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 473
goal_identified
goal_identified
=== ep: 474, time 26.79039692878723, eps 0.0010000000458121286, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 474
goal_identified
goal_identified
=== ep: 475, time 27.003704071044922, eps 0.0010000000435778447, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 475
goal_identified
=== ep: 476, time 27.23488759994507, eps 0.001000000041452528, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 476
goal_identified
=== ep: 477, time 27.062835454940796, eps 0.0010000000394308644, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 477
goal_identified
goal_identified
goal_identified
=== ep: 478, time 27.346332550048828, eps 0.0010000000375077985, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 478
goal_identified
=== ep: 479, time 36.24350166320801, eps 0.0010000000356785216, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 479
=== ep: 480, time 27.274391412734985, eps 0.0010000000339384595, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 480
goal_identified
=== ep: 481, time 27.14218831062317, eps 0.0010000000322832614, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 481
=== ep: 482, time 26.869343757629395, eps 0.0010000000307087882, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 482
goal_identified
goal_identified
=== ep: 483, time 27.363300800323486, eps 0.001000000029211103, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 483
goal_identified
goal_identified
goal_identified
=== ep: 484, time 27.15262532234192, eps 0.0010000000277864607, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 484
goal_identified
goal_identified
=== ep: 485, time 27.85067057609558, eps 0.0010000000264312988, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 485
=== ep: 486, time 27.172547817230225, eps 0.0010000000251422292, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 486
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 487, time 27.438373565673828, eps 0.0010000000239160282, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 487
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 488, time 27.126279830932617, eps 0.00100000002274963, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 488
goal_identified
=== ep: 489, time 32.0608549118042, eps 0.0010000000216401172, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 489
=== ep: 490, time 27.049484968185425, eps 0.0010000000205847162, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 490
goal_identified
=== ep: 491, time 27.319430112838745, eps 0.0010000000195807877, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 491
=== ep: 492, time 27.459541082382202, eps 0.0010000000186258216, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 492
=== ep: 493, time 27.068334341049194, eps 0.0010000000177174295, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 493
goal_identified
=== ep: 494, time 27.334164142608643, eps 0.0010000000168533404, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 494
goal_identified
=== ep: 495, time 27.24991798400879, eps 0.0010000000160313932, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 495
goal_identified
=== ep: 496, time 27.310556411743164, eps 0.001000000015249533, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 496
=== ep: 497, time 27.456697940826416, eps 0.0010000000145058043, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 497
goal_identified
=== ep: 498, time 27.50804829597473, eps 0.001000000013798348, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 498
goal_identified
goal_identified
=== ep: 499, time 36.76723790168762, eps 0.0010000000131253947, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 499
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 500, time 27.09323263168335, eps 0.0010000000124852615, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 500
=== ep: 501, time 27.526397943496704, eps 0.0010000000118763482, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 501
=== ep: 502, time 27.0051109790802, eps 0.0010000000112971319, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 502
=== ep: 503, time 26.99948477745056, eps 0.0010000000107461642, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 503
=== ep: 504, time 27.296538591384888, eps 0.0010000000102220676, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 504
goal_identified
=== ep: 505, time 27.168435096740723, eps 0.0010000000097235315, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 505
goal_identified
goal_identified
goal_identified
=== ep: 506, time 27.13201928138733, eps 0.0010000000092493092, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 506
=== ep: 507, time 27.450028657913208, eps 0.0010000000087982152, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 507
goal_identified
goal_identified
goal_identified
=== ep: 508, time 27.310648918151855, eps 0.0010000000083691212, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 508
=== ep: 509, time 34.70524740219116, eps 0.0010000000079609542, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 509
goal_identified
goal_identified
goal_identified
=== ep: 510, time 26.892916440963745, eps 0.001000000007572694, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 510
=== ep: 511, time 27.633522987365723, eps 0.0010000000072033692, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 511
goal_identified
goal_identified
=== ep: 512, time 27.38630485534668, eps 0.001000000006852057, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 512
goal_identified
=== ep: 513, time 27.082225561141968, eps 0.001000000006517878, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 513
goal_identified
goal_identified
=== ep: 514, time 26.967228174209595, eps 0.0010000000061999974, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 514
goal_identified
=== ep: 515, time 27.19667959213257, eps 0.0010000000058976199, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 515
goal_identified
goal_identified
=== ep: 516, time 27.164259433746338, eps 0.0010000000056099897, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 516
goal_identified
=== ep: 517, time 27.359716653823853, eps 0.0010000000053363872, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 517
goal_identified
=== ep: 518, time 27.27678346633911, eps 0.0010000000050761286, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 518
goal_identified
goal_identified
=== ep: 519, time 34.38374924659729, eps 0.001000000004828563, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 519
goal_identified
goal_identified
=== ep: 520, time 27.288881063461304, eps 0.001000000004593071, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 520
goal_identified
goal_identified
goal_identified
=== ep: 521, time 27.355172872543335, eps 0.0010000000043690644, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 521
goal_identified
=== ep: 522, time 27.116085290908813, eps 0.0010000000041559827, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 522
=== ep: 523, time 27.29509139060974, eps 0.0010000000039532928, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 523
goal_identified
goal_identified
goal_identified
=== ep: 524, time 27.09783101081848, eps 0.0010000000037604885, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 524
goal_identified
goal_identified
=== ep: 525, time 27.44256830215454, eps 0.0010000000035770874, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 525
goal_identified
=== ep: 526, time 27.451416492462158, eps 0.0010000000034026306, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 526
goal_identified
goal_identified
=== ep: 527, time 27.60695719718933, eps 0.0010000000032366824, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 527
goal_identified
=== ep: 528, time 27.526793479919434, eps 0.0010000000030788276, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 528
goal_identified
=== ep: 529, time 32.6346173286438, eps 0.0010000000029286714, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 529
goal_identified
goal_identified
goal_identified
=== ep: 530, time 27.01614761352539, eps 0.0010000000027858384, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 530
goal_identified
=== ep: 531, time 27.493455410003662, eps 0.0010000000026499714, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 531
goal_identified
=== ep: 532, time 27.190497875213623, eps 0.0010000000025207308, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 532
goal_identified
goal_identified
=== ep: 533, time 26.84077286720276, eps 0.0010000000023977934, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 533
goal_identified
=== ep: 534, time 27.374683380126953, eps 0.0010000000022808515, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 534
goal_identified
goal_identified
=== ep: 535, time 27.396170377731323, eps 0.0010000000021696133, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 535
=== ep: 536, time 27.500346183776855, eps 0.0010000000020637999, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 536
goal_identified
=== ep: 537, time 27.46920418739319, eps 0.0010000000019631471, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 537
goal_identified
=== ep: 538, time 27.202855110168457, eps 0.0010000000018674034, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 538
goal_identified
goal_identified
=== ep: 539, time 32.13623070716858, eps 0.001000000001776329, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 539
goal_identified
=== ep: 540, time 27.140265464782715, eps 0.0010000000016896964, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 540
=== ep: 541, time 27.354837894439697, eps 0.001000000001607289, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 541
goal_identified
=== ep: 542, time 26.776835680007935, eps 0.0010000000015289005, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 542
goal_identified
goal_identified
=== ep: 543, time 27.144959926605225, eps 0.0010000000014543352, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 543
goal_identified
=== ep: 544, time 27.65550422668457, eps 0.0010000000013834064, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 544
goal_identified
=== ep: 545, time 27.011868476867676, eps 0.001000000001315937, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 545
goal_identified
=== ep: 546, time 27.172886610031128, eps 0.0010000000012517578, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 546
=== ep: 547, time 27.660499095916748, eps 0.001000000001190709, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 547
goal_identified
goal_identified
=== ep: 548, time 27.22946786880493, eps 0.0010000000011326374, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 548
=== ep: 549, time 33.58958888053894, eps 0.001000000001077398, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 549
goal_identified
goal_identified
=== ep: 550, time 27.436149835586548, eps 0.0010000000010248527, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 550
goal_identified
=== ep: 551, time 27.37009859085083, eps 0.00100000000097487, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 551
goal_identified
goal_identified
goal_identified
=== ep: 552, time 27.30017876625061, eps 0.001000000000927325, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 552
goal_identified
goal_identified
goal_identified
=== ep: 553, time 27.136277437210083, eps 0.0010000000008820989, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 553
goal_identified
=== ep: 554, time 27.907369375228882, eps 0.0010000000008390784, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 554
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 555, time 27.598129749298096, eps 0.001000000000798156, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 555
goal_identified
=== ep: 556, time 27.515644311904907, eps 0.0010000000007592295, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 556
goal_identified
=== ep: 557, time 27.29306697845459, eps 0.0010000000007222014, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 557
goal_identified
=== ep: 558, time 27.668657541275024, eps 0.0010000000006869794, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 558
goal_identified
=== ep: 559, time 37.48899221420288, eps 0.001000000000653475, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 559
=== ep: 560, time 27.636513471603394, eps 0.0010000000006216046, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 560
goal_identified
goal_identified
goal_identified
=== ep: 561, time 27.561012268066406, eps 0.0010000000005912885, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 561
goal_identified
goal_identified
=== ep: 562, time 27.276674032211304, eps 0.0010000000005624511, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 562
goal_identified
goal_identified
=== ep: 563, time 27.38779330253601, eps 0.00100000000053502, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 563
goal_identified
goal_identified
=== ep: 564, time 27.447306156158447, eps 0.001000000000508927, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 564
goal_identified
goal_identified
goal_identified
=== ep: 565, time 27.635615825653076, eps 0.001000000000484106, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 565
goal_identified
=== ep: 566, time 27.23374080657959, eps 0.001000000000460496, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 566
goal_identified
=== ep: 567, time 27.318077087402344, eps 0.0010000000004380374, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 567
=== ep: 568, time 27.436378955841064, eps 0.001000000000416674, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 568
goal_identified
goal_identified
goal_identified
=== ep: 569, time 35.47027635574341, eps 0.0010000000003963527, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 569
goal_identified
=== ep: 570, time 27.436230182647705, eps 0.0010000000003770222, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 570
goal_identified
goal_identified
goal_identified
=== ep: 571, time 27.464651346206665, eps 0.0010000000003586346, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 571
=== ep: 572, time 27.338330507278442, eps 0.0010000000003411438, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 572
goal_identified
=== ep: 573, time 27.541675329208374, eps 0.001000000000324506, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 573
goal_identified
goal_identified
=== ep: 574, time 27.15667963027954, eps 0.0010000000003086798, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 574
=== ep: 575, time 27.189284563064575, eps 0.0010000000002936252, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 575
goal_identified
goal_identified
=== ep: 576, time 27.235265493392944, eps 0.001000000000279305, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 576
=== ep: 577, time 27.181939125061035, eps 0.0010000000002656831, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 577
goal_identified
=== ep: 578, time 27.83271074295044, eps 0.0010000000002527256, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 578
goal_identified
goal_identified
=== ep: 579, time 32.41009259223938, eps 0.0010000000002404, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 579
=== ep: 580, time 27.333117723464966, eps 0.0010000000002286756, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 580
goal_identified
=== ep: 581, time 27.32625102996826, eps 0.0010000000002175229, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 581
goal_identified
goal_identified
=== ep: 582, time 27.58999514579773, eps 0.0010000000002069142, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 582
=== ep: 583, time 27.43633484840393, eps 0.0010000000001968228, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 583
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 584, time 27.176103115081787, eps 0.0010000000001872237, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 66
=== ep: 585, time 27.607011079788208, eps 0.0010000000001780928, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 585
=== ep: 586, time 27.800814867019653, eps 0.001000000000169407, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 586
goal_identified
=== ep: 587, time 27.544203996658325, eps 0.001000000000161145, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 587
goal_identified
goal_identified
goal_identified
=== ep: 588, time 27.4194118976593, eps 0.0010000000001532858, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 588
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 589, time 33.70190954208374, eps 0.00100000000014581, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 107
goal_identified
=== ep: 590, time 27.6021146774292, eps 0.0010000000001386988, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 590
goal_identified
goal_identified
goal_identified
=== ep: 591, time 27.98594307899475, eps 0.0010000000001319344, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 591
goal_identified
=== ep: 592, time 27.77783226966858, eps 0.0010000000001255, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 592
goal_identified
goal_identified
=== ep: 593, time 26.954544067382812, eps 0.0010000000001193791, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 593
=== ep: 594, time 27.49715781211853, eps 0.001000000000113557, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 594
=== ep: 595, time 27.730010509490967, eps 0.0010000000001080186, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 595
goal_identified
=== ep: 596, time 27.587969303131104, eps 0.0010000000001027505, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 596
goal_identified
=== ep: 597, time 27.56206202507019, eps 0.0010000000000977393, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 597
goal_identified
goal_identified
goal_identified
=== ep: 598, time 27.67590308189392, eps 0.0010000000000929725, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 598
goal_identified
=== ep: 599, time 34.17170715332031, eps 0.0010000000000884382, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 599
goal_identified
goal_identified
=== ep: 600, time 27.674559831619263, eps 0.001000000000084125, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 600
goal_identified
goal_identified
=== ep: 601, time 27.487852811813354, eps 0.0010000000000800222, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 601
goal_identified
=== ep: 602, time 27.40683078765869, eps 0.0010000000000761195, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 602
goal_identified
goal_identified
=== ep: 603, time 27.47580313682556, eps 0.0010000000000724072, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 603
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 604, time 27.4153835773468, eps 0.0010000000000688757, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 142
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 605, time 27.30950617790222, eps 0.0010000000000655166, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 188
goal_identified
=== ep: 606, time 27.783473014831543, eps 0.0010000000000623215, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 606
goal_identified
goal_identified
=== ep: 607, time 27.818617343902588, eps 0.001000000000059282, sum reward: 2, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 607
goal_identified
goal_identified
=== ep: 608, time 27.444704055786133, eps 0.0010000000000563907, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 608
goal_identified
goal_identified
=== ep: 609, time 35.96796679496765, eps 0.0010000000000536405, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 609
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 610, time 27.72891092300415, eps 0.0010000000000510245, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 610
goal_identified
=== ep: 611, time 27.79670238494873, eps 0.0010000000000485358, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 611
goal_identified
goal_identified
=== ep: 612, time 27.946320295333862, eps 0.0010000000000461688, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 612
goal_identified
=== ep: 613, time 27.70868444442749, eps 0.0010000000000439171, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 613
goal_identified
=== ep: 614, time 26.86972188949585, eps 0.0010000000000417752, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 614
=== ep: 615, time 27.893999099731445, eps 0.0010000000000397378, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 11 > 10.0 and we are deleting ep 615
=== ep: 616, time 27.9667866230011, eps 0.0010000000000377999, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 616
=== ep: 617, time 27.723248958587646, eps 0.0010000000000359563, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 617
goal_identified
goal_identified
goal_identified
=== ep: 618, time 27.233694076538086, eps 0.0010000000000342027, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 618
goal_identified
=== ep: 619, time 34.00199747085571, eps 0.0010000000000325345, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 619
goal_identified
goal_identified
=== ep: 620, time 27.336739540100098, eps 0.001000000000030948, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 620
=== ep: 621, time 27.551851272583008, eps 0.0010000000000294385, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 621
goal_identified
goal_identified
goal_identified
=== ep: 622, time 27.555898189544678, eps 0.0010000000000280028, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 622
goal_identified
=== ep: 623, time 27.8876531124115, eps 0.0010000000000266371, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 623
goal_identified
goal_identified
=== ep: 624, time 27.49786114692688, eps 0.001000000000025338, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 624
goal_identified
=== ep: 625, time 27.55789875984192, eps 0.0010000000000241023, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 625
goal_identified
=== ep: 626, time 27.391784191131592, eps 0.0010000000000229268, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 626
=== ep: 627, time 28.96252989768982, eps 0.0010000000000218085, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 627
goal_identified
goal_identified
=== ep: 628, time 27.37836217880249, eps 0.001000000000020745, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 628
goal_identified
=== ep: 629, time 32.298521995544434, eps 0.0010000000000197332, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 629
=== ep: 630, time 27.426833868026733, eps 0.0010000000000187708, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 630
goal_identified
=== ep: 631, time 27.596178770065308, eps 0.0010000000000178553, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 631
goal_identified
goal_identified
=== ep: 632, time 27.4971284866333, eps 0.0010000000000169845, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 632
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 633, time 27.599546194076538, eps 0.0010000000000161562, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 633
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 634, time 27.467261791229248, eps 0.0010000000000153684, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 204
goal_identified
goal_identified
goal_identified
=== ep: 635, time 27.715534687042236, eps 0.0010000000000146188, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 635
goal_identified
goal_identified
goal_identified
=== ep: 636, time 27.224714040756226, eps 0.0010000000000139058, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 636
goal_identified
=== ep: 637, time 27.53028964996338, eps 0.0010000000000132275, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 637
goal_identified
=== ep: 638, time 27.953047513961792, eps 0.0010000000000125824, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 638
goal_identified
goal_identified
=== ep: 639, time 35.34125328063965, eps 0.0010000000000119687, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 639
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 640, time 27.577115058898926, eps 0.001000000000011385, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 270
goal_identified
=== ep: 641, time 27.965823650360107, eps 0.00100000000001083, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 641
goal_identified
=== ep: 642, time 27.368820190429688, eps 0.0010000000000103017, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 642
goal_identified
=== ep: 643, time 27.535420417785645, eps 0.0010000000000097993, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 643
=== ep: 644, time 27.94219136238098, eps 0.0010000000000093213, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 644
=== ep: 645, time 27.59843611717224, eps 0.0010000000000088666, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 645
goal_identified
goal_identified
=== ep: 646, time 27.521289825439453, eps 0.0010000000000084342, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 646
goal_identified
goal_identified
=== ep: 647, time 27.29364562034607, eps 0.001000000000008023, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 647
goal_identified
goal_identified
=== ep: 648, time 27.358825206756592, eps 0.0010000000000076317, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 648
=== ep: 649, time 34.14422416687012, eps 0.0010000000000072594, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 649
goal_identified
goal_identified
goal_identified
=== ep: 650, time 27.517484188079834, eps 0.0010000000000069055, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 650
=== ep: 651, time 27.858418703079224, eps 0.0010000000000065686, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 651
goal_identified
=== ep: 652, time 27.302648544311523, eps 0.0010000000000062483, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 652
goal_identified
goal_identified
goal_identified
=== ep: 653, time 27.387826204299927, eps 0.0010000000000059436, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 653
goal_identified
=== ep: 654, time 27.895605325698853, eps 0.0010000000000056537, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 654
goal_identified
=== ep: 655, time 28.03970718383789, eps 0.0010000000000053779, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 655
goal_identified
=== ep: 656, time 27.365549325942993, eps 0.0010000000000051157, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 656
goal_identified
goal_identified
goal_identified
=== ep: 657, time 27.48837947845459, eps 0.0010000000000048661, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 657
goal_identified
=== ep: 658, time 27.30123734474182, eps 0.001000000000004629, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 658
goal_identified
goal_identified
=== ep: 659, time 34.93754744529724, eps 0.0010000000000044032, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 659
goal_identified
=== ep: 660, time 27.27262854576111, eps 0.0010000000000041883, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 660
goal_identified
goal_identified
=== ep: 661, time 27.219310760498047, eps 0.001000000000003984, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 661
goal_identified
=== ep: 662, time 27.68751835823059, eps 0.0010000000000037897, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 662
goal_identified
goal_identified
=== ep: 663, time 27.250751495361328, eps 0.001000000000003605, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 663
goal_identified
goal_identified
=== ep: 664, time 27.585857629776, eps 0.0010000000000034291, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 664
goal_identified
=== ep: 665, time 27.4624183177948, eps 0.001000000000003262, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 665
goal_identified
=== ep: 666, time 27.77404499053955, eps 0.0010000000000031028, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 666
goal_identified
goal_identified
=== ep: 667, time 27.666009187698364, eps 0.0010000000000029514, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 667
goal_identified
goal_identified
goal_identified
=== ep: 668, time 27.744626998901367, eps 0.0010000000000028075, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 668
=== ep: 669, time 32.86093068122864, eps 0.0010000000000026706, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 669
goal_identified
goal_identified
goal_identified
=== ep: 670, time 27.109791040420532, eps 0.0010000000000025403, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 670
=== ep: 671, time 27.97493839263916, eps 0.0010000000000024165, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 671
goal_identified
goal_identified
=== ep: 672, time 27.29440927505493, eps 0.0010000000000022985, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 672
goal_identified
=== ep: 673, time 27.210277557373047, eps 0.0010000000000021864, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 11 > 10.0 and we are deleting ep 673
goal_identified
=== ep: 674, time 27.108179092407227, eps 0.00100000000000208, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 674
=== ep: 675, time 27.565624237060547, eps 0.0010000000000019785, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 675
goal_identified
goal_identified
=== ep: 676, time 27.020861864089966, eps 0.001000000000001882, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 676
goal_identified
=== ep: 677, time 28.69630241394043, eps 0.0010000000000017903, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 677
goal_identified
goal_identified
goal_identified
=== ep: 678, time 27.263754844665527, eps 0.0010000000000017029, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 678
goal_identified
=== ep: 679, time 34.98635673522949, eps 0.0010000000000016198, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 679
=== ep: 680, time 27.429001569747925, eps 0.0010000000000015409, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 680
=== ep: 681, time 27.085158586502075, eps 0.0010000000000014656, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 681
goal_identified
=== ep: 682, time 27.66147541999817, eps 0.0010000000000013943, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 682
=== ep: 683, time 27.48678493499756, eps 0.0010000000000013262, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 683
goal_identified
goal_identified
=== ep: 684, time 27.789433002471924, eps 0.0010000000000012616, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 684
goal_identified
goal_identified
=== ep: 685, time 27.69341015815735, eps 0.0010000000000012, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 685
=== ep: 686, time 27.270655870437622, eps 0.0010000000000011415, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 686
goal_identified
goal_identified
=== ep: 687, time 27.4589262008667, eps 0.0010000000000010857, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 687
goal_identified
=== ep: 688, time 27.542417764663696, eps 0.0010000000000010328, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 688
goal_identified
=== ep: 689, time 34.11029577255249, eps 0.0010000000000009825, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 689
goal_identified
=== ep: 690, time 27.420671224594116, eps 0.0010000000000009346, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 690
goal_identified
goal_identified
=== ep: 691, time 27.013972759246826, eps 0.001000000000000889, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 691
goal_identified
=== ep: 692, time 27.413102865219116, eps 0.0010000000000008457, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 692
goal_identified
=== ep: 693, time 27.62692403793335, eps 0.0010000000000008045, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 693
goal_identified
=== ep: 694, time 27.903539657592773, eps 0.0010000000000007653, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 694
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 695, time 27.664204359054565, eps 0.0010000000000007277, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 695
goal_identified
goal_identified
=== ep: 696, time 27.6284019947052, eps 0.0010000000000006924, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 696
goal_identified
=== ep: 697, time 27.520302534103394, eps 0.0010000000000006586, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 697
goal_identified
=== ep: 698, time 27.637707233428955, eps 0.0010000000000006265, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 698
=== ep: 699, time 33.89917516708374, eps 0.001000000000000596, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 699
=== ep: 700, time 27.618650197982788, eps 0.0010000000000005668, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 700
=== ep: 701, time 27.373992443084717, eps 0.0010000000000005393, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 701
goal_identified
goal_identified
goal_identified
=== ep: 702, time 27.647096872329712, eps 0.0010000000000005128, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 702
=== ep: 703, time 27.412853956222534, eps 0.001000000000000488, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 703
goal_identified
=== ep: 704, time 27.133314847946167, eps 0.001000000000000464, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 704
goal_identified
goal_identified
goal_identified
=== ep: 705, time 27.642468214035034, eps 0.0010000000000004415, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 705
goal_identified
=== ep: 706, time 27.355048656463623, eps 0.00100000000000042, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 706
goal_identified
=== ep: 707, time 27.4056396484375, eps 0.0010000000000003994, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 707
=== ep: 708, time 27.59657382965088, eps 0.00100000000000038, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 708
goal_identified
=== ep: 709, time 36.34477972984314, eps 0.0010000000000003615, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 709
goal_identified
goal_identified
goal_identified
=== ep: 710, time 27.579193353652954, eps 0.0010000000000003437, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 710
goal_identified
=== ep: 711, time 27.1495418548584, eps 0.001000000000000327, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 711
goal_identified
=== ep: 712, time 26.90402317047119, eps 0.0010000000000003112, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 712
goal_identified
goal_identified
=== ep: 713, time 27.416098594665527, eps 0.001000000000000296, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 713
goal_identified
goal_identified
=== ep: 714, time 27.280680894851685, eps 0.0010000000000002815, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 714
goal_identified
=== ep: 715, time 27.41126251220703, eps 0.0010000000000002678, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 715
goal_identified
=== ep: 716, time 27.049835443496704, eps 0.0010000000000002548, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 716
goal_identified
=== ep: 717, time 27.487724781036377, eps 0.0010000000000002422, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 717
goal_identified
goal_identified
goal_identified
=== ep: 718, time 27.3939151763916, eps 0.0010000000000002305, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 718
=== ep: 719, time 33.10497164726257, eps 0.0010000000000002192, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 719
goal_identified
goal_identified
goal_identified
=== ep: 720, time 27.388899087905884, eps 0.0010000000000002086, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 720
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 721, time 27.756147861480713, eps 0.0010000000000001984, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 721
=== ep: 722, time 27.39982795715332, eps 0.0010000000000001887, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 722
goal_identified
goal_identified
=== ep: 723, time 27.404335975646973, eps 0.0010000000000001796, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 723
goal_identified
goal_identified
=== ep: 724, time 27.51532745361328, eps 0.0010000000000001707, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 724
goal_identified
goal_identified
goal_identified
=== ep: 725, time 27.398613929748535, eps 0.0010000000000001624, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 725
goal_identified
goal_identified
=== ep: 726, time 27.212372064590454, eps 0.0010000000000001544, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 726
=== ep: 727, time 27.415761947631836, eps 0.001000000000000147, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 727
goal_identified
=== ep: 728, time 27.42516016960144, eps 0.0010000000000001399, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 728
=== ep: 729, time 35.53915810585022, eps 0.001000000000000133, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 729
goal_identified
goal_identified
goal_identified
=== ep: 730, time 27.86302351951599, eps 0.0010000000000001264, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 730
goal_identified
goal_identified
=== ep: 731, time 27.271442651748657, eps 0.0010000000000001204, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 731
goal_identified
goal_identified
=== ep: 732, time 27.331985235214233, eps 0.0010000000000001145, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 732
goal_identified
=== ep: 733, time 27.400875329971313, eps 0.0010000000000001089, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 733
goal_identified
=== ep: 734, time 27.684178113937378, eps 0.0010000000000001037, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 734
goal_identified
=== ep: 735, time 27.80092716217041, eps 0.0010000000000000985, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 735
=== ep: 736, time 27.38005018234253, eps 0.0010000000000000937, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 736
goal_identified
=== ep: 737, time 27.03654146194458, eps 0.0010000000000000891, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 737
goal_identified
goal_identified
goal_identified
=== ep: 738, time 27.376677989959717, eps 0.0010000000000000848, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 738
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 739, time 33.88853216171265, eps 0.0010000000000000807, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 366
goal_identified
=== ep: 740, time 27.675978660583496, eps 0.0010000000000000768, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 740
goal_identified
goal_identified
=== ep: 741, time 27.29748797416687, eps 0.001000000000000073, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 741
goal_identified
=== ep: 742, time 27.42854619026184, eps 0.0010000000000000694, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 742
=== ep: 743, time 27.83209204673767, eps 0.001000000000000066, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 743
goal_identified
goal_identified
=== ep: 744, time 27.425946474075317, eps 0.001000000000000063, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 744
goal_identified
goal_identified
=== ep: 745, time 27.63211178779602, eps 0.0010000000000000599, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 745
goal_identified
goal_identified
goal_identified
=== ep: 746, time 27.839242696762085, eps 0.0010000000000000568, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 746
=== ep: 747, time 27.590685844421387, eps 0.001000000000000054, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 747
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 748, time 27.439145803451538, eps 0.0010000000000000514, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 748
goal_identified
goal_identified
=== ep: 749, time 34.5924973487854, eps 0.001000000000000049, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 749
goal_identified
goal_identified
=== ep: 750, time 27.62234878540039, eps 0.0010000000000000466, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 750
goal_identified
=== ep: 751, time 27.59858512878418, eps 0.0010000000000000443, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 751
goal_identified
goal_identified
=== ep: 752, time 27.692826509475708, eps 0.001000000000000042, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 752
goal_identified
goal_identified
=== ep: 753, time 27.375076293945312, eps 0.0010000000000000401, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 753
goal_identified
goal_identified
=== ep: 754, time 27.46689486503601, eps 0.0010000000000000382, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 754
goal_identified
=== ep: 755, time 27.14286732673645, eps 0.0010000000000000362, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 755
goal_identified
goal_identified
goal_identified
=== ep: 756, time 27.45273494720459, eps 0.0010000000000000345, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 756
goal_identified
goal_identified
=== ep: 757, time 27.433382987976074, eps 0.0010000000000000328, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 757
goal_identified
=== ep: 758, time 27.63983654975891, eps 0.0010000000000000312, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 758
=== ep: 759, time 35.31938123703003, eps 0.0010000000000000297, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 759
goal_identified
goal_identified
=== ep: 760, time 27.096399784088135, eps 0.0010000000000000282, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 760
goal_identified
goal_identified
=== ep: 761, time 27.566797018051147, eps 0.001000000000000027, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 761
goal_identified
goal_identified
=== ep: 762, time 27.405941247940063, eps 0.0010000000000000256, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 762
goal_identified
goal_identified
=== ep: 763, time 27.50670289993286, eps 0.0010000000000000243, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 763
=== ep: 764, time 27.553396701812744, eps 0.0010000000000000232, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 764
goal_identified
=== ep: 765, time 27.374123096466064, eps 0.001000000000000022, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 765
goal_identified
=== ep: 766, time 27.150311708450317, eps 0.0010000000000000208, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 766
goal_identified
=== ep: 767, time 27.666382551193237, eps 0.00100000000000002, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 767
goal_identified
goal_identified
=== ep: 768, time 27.304996490478516, eps 0.0010000000000000189, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 768
goal_identified
goal_identified
goal_identified
=== ep: 769, time 33.6790235042572, eps 0.001000000000000018, sum reward: 3, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 397
goal_identified
goal_identified
=== ep: 770, time 27.58418107032776, eps 0.0010000000000000172, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 770
goal_identified
=== ep: 771, time 27.799896240234375, eps 0.0010000000000000163, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 771
goal_identified
=== ep: 772, time 27.96563696861267, eps 0.0010000000000000154, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 772
=== ep: 773, time 27.278148651123047, eps 0.0010000000000000148, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 773
=== ep: 774, time 27.63715434074402, eps 0.0010000000000000141, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 774
goal_identified
goal_identified
=== ep: 775, time 27.535515069961548, eps 0.0010000000000000132, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 775
goal_identified
=== ep: 776, time 27.41162633895874, eps 0.0010000000000000126, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 776
goal_identified
goal_identified
=== ep: 777, time 27.19633388519287, eps 0.0010000000000000122, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 777
goal_identified
goal_identified
=== ep: 778, time 27.437739849090576, eps 0.0010000000000000115, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 778
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 779, time 36.70091271400452, eps 0.0010000000000000109, sum reward: 4, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 779
goal_identified
=== ep: 780, time 27.368635654449463, eps 0.0010000000000000104, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 780
goal_identified
=== ep: 781, time 27.637452840805054, eps 0.00100000000000001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 781
=== ep: 782, time 27.514466047286987, eps 0.0010000000000000093, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 782
=== ep: 783, time 27.654702186584473, eps 0.001000000000000009, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 783
goal_identified
goal_identified
=== ep: 784, time 27.866371631622314, eps 0.0010000000000000085, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 784
goal_identified
=== ep: 785, time 27.78276491165161, eps 0.001000000000000008, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 785
=== ep: 786, time 27.927778959274292, eps 0.0010000000000000076, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 786
=== ep: 787, time 27.4646213054657, eps 0.0010000000000000074, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 787
goal_identified
=== ep: 788, time 27.893644094467163, eps 0.001000000000000007, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 788
=== ep: 789, time 34.06058096885681, eps 0.0010000000000000067, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 789
goal_identified
goal_identified
=== ep: 790, time 27.373648405075073, eps 0.0010000000000000063, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 790
goal_identified
goal_identified
=== ep: 791, time 27.636749744415283, eps 0.001000000000000006, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 791
=== ep: 792, time 27.855180025100708, eps 0.0010000000000000057, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 792
goal_identified
goal_identified
goal_identified
=== ep: 793, time 27.46321177482605, eps 0.0010000000000000054, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 793
=== ep: 794, time 28.048911333084106, eps 0.0010000000000000052, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 794
=== ep: 795, time 27.06550407409668, eps 0.001000000000000005, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 795
=== ep: 796, time 28.000009059906006, eps 0.0010000000000000048, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 796
=== ep: 797, time 27.710530042648315, eps 0.0010000000000000044, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 797
goal_identified
=== ep: 798, time 27.2245454788208, eps 0.0010000000000000041, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 798
goal_identified
=== ep: 799, time 35.935378313064575, eps 0.0010000000000000041, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 799
goal_identified
=== ep: 800, time 27.730685472488403, eps 0.001000000000000004, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 800
goal_identified
=== ep: 801, time 27.527655839920044, eps 0.0010000000000000037, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 801
=== ep: 802, time 27.750954389572144, eps 0.0010000000000000035, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 802
goal_identified
=== ep: 803, time 27.405596017837524, eps 0.0010000000000000033, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 803
goal_identified
goal_identified
goal_identified
=== ep: 804, time 27.215962171554565, eps 0.001000000000000003, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 804
=== ep: 805, time 27.238319396972656, eps 0.001000000000000003, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 805
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 806, time 27.413312673568726, eps 0.0010000000000000028, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 584
=== ep: 807, time 27.579724550247192, eps 0.0010000000000000026, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 807
goal_identified
goal_identified
goal_identified
=== ep: 808, time 27.670170783996582, eps 0.0010000000000000026, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 808
goal_identified
=== ep: 809, time 34.21766471862793, eps 0.0010000000000000024, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 809
goal_identified
=== ep: 810, time 27.459428071975708, eps 0.0010000000000000024, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 810
goal_identified
=== ep: 811, time 27.571512699127197, eps 0.0010000000000000022, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 811
goal_identified
=== ep: 812, time 27.340229034423828, eps 0.0010000000000000022, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 812
goal_identified
=== ep: 813, time 27.366718292236328, eps 0.001000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 813
goal_identified
goal_identified
=== ep: 814, time 27.684635877609253, eps 0.001000000000000002, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 814
=== ep: 815, time 27.487266778945923, eps 0.0010000000000000018, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 815
goal_identified
goal_identified
=== ep: 816, time 27.125409841537476, eps 0.0010000000000000018, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 816
=== ep: 817, time 26.948675394058228, eps 0.0010000000000000018, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 817
goal_identified
=== ep: 818, time 27.80338978767395, eps 0.0010000000000000015, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 818
goal_identified
goal_identified
goal_identified
=== ep: 819, time 34.002272844314575, eps 0.0010000000000000015, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 819
goal_identified
=== ep: 820, time 28.08647346496582, eps 0.0010000000000000013, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 820
goal_identified
=== ep: 821, time 27.631946086883545, eps 0.0010000000000000013, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 821
=== ep: 822, time 27.406960248947144, eps 0.0010000000000000013, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 822
goal_identified
=== ep: 823, time 27.583271980285645, eps 0.0010000000000000013, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 823
goal_identified
=== ep: 824, time 27.272878408432007, eps 0.001000000000000001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 824
goal_identified
goal_identified
=== ep: 825, time 27.753670930862427, eps 0.001000000000000001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 825
goal_identified
goal_identified
=== ep: 826, time 27.160552263259888, eps 0.001000000000000001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 826
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 827, time 27.386049270629883, eps 0.001000000000000001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 827
goal_identified
goal_identified
=== ep: 828, time 27.085269927978516, eps 0.0010000000000000009, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 828
goal_identified
goal_identified
=== ep: 829, time 32.7783420085907, eps 0.0010000000000000009, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 829
goal_identified
=== ep: 830, time 27.077048301696777, eps 0.0010000000000000009, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 830
=== ep: 831, time 27.445518016815186, eps 0.0010000000000000009, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 831
goal_identified
goal_identified
=== ep: 832, time 27.523597717285156, eps 0.0010000000000000009, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 832
goal_identified
=== ep: 833, time 27.342705726623535, eps 0.0010000000000000007, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 833
goal_identified
goal_identified
=== ep: 834, time 27.770229816436768, eps 0.0010000000000000007, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 834
goal_identified
=== ep: 835, time 27.42757225036621, eps 0.0010000000000000007, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 835
=== ep: 836, time 27.584858179092407, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 836
=== ep: 837, time 27.338248014450073, eps 0.0010000000000000007, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 837
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 838, time 27.410677433013916, eps 0.0010000000000000007, sum reward: 7, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 604
goal_identified
goal_identified
=== ep: 839, time 33.723732709884644, eps 0.0010000000000000007, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 839
=== ep: 840, time 27.311688661575317, eps 0.0010000000000000005, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 840
goal_identified
goal_identified
=== ep: 841, time 27.570271492004395, eps 0.0010000000000000005, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 841
goal_identified
goal_identified
=== ep: 842, time 27.852296352386475, eps 0.0010000000000000005, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 842
goal_identified
goal_identified
goal_identified
=== ep: 843, time 27.280046224594116, eps 0.0010000000000000005, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 843
goal_identified
goal_identified
goal_identified
=== ep: 844, time 27.21282935142517, eps 0.0010000000000000005, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 844
goal_identified
goal_identified
=== ep: 845, time 27.26255989074707, eps 0.0010000000000000005, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 845
=== ep: 846, time 28.005677223205566, eps 0.0010000000000000005, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 846
goal_identified
goal_identified
goal_identified
=== ep: 847, time 27.818899869918823, eps 0.0010000000000000005, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 847
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 848, time 27.35686945915222, eps 0.0010000000000000005, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 605
=== ep: 849, time 32.498353242874146, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 13/13)
== current size of memory is eps 11 > 10.0 and we are deleting ep 849
goal_identified
goal_identified
=== ep: 850, time 27.89538288116455, eps 0.0010000000000000002, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 850
goal_identified
goal_identified
goal_identified
=== ep: 851, time 27.974443197250366, eps 0.0010000000000000002, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 851
goal_identified
=== ep: 852, time 27.621187686920166, eps 0.0010000000000000002, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 852
=== ep: 853, time 27.851349353790283, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 853
=== ep: 854, time 27.590426206588745, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 854
goal_identified
=== ep: 855, time 27.877215147018433, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 855
goal_identified
goal_identified
=== ep: 856, time 27.08179211616516, eps 0.0010000000000000002, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 856
goal_identified
goal_identified
=== ep: 857, time 27.61889100074768, eps 0.0010000000000000002, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 857
goal_identified
=== ep: 858, time 27.65882134437561, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 858
goal_identified
=== ep: 859, time 33.706578493118286, eps 0.0010000000000000002, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 859
goal_identified
=== ep: 860, time 27.95061755180359, eps 0.0010000000000000002, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 860
=== ep: 861, time 27.532551527023315, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 861
goal_identified
=== ep: 862, time 27.25047278404236, eps 0.0010000000000000002, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 862
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 863, time 27.582192420959473, eps 0.0010000000000000002, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 634
=== ep: 864, time 27.34615206718445, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 864
goal_identified
=== ep: 865, time 27.573173761367798, eps 0.0010000000000000002, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 865
goal_identified
=== ep: 866, time 27.705238342285156, eps 0.0010000000000000002, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 866
=== ep: 867, time 27.41150736808777, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 867
goal_identified
goal_identified
goal_identified
=== ep: 868, time 27.76228380203247, eps 0.0010000000000000002, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 868
goal_identified
goal_identified
goal_identified
=== ep: 869, time 34.45730543136597, eps 0.0010000000000000002, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 869
goal_identified
=== ep: 870, time 27.643925666809082, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 870
=== ep: 871, time 27.520176887512207, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 871
=== ep: 872, time 27.453263998031616, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 872
goal_identified
goal_identified
=== ep: 873, time 27.487020015716553, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 873
goal_identified
goal_identified
=== ep: 874, time 27.52467918395996, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 874
=== ep: 875, time 27.555909395217896, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 875
goal_identified
=== ep: 876, time 27.59928297996521, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 876
goal_identified
goal_identified
=== ep: 877, time 27.30580449104309, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 877
=== ep: 878, time 27.562352657318115, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 878
goal_identified
=== ep: 879, time 33.12923312187195, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 879
goal_identified
goal_identified
=== ep: 880, time 27.182955980300903, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 880
goal_identified
goal_identified
goal_identified
=== ep: 881, time 27.6369845867157, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 881
goal_identified
=== ep: 882, time 27.361921072006226, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 882
goal_identified
goal_identified
goal_identified
=== ep: 883, time 26.766646146774292, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 883
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 884, time 27.337348461151123, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 640
goal_identified
goal_identified
=== ep: 885, time 27.586381912231445, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 885
goal_identified
=== ep: 886, time 27.59648561477661, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 886
goal_identified
=== ep: 887, time 27.944750547409058, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 887
goal_identified
=== ep: 888, time 27.0964138507843, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 888
goal_identified
=== ep: 889, time 33.644009828567505, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 889
goal_identified
goal_identified
=== ep: 890, time 27.512258529663086, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 890
goal_identified
goal_identified
=== ep: 891, time 27.9113028049469, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 891
=== ep: 892, time 27.521794080734253, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 892
=== ep: 893, time 27.534091234207153, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 893
goal_identified
=== ep: 894, time 27.14242959022522, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 894
goal_identified
goal_identified
=== ep: 895, time 27.61415123939514, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 895
goal_identified
goal_identified
=== ep: 896, time 27.66658902168274, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 896
goal_identified
goal_identified
goal_identified
=== ep: 897, time 27.678701877593994, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 897
goal_identified
goal_identified
=== ep: 898, time 27.43317723274231, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 898
=== ep: 899, time 33.41485118865967, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 899
goal_identified
goal_identified
=== ep: 900, time 27.372672080993652, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 900
=== ep: 901, time 27.602051973342896, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 901
goal_identified
goal_identified
goal_identified
=== ep: 902, time 27.33159637451172, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 902
goal_identified
goal_identified
=== ep: 903, time 27.298523902893066, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 903
goal_identified
=== ep: 904, time 27.56893563270569, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 904
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 905, time 27.275845527648926, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 739
goal_identified
goal_identified
=== ep: 906, time 27.65612030029297, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 906
=== ep: 907, time 27.658541917800903, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 907
=== ep: 908, time 27.254683256149292, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 908
goal_identified
goal_identified
=== ep: 909, time 31.731778383255005, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 909
goal_identified
=== ep: 910, time 27.389759302139282, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 910
=== ep: 911, time 27.7519052028656, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 911
goal_identified
=== ep: 912, time 27.290993452072144, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 912
goal_identified
=== ep: 913, time 27.296512603759766, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 913
=== ep: 914, time 27.4778790473938, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 914
=== ep: 915, time 27.33083701133728, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 915
goal_identified
=== ep: 916, time 27.638203620910645, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 916
=== ep: 917, time 27.447364330291748, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 917
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 918, time 27.32994532585144, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 848
goal_identified
=== ep: 919, time 33.15142798423767, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 919
goal_identified
=== ep: 920, time 27.540435552597046, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 920
goal_identified
=== ep: 921, time 27.413867473602295, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 921
goal_identified
goal_identified
goal_identified
=== ep: 922, time 27.441638708114624, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 922
goal_identified
=== ep: 923, time 28.150877237319946, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 923
=== ep: 924, time 27.631290197372437, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 924
goal_identified
=== ep: 925, time 27.6657931804657, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 925
=== ep: 926, time 27.6330828666687, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 926
goal_identified
goal_identified
=== ep: 927, time 26.82585334777832, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 927
goal_identified
=== ep: 928, time 27.25965452194214, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 928
goal_identified
=== ep: 929, time 33.32361030578613, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 929
goal_identified
=== ep: 930, time 27.19937539100647, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 930
=== ep: 931, time 27.42901039123535, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 931
goal_identified
goal_identified
=== ep: 932, time 27.328620672225952, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 932
goal_identified
goal_identified
goal_identified
=== ep: 933, time 27.479249715805054, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 933
=== ep: 934, time 27.342954397201538, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 934
goal_identified
=== ep: 935, time 27.680150032043457, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 935
goal_identified
goal_identified
=== ep: 936, time 27.19800043106079, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 936
goal_identified
=== ep: 937, time 27.437984943389893, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 937
goal_identified
goal_identified
=== ep: 938, time 27.229865312576294, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 938
=== ep: 939, time 33.52279448509216, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 939
goal_identified
goal_identified
=== ep: 940, time 27.67355465888977, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 940
=== ep: 941, time 27.53491187095642, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 941
goal_identified
goal_identified
=== ep: 942, time 27.593034744262695, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 942
goal_identified
goal_identified
goal_identified
=== ep: 943, time 27.433159828186035, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 943
goal_identified
goal_identified
goal_identified
=== ep: 944, time 27.32472324371338, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 944
goal_identified
goal_identified
goal_identified
=== ep: 945, time 27.35727071762085, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 945
goal_identified
goal_identified
=== ep: 946, time 27.374231815338135, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 946
=== ep: 947, time 27.258336305618286, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 947
goal_identified
goal_identified
goal_identified
=== ep: 948, time 27.596576929092407, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 948
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 949, time 33.3102810382843, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 949
goal_identified
=== ep: 950, time 27.358646392822266, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 950
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 951, time 27.784736156463623, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 863
goal_identified
goal_identified
=== ep: 952, time 27.06889271736145, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 952
=== ep: 953, time 27.606179237365723, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 953
=== ep: 954, time 27.460755586624146, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 954
goal_identified
=== ep: 955, time 27.6166775226593, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 955
goal_identified
goal_identified
=== ep: 956, time 27.349530935287476, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 956
goal_identified
goal_identified
goal_identified
=== ep: 957, time 27.71273708343506, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 957
goal_identified
=== ep: 958, time 27.464805126190186, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 958
goal_identified
=== ep: 959, time 32.906646966934204, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 959
goal_identified
goal_identified
goal_identified
=== ep: 960, time 27.56213068962097, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 960
=== ep: 961, time 27.860127449035645, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 961
=== ep: 962, time 27.37334680557251, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 962
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 963, time 27.587765216827393, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 884
goal_identified
goal_identified
goal_identified
=== ep: 964, time 27.66460394859314, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 964
goal_identified
goal_identified
=== ep: 965, time 27.537293195724487, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 965
goal_identified
goal_identified
=== ep: 966, time 27.460144519805908, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 966
=== ep: 967, time 27.841639280319214, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 967
=== ep: 968, time 27.6708025932312, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 968
=== ep: 969, time 32.86100792884827, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 969
=== ep: 970, time 27.413470029830933, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 970
=== ep: 971, time 27.48840045928955, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 971
goal_identified
goal_identified
goal_identified
=== ep: 972, time 27.466989040374756, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 972
goal_identified
goal_identified
goal_identified
=== ep: 973, time 27.309685707092285, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 973
goal_identified
=== ep: 974, time 26.902790546417236, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 974
goal_identified
goal_identified
goal_identified
=== ep: 975, time 27.311250686645508, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 975
goal_identified
goal_identified
=== ep: 976, time 27.83213472366333, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 976
=== ep: 977, time 27.378567457199097, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 977
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 978, time 27.175962924957275, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 978
goal_identified
goal_identified
=== ep: 979, time 32.61793923377991, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 979
goal_identified
goal_identified
=== ep: 980, time 27.40325403213501, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 980
=== ep: 981, time 27.64841651916504, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 981
goal_identified
goal_identified
=== ep: 982, time 27.534088373184204, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 982
=== ep: 983, time 27.513846158981323, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 983
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 984, time 27.38106369972229, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 905
goal_identified
goal_identified
goal_identified
=== ep: 985, time 27.540767431259155, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 985
goal_identified
goal_identified
=== ep: 986, time 27.16922926902771, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 986
goal_identified
goal_identified
=== ep: 987, time 27.090471744537354, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 987
goal_identified
=== ep: 988, time 27.57535672187805, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 988
=== ep: 989, time 31.983778476715088, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 989
goal_identified
goal_identified
=== ep: 990, time 27.114075422286987, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 990
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 991, time 27.553797960281372, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 918
goal_identified
=== ep: 992, time 27.671534776687622, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 992
goal_identified
=== ep: 993, time 27.716375589370728, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 993
goal_identified
goal_identified
=== ep: 994, time 27.38390326499939, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 994
goal_identified
=== ep: 995, time 27.420060873031616, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 995
goal_identified
=== ep: 996, time 27.663591146469116, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 996
goal_identified
=== ep: 997, time 27.66154932975769, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 997
=== ep: 998, time 27.31428623199463, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 998
goal_identified
goal_identified
=== ep: 999, time 31.54327940940857, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 999
=== ep: 1000, time 27.45488667488098, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1000
=== ep: 1001, time 27.624370574951172, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1001
goal_identified
goal_identified
=== ep: 1002, time 27.58850598335266, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1002
goal_identified
=== ep: 1003, time 27.629340410232544, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1003
goal_identified
goal_identified
goal_identified
=== ep: 1004, time 27.59823727607727, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1004
goal_identified
goal_identified
=== ep: 1005, time 27.6401526927948, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1005
=== ep: 1006, time 27.589834690093994, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1006
=== ep: 1007, time 27.24091863632202, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1007
goal_identified
goal_identified
=== ep: 1008, time 27.380112409591675, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1008
goal_identified
=== ep: 1009, time 32.73459529876709, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1009
goal_identified
goal_identified
=== ep: 1010, time 27.467775106430054, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1010
goal_identified
=== ep: 1011, time 27.624061822891235, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1011
=== ep: 1012, time 27.77797293663025, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1012
goal_identified
=== ep: 1013, time 28.01862120628357, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1013
=== ep: 1014, time 27.37899088859558, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1014
goal_identified
goal_identified
=== ep: 1015, time 27.621914386749268, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1015
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1016, time 27.007054567337036, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 951
goal_identified
=== ep: 1017, time 27.23093008995056, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1017
goal_identified
goal_identified
=== ep: 1018, time 27.88658046722412, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1018
=== ep: 1019, time 33.05250382423401, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1019
=== ep: 1020, time 27.498933792114258, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1020
=== ep: 1021, time 27.444705486297607, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1021
goal_identified
goal_identified
goal_identified
=== ep: 1022, time 27.364055395126343, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1022
goal_identified
goal_identified
=== ep: 1023, time 28.11520290374756, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1023
goal_identified
goal_identified
=== ep: 1024, time 26.917755603790283, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1024
goal_identified
goal_identified
=== ep: 1025, time 27.179673194885254, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1025
goal_identified
goal_identified
=== ep: 1026, time 27.181671619415283, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1026
=== ep: 1027, time 27.815725803375244, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1027
goal_identified
goal_identified
=== ep: 1028, time 27.357664346694946, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1028
goal_identified
goal_identified
=== ep: 1029, time 33.05219650268555, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1029
goal_identified
goal_identified
=== ep: 1030, time 27.392616987228394, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1030
goal_identified
=== ep: 1031, time 27.424376249313354, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1031
=== ep: 1032, time 27.37158727645874, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1032
=== ep: 1033, time 27.237223148345947, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1033
goal_identified
goal_identified
goal_identified
=== ep: 1034, time 27.018492937088013, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1034
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1035, time 27.445586681365967, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 963
goal_identified
goal_identified
=== ep: 1036, time 27.661604166030884, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1036
goal_identified
goal_identified
=== ep: 1037, time 27.587586402893066, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1037
goal_identified
goal_identified
=== ep: 1038, time 27.589415073394775, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1038
goal_identified
=== ep: 1039, time 33.925493240356445, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1039
goal_identified
goal_identified
goal_identified
=== ep: 1040, time 27.538954973220825, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1040
=== ep: 1041, time 27.446471452713013, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1041
goal_identified
goal_identified
goal_identified
=== ep: 1042, time 27.97773027420044, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1042
goal_identified
=== ep: 1043, time 27.78721308708191, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1043
goal_identified
goal_identified
=== ep: 1044, time 27.53096842765808, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1044
=== ep: 1045, time 28.125027656555176, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1045
goal_identified
=== ep: 1046, time 27.114425659179688, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1046
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1047, time 27.332335472106934, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1047
goal_identified
=== ep: 1048, time 27.158597469329834, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1048
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1049, time 33.04010367393494, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 984
goal_identified
goal_identified
=== ep: 1050, time 27.298306703567505, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1050
goal_identified
goal_identified
goal_identified
=== ep: 1051, time 27.2044038772583, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1051
=== ep: 1052, time 27.17930769920349, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1052
goal_identified
goal_identified
=== ep: 1053, time 27.684578895568848, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1053
goal_identified
goal_identified
=== ep: 1054, time 27.81347155570984, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1054
=== ep: 1055, time 27.14022207260132, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1055
goal_identified
goal_identified
=== ep: 1056, time 27.33471155166626, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1056
goal_identified
=== ep: 1057, time 27.445210933685303, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1057
goal_identified
goal_identified
goal_identified
=== ep: 1058, time 27.534838914871216, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1058
goal_identified
goal_identified
goal_identified
=== ep: 1059, time 32.86456656455994, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1059
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1060, time 27.42890429496765, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1060
goal_identified
goal_identified
=== ep: 1061, time 27.403525352478027, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1061
goal_identified
goal_identified
goal_identified
=== ep: 1062, time 27.16067934036255, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1062
goal_identified
=== ep: 1063, time 27.419188022613525, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1063
=== ep: 1064, time 27.411940813064575, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1064
goal_identified
goal_identified
goal_identified
=== ep: 1065, time 27.51871633529663, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1065
goal_identified
=== ep: 1066, time 27.446569681167603, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1066
goal_identified
=== ep: 1067, time 27.702428817749023, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1067
goal_identified
=== ep: 1068, time 27.146897315979004, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1068
goal_identified
goal_identified
=== ep: 1069, time 33.16868042945862, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1069
goal_identified
goal_identified
goal_identified
=== ep: 1070, time 27.27712845802307, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1070
goal_identified
goal_identified
=== ep: 1071, time 27.562589406967163, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1071
goal_identified
goal_identified
=== ep: 1072, time 27.604116678237915, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1072
goal_identified
=== ep: 1073, time 27.436224460601807, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1073
goal_identified
goal_identified
goal_identified
=== ep: 1074, time 27.371790409088135, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1074
goal_identified
=== ep: 1075, time 27.46341896057129, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1075
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1076, time 27.784311771392822, eps 0.001, sum reward: 5, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1076
=== ep: 1077, time 27.801544666290283, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1077
goal_identified
=== ep: 1078, time 27.123169660568237, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1078
goal_identified
goal_identified
=== ep: 1079, time 32.17762303352356, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1079
goal_identified
=== ep: 1080, time 27.58700442314148, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1080
goal_identified
=== ep: 1081, time 27.616043090820312, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1081
goal_identified
=== ep: 1082, time 27.254728078842163, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1082
goal_identified
goal_identified
=== ep: 1083, time 27.327456951141357, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1083
goal_identified
goal_identified
=== ep: 1084, time 27.077775478363037, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1084
goal_identified
goal_identified
=== ep: 1085, time 27.253483057022095, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1085
goal_identified
=== ep: 1086, time 27.49164581298828, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1086
goal_identified
goal_identified
goal_identified
=== ep: 1087, time 27.031694412231445, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1087
goal_identified
=== ep: 1088, time 27.556172132492065, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1088
goal_identified
=== ep: 1089, time 33.19359302520752, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1089
goal_identified
goal_identified
goal_identified
=== ep: 1090, time 27.566705465316772, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1090
=== ep: 1091, time 27.532326459884644, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1091
=== ep: 1092, time 27.299884796142578, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1092
goal_identified
=== ep: 1093, time 27.244792461395264, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1093
goal_identified
goal_identified
=== ep: 1094, time 27.360894441604614, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1094
=== ep: 1095, time 27.998944520950317, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1095
goal_identified
goal_identified
=== ep: 1096, time 27.54927968978882, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1096
=== ep: 1097, time 27.397175312042236, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1097
goal_identified
=== ep: 1098, time 27.219616889953613, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1098
goal_identified
goal_identified
goal_identified
=== ep: 1099, time 33.35049319267273, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1099
goal_identified
=== ep: 1100, time 27.395628929138184, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1100
goal_identified
=== ep: 1101, time 27.384459257125854, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1101
=== ep: 1102, time 27.27100968360901, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1102
goal_identified
goal_identified
=== ep: 1103, time 27.149860620498657, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1103
goal_identified
=== ep: 1104, time 27.53257918357849, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1104
goal_identified
goal_identified
goal_identified
=== ep: 1105, time 27.098599672317505, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1105
goal_identified
goal_identified
=== ep: 1106, time 27.362892627716064, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1106
goal_identified
goal_identified
goal_identified
=== ep: 1107, time 27.69953417778015, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1107
goal_identified
goal_identified
goal_identified
=== ep: 1108, time 26.941837549209595, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1108
goal_identified
=== ep: 1109, time 33.01875925064087, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1109
goal_identified
=== ep: 1110, time 27.46419334411621, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1110
goal_identified
=== ep: 1111, time 27.335391521453857, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1111
goal_identified
=== ep: 1112, time 27.640483856201172, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1112
goal_identified
goal_identified
goal_identified
=== ep: 1113, time 27.06137204170227, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1113
goal_identified
=== ep: 1114, time 27.38584566116333, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1114
goal_identified
=== ep: 1115, time 27.144160985946655, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1115
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1116, time 27.471139907836914, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 991
=== ep: 1117, time 28.230064630508423, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1117
goal_identified
goal_identified
=== ep: 1118, time 27.2603440284729, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1118
goal_identified
=== ep: 1119, time 32.4294855594635, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1119
goal_identified
=== ep: 1120, time 27.314218997955322, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1120
=== ep: 1121, time 27.55157732963562, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1121
goal_identified
=== ep: 1122, time 27.337131023406982, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1122
goal_identified
goal_identified
goal_identified
=== ep: 1123, time 27.74595332145691, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1123
goal_identified
=== ep: 1124, time 27.089261531829834, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1124
=== ep: 1125, time 27.126300811767578, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1125
goal_identified
=== ep: 1126, time 26.677223920822144, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1126
=== ep: 1127, time 26.573787689208984, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1127
goal_identified
goal_identified
=== ep: 1128, time 26.49821162223816, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1128
=== ep: 1129, time 32.72531771659851, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1129
goal_identified
goal_identified
=== ep: 1130, time 26.380531311035156, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1130
goal_identified
goal_identified
=== ep: 1131, time 26.74100136756897, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1131
goal_identified
=== ep: 1132, time 26.710442304611206, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1132
goal_identified
=== ep: 1133, time 26.665541648864746, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1133
=== ep: 1134, time 27.22532820701599, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1134
goal_identified
=== ep: 1135, time 26.688056707382202, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1135
goal_identified
goal_identified
goal_identified
=== ep: 1136, time 26.298800230026245, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1136
goal_identified
goal_identified
=== ep: 1137, time 26.614349365234375, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1137
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1138, time 26.823573112487793, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1138
goal_identified
=== ep: 1139, time 32.53066420555115, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1139
goal_identified
=== ep: 1140, time 26.376808643341064, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1140
=== ep: 1141, time 26.85671353340149, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1141
=== ep: 1142, time 26.163477659225464, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1142
goal_identified
goal_identified
=== ep: 1143, time 26.683106899261475, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1143
goal_identified
goal_identified
goal_identified
=== ep: 1144, time 26.511911869049072, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1144
goal_identified
goal_identified
=== ep: 1145, time 26.71948528289795, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1145
goal_identified
goal_identified
=== ep: 1146, time 26.545288562774658, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1146
goal_identified
=== ep: 1147, time 26.89513063430786, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1147
goal_identified
goal_identified
goal_identified
=== ep: 1148, time 26.475134134292603, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1148
goal_identified
goal_identified
goal_identified
=== ep: 1149, time 36.10951113700867, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1149
goal_identified
goal_identified
goal_identified
=== ep: 1150, time 26.93809747695923, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1150
goal_identified
=== ep: 1151, time 26.72143030166626, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1151
goal_identified
=== ep: 1152, time 26.856982946395874, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1152
=== ep: 1153, time 26.757592916488647, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1153
goal_identified
goal_identified
goal_identified
=== ep: 1154, time 26.76834201812744, eps 0.001, sum reward: 3, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1154
=== ep: 1155, time 26.9167742729187, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1155
goal_identified
goal_identified
=== ep: 1156, time 26.810187816619873, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1156
=== ep: 1157, time 26.74967670440674, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1157
=== ep: 1158, time 26.689531803131104, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1158
goal_identified
=== ep: 1159, time 30.922728538513184, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1159
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1160, time 26.59329319000244, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1160
goal_identified
=== ep: 1161, time 26.579916954040527, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1161
goal_identified
=== ep: 1162, time 26.677311658859253, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1162
goal_identified
goal_identified
=== ep: 1163, time 26.54976487159729, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1163
goal_identified
=== ep: 1164, time 26.972031354904175, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1164
goal_identified
goal_identified
=== ep: 1165, time 26.41096329689026, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1165
goal_identified
goal_identified
goal_identified
=== ep: 1166, time 26.78430724143982, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1166
goal_identified
=== ep: 1167, time 26.500863552093506, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1167
goal_identified
goal_identified
goal_identified
=== ep: 1168, time 26.24768304824829, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1168
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1169, time 31.43393588066101, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1035
=== ep: 1170, time 26.907379865646362, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1170
goal_identified
goal_identified
=== ep: 1171, time 26.700631380081177, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1171
=== ep: 1172, time 26.32221245765686, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1172
=== ep: 1173, time 27.05041766166687, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1173
=== ep: 1174, time 26.435441493988037, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1174
=== ep: 1175, time 27.059479236602783, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1175
=== ep: 1176, time 26.680885076522827, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1176
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1177, time 26.502856492996216, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1177
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1178, time 27.182108640670776, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1178
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1179, time 32.81285357475281, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1179
=== ep: 1180, time 26.934298038482666, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1180
goal_identified
=== ep: 1181, time 27.30859112739563, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1181
goal_identified
=== ep: 1182, time 26.422032117843628, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1182
goal_identified
goal_identified
=== ep: 1183, time 26.378342628479004, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1183
goal_identified
=== ep: 1184, time 26.797208547592163, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1184
goal_identified
=== ep: 1185, time 27.135236978530884, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1185
goal_identified
=== ep: 1186, time 27.00519347190857, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1186
goal_identified
goal_identified
goal_identified
=== ep: 1187, time 26.704599380493164, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1187
goal_identified
=== ep: 1188, time 26.758106231689453, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1188
goal_identified
goal_identified
goal_identified
=== ep: 1189, time 32.201621770858765, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1189
goal_identified
goal_identified
=== ep: 1190, time 26.68887448310852, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1190
goal_identified
=== ep: 1191, time 26.876547813415527, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1191
goal_identified
=== ep: 1192, time 26.47130584716797, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1192
goal_identified
goal_identified
=== ep: 1193, time 26.723551511764526, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1193
goal_identified
=== ep: 1194, time 26.41950273513794, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1194
goal_identified
=== ep: 1195, time 26.71402931213379, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1195
goal_identified
goal_identified
=== ep: 1196, time 26.87327218055725, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1196
goal_identified
=== ep: 1197, time 26.190121173858643, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1197
goal_identified
goal_identified
=== ep: 1198, time 25.68947720527649, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1198
=== ep: 1199, time 32.44276714324951, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1199
goal_identified
=== ep: 1200, time 26.44633150100708, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1200
goal_identified
=== ep: 1201, time 27.056568384170532, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1201
goal_identified
goal_identified
goal_identified
=== ep: 1202, time 26.385016441345215, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1202
=== ep: 1203, time 26.525896549224854, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1203
goal_identified
=== ep: 1204, time 27.077488899230957, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1204
goal_identified
goal_identified
goal_identified
=== ep: 1205, time 26.95348620414734, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1205
goal_identified
=== ep: 1206, time 26.844982624053955, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1206
goal_identified
=== ep: 1207, time 26.61678695678711, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1207
goal_identified
goal_identified
=== ep: 1208, time 26.460402250289917, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1208
=== ep: 1209, time 31.657076358795166, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1209
=== ep: 1210, time 26.655125617980957, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1210
=== ep: 1211, time 26.36409640312195, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1211
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1212, time 26.359867334365845, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1169
goal_identified
=== ep: 1213, time 26.383888006210327, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1213
goal_identified
goal_identified
=== ep: 1214, time 26.65722632408142, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1214
=== ep: 1215, time 26.72119450569153, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1215
=== ep: 1216, time 26.535712242126465, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1216
goal_identified
goal_identified
=== ep: 1217, time 26.50912308692932, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1217
goal_identified
=== ep: 1218, time 26.74848198890686, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1218
=== ep: 1219, time 32.02806234359741, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1219
goal_identified
=== ep: 1220, time 26.73216962814331, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1220
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1221, time 26.461588859558105, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1221
goal_identified
=== ep: 1222, time 26.876063346862793, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1222
=== ep: 1223, time 26.20750665664673, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1223
goal_identified
goal_identified
=== ep: 1224, time 26.497620344161987, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1224
goal_identified
=== ep: 1225, time 26.721824645996094, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1225
goal_identified
goal_identified
goal_identified
=== ep: 1226, time 26.394202947616577, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1226
goal_identified
goal_identified
=== ep: 1227, time 26.750518798828125, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1227
goal_identified
goal_identified
=== ep: 1228, time 26.64957880973816, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1228
goal_identified
goal_identified
=== ep: 1229, time 32.06239438056946, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1229
goal_identified
=== ep: 1230, time 26.397007942199707, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1230
goal_identified
goal_identified
=== ep: 1231, time 26.49905014038086, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1231
goal_identified
=== ep: 1232, time 26.447415113449097, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1232
goal_identified
=== ep: 1233, time 26.14521598815918, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1233
=== ep: 1234, time 26.86577081680298, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1234
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1235, time 26.905792713165283, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 420
goal_identified
goal_identified
goal_identified
=== ep: 1236, time 26.522447109222412, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1236
=== ep: 1237, time 27.145107984542847, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1237
goal_identified
goal_identified
=== ep: 1238, time 26.43239688873291, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1238
goal_identified
goal_identified
=== ep: 1239, time 32.82591390609741, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1239
goal_identified
goal_identified
goal_identified
=== ep: 1240, time 26.971060514450073, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1240
goal_identified
=== ep: 1241, time 26.52760624885559, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1241
goal_identified
goal_identified
goal_identified
=== ep: 1242, time 26.782491207122803, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1242
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1243, time 26.557289838790894, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1243
goal_identified
=== ep: 1244, time 26.853822469711304, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1244
goal_identified
=== ep: 1245, time 26.63655185699463, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1245
goal_identified
=== ep: 1246, time 26.62456202507019, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1246
=== ep: 1247, time 27.129847049713135, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1247
goal_identified
=== ep: 1248, time 26.78921389579773, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1248
goal_identified
=== ep: 1249, time 32.681883811950684, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1249
=== ep: 1250, time 26.39730405807495, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1250
goal_identified
=== ep: 1251, time 26.949986219406128, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1251
goal_identified
goal_identified
goal_identified
=== ep: 1252, time 26.44256854057312, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1252
=== ep: 1253, time 26.79086685180664, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1253
goal_identified
=== ep: 1254, time 26.94853115081787, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1254
goal_identified
=== ep: 1255, time 26.265907764434814, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1255
goal_identified
goal_identified
=== ep: 1256, time 26.635430812835693, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1256
goal_identified
=== ep: 1257, time 26.403078317642212, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1257
goal_identified
=== ep: 1258, time 26.728713512420654, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1258
goal_identified
=== ep: 1259, time 33.18881344795227, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1259
goal_identified
=== ep: 1260, time 26.404568195343018, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1260
goal_identified
goal_identified
=== ep: 1261, time 26.554931640625, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1261
goal_identified
goal_identified
=== ep: 1262, time 26.4235577583313, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1262
goal_identified
goal_identified
goal_identified
=== ep: 1263, time 26.62821912765503, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1263
=== ep: 1264, time 26.751869916915894, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1264
goal_identified
=== ep: 1265, time 26.222190856933594, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1265
goal_identified
=== ep: 1266, time 26.2630672454834, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1266
goal_identified
=== ep: 1267, time 26.61160159111023, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1267
goal_identified
=== ep: 1268, time 27.11866521835327, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1268
goal_identified
=== ep: 1269, time 32.23318600654602, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1269
goal_identified
=== ep: 1270, time 26.656758308410645, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1270
=== ep: 1271, time 26.720521211624146, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1271
goal_identified
=== ep: 1272, time 26.63578224182129, eps 0.001, sum reward: 1, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1272
goal_identified
=== ep: 1273, time 26.250703811645508, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1273
goal_identified
goal_identified
goal_identified
=== ep: 1274, time 26.735259294509888, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1274
goal_identified
=== ep: 1275, time 26.6108341217041, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1275
=== ep: 1276, time 26.52426266670227, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1276
goal_identified
=== ep: 1277, time 26.709251880645752, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1277
goal_identified
goal_identified
goal_identified
=== ep: 1278, time 26.577336311340332, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1278
goal_identified
goal_identified
=== ep: 1279, time 32.54808712005615, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1279
goal_identified
=== ep: 1280, time 26.698218822479248, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1280
goal_identified
=== ep: 1281, time 26.69999599456787, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1281
goal_identified
goal_identified
=== ep: 1282, time 26.38634443283081, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1282
goal_identified
goal_identified
goal_identified
=== ep: 1283, time 26.61249089241028, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1283
goal_identified
=== ep: 1284, time 26.922800302505493, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1284
=== ep: 1285, time 26.40972375869751, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1285
=== ep: 1286, time 26.702757596969604, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1286
goal_identified
=== ep: 1287, time 26.681675910949707, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1287
goal_identified
goal_identified
goal_identified
=== ep: 1288, time 26.647744178771973, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1288
=== ep: 1289, time 32.71519708633423, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1289
goal_identified
=== ep: 1290, time 26.911105155944824, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1290
=== ep: 1291, time 26.75573992729187, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1291
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1292, time 26.634313583374023, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1292
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1293, time 26.805286169052124, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1293
=== ep: 1294, time 26.967750072479248, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1294
goal_identified
=== ep: 1295, time 26.716889142990112, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1295
goal_identified
goal_identified
goal_identified
=== ep: 1296, time 26.619227409362793, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1296
goal_identified
goal_identified
=== ep: 1297, time 27.030211448669434, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1297
goal_identified
=== ep: 1298, time 26.83579969406128, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1298
goal_identified
=== ep: 1299, time 32.11925411224365, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1299
goal_identified
goal_identified
goal_identified
=== ep: 1300, time 26.674224615097046, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1300
goal_identified
goal_identified
=== ep: 1301, time 26.662487506866455, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1301
goal_identified
=== ep: 1302, time 26.72032856941223, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1302
goal_identified
=== ep: 1303, time 26.413183212280273, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1303
=== ep: 1304, time 26.758339405059814, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1304
=== ep: 1305, time 26.832358598709106, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1305
=== ep: 1306, time 26.734245777130127, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1306
goal_identified
=== ep: 1307, time 26.632020711898804, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1307
=== ep: 1308, time 26.74059224128723, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1308
goal_identified
goal_identified
=== ep: 1309, time 32.658613443374634, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1309
=== ep: 1310, time 26.94564127922058, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1310
=== ep: 1311, time 26.96115207672119, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1311
goal_identified
=== ep: 1312, time 25.97162938117981, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1312
goal_identified
=== ep: 1313, time 26.50678563117981, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1313
goal_identified
goal_identified
=== ep: 1314, time 26.511381149291992, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1314
goal_identified
=== ep: 1315, time 26.81714677810669, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1315
goal_identified
=== ep: 1316, time 26.387492418289185, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1316
=== ep: 1317, time 27.780558109283447, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1317
goal_identified
=== ep: 1318, time 26.97239065170288, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1318
goal_identified
=== ep: 1319, time 32.44598889350891, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1319
=== ep: 1320, time 26.456963300704956, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1320
goal_identified
goal_identified
=== ep: 1321, time 26.542304515838623, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1321
goal_identified
=== ep: 1322, time 26.628901958465576, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1322
=== ep: 1323, time 26.773303031921387, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1323
goal_identified
=== ep: 1324, time 26.8488929271698, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1324
goal_identified
goal_identified
=== ep: 1325, time 26.546919584274292, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1325
goal_identified
goal_identified
=== ep: 1326, time 26.4961941242218, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1326
goal_identified
goal_identified
=== ep: 1327, time 26.578515768051147, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1327
goal_identified
goal_identified
=== ep: 1328, time 26.424448251724243, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1328
goal_identified
goal_identified
=== ep: 1329, time 32.56738996505737, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1329
goal_identified
=== ep: 1330, time 26.497032642364502, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1330
=== ep: 1331, time 26.789897441864014, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1331
=== ep: 1332, time 26.92927074432373, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1332
goal_identified
=== ep: 1333, time 26.904186248779297, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1333
goal_identified
=== ep: 1334, time 26.49164605140686, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1334
goal_identified
=== ep: 1335, time 26.737589836120605, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1335
goal_identified
goal_identified
goal_identified
=== ep: 1336, time 26.525946855545044, eps 0.001, sum reward: 3, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1336
=== ep: 1337, time 26.914215326309204, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1337
=== ep: 1338, time 26.26115369796753, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1338
goal_identified
goal_identified
goal_identified
=== ep: 1339, time 31.090884685516357, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1339
=== ep: 1340, time 26.898065328598022, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1340
goal_identified
=== ep: 1341, time 26.529390811920166, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1341
goal_identified
goal_identified
=== ep: 1342, time 26.16400957107544, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1342
goal_identified
=== ep: 1343, time 26.164777040481567, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1343
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1344, time 26.168543815612793, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1344
=== ep: 1345, time 26.192877054214478, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1345
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1346, time 26.092503786087036, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1346
goal_identified
goal_identified
=== ep: 1347, time 26.581971406936646, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1347
goal_identified
goal_identified
goal_identified
=== ep: 1348, time 26.62717580795288, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1348
goal_identified
goal_identified
=== ep: 1349, time 30.820963382720947, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1349
goal_identified
goal_identified
=== ep: 1350, time 26.54454755783081, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1350
goal_identified
goal_identified
goal_identified
=== ep: 1351, time 26.083008527755737, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1351
=== ep: 1352, time 26.510135889053345, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1352
goal_identified
=== ep: 1353, time 26.246312618255615, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1353
goal_identified
goal_identified
goal_identified
=== ep: 1354, time 26.19464349746704, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1354
goal_identified
goal_identified
goal_identified
=== ep: 1355, time 26.303930044174194, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1355
=== ep: 1356, time 26.652299642562866, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1356
goal_identified
=== ep: 1357, time 26.716978549957275, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1357
=== ep: 1358, time 26.53995442390442, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1358
=== ep: 1359, time 31.123990774154663, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1359
goal_identified
=== ep: 1360, time 26.587153434753418, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1360
goal_identified
goal_identified
goal_identified
=== ep: 1361, time 26.201225519180298, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1361
=== ep: 1362, time 26.774705171585083, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1362
=== ep: 1363, time 26.68792963027954, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1363
goal_identified
goal_identified
=== ep: 1364, time 26.51516842842102, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1364
goal_identified
=== ep: 1365, time 26.109673261642456, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1365
goal_identified
=== ep: 1366, time 26.54337453842163, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1366
goal_identified
goal_identified
=== ep: 1367, time 26.529496669769287, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1367
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1368, time 26.292983293533325, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1368
goal_identified
=== ep: 1369, time 30.772587299346924, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1369
goal_identified
=== ep: 1370, time 26.574455738067627, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1370
goal_identified
=== ep: 1371, time 26.542309761047363, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1371
=== ep: 1372, time 26.82821226119995, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1372
=== ep: 1373, time 26.78272843360901, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1373
goal_identified
=== ep: 1374, time 26.392778396606445, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1374
goal_identified
=== ep: 1375, time 26.796351432800293, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1375
goal_identified
goal_identified
goal_identified
=== ep: 1376, time 26.528590202331543, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1376
goal_identified
goal_identified
goal_identified
=== ep: 1377, time 26.17690658569336, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1377
goal_identified
=== ep: 1378, time 26.589062929153442, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1378
goal_identified
goal_identified
goal_identified
=== ep: 1379, time 31.264289140701294, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1379
goal_identified
=== ep: 1380, time 26.67576241493225, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1380
goal_identified
goal_identified
=== ep: 1381, time 26.203689098358154, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1381
goal_identified
goal_identified
=== ep: 1382, time 27.065780639648438, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1382
goal_identified
goal_identified
=== ep: 1383, time 26.497639656066895, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1383
goal_identified
=== ep: 1384, time 26.450520992279053, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1384
goal_identified
goal_identified
=== ep: 1385, time 26.121445178985596, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1385
=== ep: 1386, time 26.49541711807251, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1386
goal_identified
=== ep: 1387, time 26.98857092857361, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1387
goal_identified
goal_identified
=== ep: 1388, time 26.336485862731934, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1388
goal_identified
=== ep: 1389, time 30.60411548614502, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1389
=== ep: 1390, time 27.22568154335022, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1390
goal_identified
goal_identified
=== ep: 1391, time 26.53538203239441, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1391
=== ep: 1392, time 26.227928161621094, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1392
goal_identified
goal_identified
=== ep: 1393, time 26.447187900543213, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1393
goal_identified
goal_identified
=== ep: 1394, time 26.360301733016968, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1394
goal_identified
=== ep: 1395, time 26.734501600265503, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1395
=== ep: 1396, time 26.579127311706543, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1396
goal_identified
=== ep: 1397, time 26.436671495437622, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1397
=== ep: 1398, time 26.30270552635193, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1398
goal_identified
=== ep: 1399, time 30.962535619735718, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1399
goal_identified
goal_identified
=== ep: 1400, time 26.6534743309021, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1400
goal_identified
=== ep: 1401, time 26.474520206451416, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1401
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1402, time 26.515541076660156, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1402
=== ep: 1403, time 26.440508127212524, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1403
goal_identified
=== ep: 1404, time 26.478115797042847, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1404
=== ep: 1405, time 26.562368154525757, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1405
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1406, time 26.407582998275757, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1406
goal_identified
=== ep: 1407, time 26.83236527442932, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1407
=== ep: 1408, time 26.609580755233765, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1408
goal_identified
goal_identified
=== ep: 1409, time 30.947349548339844, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1409
goal_identified
=== ep: 1410, time 26.504993438720703, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1410
goal_identified
=== ep: 1411, time 27.07868766784668, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1411
goal_identified
goal_identified
goal_identified
=== ep: 1412, time 26.579160928726196, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1412
goal_identified
goal_identified
goal_identified
=== ep: 1413, time 26.634575366973877, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1413
goal_identified
=== ep: 1414, time 26.517559051513672, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1414
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1415, time 26.242427110671997, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1415
goal_identified
goal_identified
=== ep: 1416, time 26.699519634246826, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1416
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1417, time 26.486467838287354, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1417
goal_identified
goal_identified
goal_identified
=== ep: 1418, time 26.44095540046692, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1418
=== ep: 1419, time 30.719736576080322, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1419
goal_identified
goal_identified
=== ep: 1420, time 26.689222812652588, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1420
goal_identified
goal_identified
=== ep: 1421, time 26.349283456802368, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1421
goal_identified
goal_identified
goal_identified
=== ep: 1422, time 26.808509826660156, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1422
goal_identified
=== ep: 1423, time 26.472788095474243, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1423
goal_identified
goal_identified
goal_identified
=== ep: 1424, time 26.311904191970825, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1424
goal_identified
=== ep: 1425, time 26.520312547683716, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1425
goal_identified
=== ep: 1426, time 26.400827169418335, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1426
goal_identified
goal_identified
=== ep: 1427, time 26.34662389755249, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1427
goal_identified
goal_identified
=== ep: 1428, time 26.5051748752594, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1428
goal_identified
goal_identified
=== ep: 1429, time 30.78058385848999, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1429
goal_identified
goal_identified
=== ep: 1430, time 26.172675132751465, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1430
goal_identified
=== ep: 1431, time 26.86497211456299, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1431
goal_identified
=== ep: 1432, time 26.715834379196167, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1432
goal_identified
=== ep: 1433, time 26.560002088546753, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1433
goal_identified
goal_identified
goal_identified
=== ep: 1434, time 26.765463829040527, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1434
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1435, time 26.52553129196167, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1435
goal_identified
goal_identified
goal_identified
=== ep: 1436, time 26.798335075378418, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1436
=== ep: 1437, time 26.198760747909546, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1437
goal_identified
goal_identified
=== ep: 1438, time 26.277167797088623, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1438
goal_identified
=== ep: 1439, time 30.56694722175598, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1439
goal_identified
goal_identified
goal_identified
=== ep: 1440, time 26.490779876708984, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1440
goal_identified
=== ep: 1441, time 26.40085458755493, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1441
goal_identified
goal_identified
=== ep: 1442, time 26.57850217819214, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1442
goal_identified
=== ep: 1443, time 26.357641220092773, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1443
goal_identified
goal_identified
=== ep: 1444, time 26.68534803390503, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1444
goal_identified
goal_identified
=== ep: 1445, time 26.249980688095093, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1445
goal_identified
=== ep: 1446, time 26.86290717124939, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1446
goal_identified
goal_identified
=== ep: 1447, time 26.561152458190918, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1447
=== ep: 1448, time 26.65773844718933, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1448
goal_identified
goal_identified
=== ep: 1449, time 31.229459285736084, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1449
goal_identified
goal_identified
goal_identified
=== ep: 1450, time 26.41857361793518, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1450
=== ep: 1451, time 26.840444564819336, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1451
goal_identified
goal_identified
=== ep: 1452, time 26.13226294517517, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1452
=== ep: 1453, time 26.380770683288574, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1453
goal_identified
=== ep: 1454, time 26.47380018234253, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1454
goal_identified
=== ep: 1455, time 26.57440161705017, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1455
goal_identified
=== ep: 1456, time 26.571404218673706, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1456
goal_identified
goal_identified
goal_identified
=== ep: 1457, time 26.521597146987915, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1457
=== ep: 1458, time 26.594061613082886, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1458
goal_identified
=== ep: 1459, time 30.8509304523468, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1459
goal_identified
goal_identified
goal_identified
=== ep: 1460, time 26.51301860809326, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1460
goal_identified
=== ep: 1461, time 26.459558725357056, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1461
goal_identified
=== ep: 1462, time 26.690298795700073, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1462
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1463, time 26.727054834365845, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1463
goal_identified
=== ep: 1464, time 26.189971446990967, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1464
goal_identified
=== ep: 1465, time 26.20941662788391, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1465
goal_identified
=== ep: 1466, time 26.262442111968994, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1466
goal_identified
goal_identified
=== ep: 1467, time 26.42593026161194, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1467
goal_identified
goal_identified
=== ep: 1468, time 26.232665061950684, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1468
goal_identified
=== ep: 1469, time 31.23779273033142, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1469
=== ep: 1470, time 26.662879467010498, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1470
goal_identified
goal_identified
=== ep: 1471, time 26.29649519920349, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1471
goal_identified
=== ep: 1472, time 26.69010591506958, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1472
goal_identified
goal_identified
goal_identified
=== ep: 1473, time 26.285382509231567, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1473
goal_identified
goal_identified
=== ep: 1474, time 26.647257804870605, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1474
goal_identified
goal_identified
goal_identified
=== ep: 1475, time 26.419811725616455, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1475
goal_identified
=== ep: 1476, time 26.392672777175903, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1476
goal_identified
goal_identified
goal_identified
=== ep: 1477, time 26.50676202774048, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1477
goal_identified
goal_identified
goal_identified
=== ep: 1478, time 26.519081354141235, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1478
goal_identified
goal_identified
=== ep: 1479, time 31.222017526626587, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1479
goal_identified
goal_identified
goal_identified
=== ep: 1480, time 26.269121170043945, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1480
goal_identified
goal_identified
=== ep: 1481, time 26.245506048202515, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1481
goal_identified
goal_identified
goal_identified
=== ep: 1482, time 26.734030961990356, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1482
=== ep: 1483, time 26.699193477630615, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1483
goal_identified
=== ep: 1484, time 26.498926162719727, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1484
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1485, time 26.700445652008057, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1485
goal_identified
=== ep: 1486, time 26.383023023605347, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1486
goal_identified
goal_identified
=== ep: 1487, time 26.69707679748535, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1487
goal_identified
=== ep: 1488, time 26.03276491165161, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1488
goal_identified
goal_identified
=== ep: 1489, time 30.804513454437256, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1489
=== ep: 1490, time 26.932507753372192, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1490
goal_identified
goal_identified
=== ep: 1491, time 26.48635506629944, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1491
goal_identified
goal_identified
=== ep: 1492, time 26.355037927627563, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1492
=== ep: 1493, time 26.340392112731934, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1493
=== ep: 1494, time 26.58915066719055, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1494
goal_identified
=== ep: 1495, time 26.510326147079468, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1495
goal_identified
=== ep: 1496, time 26.439063549041748, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1496
goal_identified
goal_identified
goal_identified
=== ep: 1497, time 26.400874853134155, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1497
=== ep: 1498, time 26.677860736846924, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1498
=== ep: 1499, time 30.588337898254395, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1499
=== ep: 1500, time 26.427775144577026, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1500
=== ep: 1501, time 26.651909112930298, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1501
goal_identified
=== ep: 1502, time 26.620730876922607, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1502
goal_identified
goal_identified
goal_identified
=== ep: 1503, time 26.560362577438354, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1503
goal_identified
goal_identified
=== ep: 1504, time 26.913712978363037, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1504
=== ep: 1505, time 26.776233911514282, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1505
goal_identified
goal_identified
=== ep: 1506, time 26.67236590385437, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1506
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1507, time 26.161558628082275, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 589
goal_identified
=== ep: 1508, time 26.850150108337402, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1508
goal_identified
goal_identified
goal_identified
=== ep: 1509, time 31.12090563774109, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1509
goal_identified
=== ep: 1510, time 26.80988335609436, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1510
goal_identified
goal_identified
goal_identified
=== ep: 1511, time 26.83352279663086, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1511
goal_identified
=== ep: 1512, time 26.519549131393433, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1512
goal_identified
=== ep: 1513, time 26.385311365127563, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1513
goal_identified
goal_identified
=== ep: 1514, time 26.761629581451416, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1514
=== ep: 1515, time 26.670220851898193, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1515
goal_identified
=== ep: 1516, time 26.99323534965515, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1516
goal_identified
=== ep: 1517, time 26.67455267906189, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1517
=== ep: 1518, time 26.69079065322876, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1518
goal_identified
goal_identified
=== ep: 1519, time 31.196032524108887, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1519
goal_identified
goal_identified
=== ep: 1520, time 26.96328091621399, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1520
goal_identified
=== ep: 1521, time 26.615066528320312, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1521
goal_identified
goal_identified
=== ep: 1522, time 26.64507222175598, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1522
=== ep: 1523, time 26.5441997051239, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1523
=== ep: 1524, time 26.695831298828125, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1524
=== ep: 1525, time 26.570260047912598, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1525
goal_identified
=== ep: 1526, time 26.364542961120605, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1526
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1527, time 26.527918338775635, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 769
=== ep: 1528, time 26.814133644104004, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1528
goal_identified
=== ep: 1529, time 31.676552534103394, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1529
goal_identified
goal_identified
goal_identified
=== ep: 1530, time 26.508284330368042, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1530
goal_identified
=== ep: 1531, time 26.72009587287903, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1531
goal_identified
=== ep: 1532, time 26.6300106048584, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1532
goal_identified
=== ep: 1533, time 26.609554052352905, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1533
goal_identified
goal_identified
goal_identified
=== ep: 1534, time 26.848347425460815, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1534
goal_identified
=== ep: 1535, time 27.104204416275024, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1535
goal_identified
=== ep: 1536, time 26.395304441452026, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1536
goal_identified
goal_identified
=== ep: 1537, time 26.447720050811768, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1537
goal_identified
=== ep: 1538, time 26.215923309326172, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1538
goal_identified
=== ep: 1539, time 31.191686153411865, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1539
goal_identified
=== ep: 1540, time 26.299875736236572, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1540
goal_identified
goal_identified
=== ep: 1541, time 26.51933479309082, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1541
goal_identified
=== ep: 1542, time 26.258625030517578, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1542
=== ep: 1543, time 26.850694179534912, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1543
goal_identified
goal_identified
=== ep: 1544, time 26.518942832946777, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1544
goal_identified
=== ep: 1545, time 26.708080053329468, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1545
=== ep: 1546, time 26.52976417541504, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1546
goal_identified
goal_identified
=== ep: 1547, time 26.389477968215942, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1547
goal_identified
goal_identified
=== ep: 1548, time 26.353881359100342, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1548
goal_identified
=== ep: 1549, time 31.7404522895813, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1549
=== ep: 1550, time 26.349019050598145, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1550
goal_identified
=== ep: 1551, time 26.276915073394775, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1551
goal_identified
goal_identified
goal_identified
=== ep: 1552, time 26.57208013534546, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1552
goal_identified
goal_identified
=== ep: 1553, time 26.424505472183228, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1553
goal_identified
goal_identified
goal_identified
=== ep: 1554, time 26.38167643547058, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1554
=== ep: 1555, time 26.396602392196655, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1555
goal_identified
goal_identified
goal_identified
=== ep: 1556, time 26.766657829284668, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1556
=== ep: 1557, time 26.609776496887207, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1557
goal_identified
goal_identified
=== ep: 1558, time 26.04552960395813, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1558
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1559, time 31.000470638275146, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1559
goal_identified
=== ep: 1560, time 26.796258687973022, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1560
goal_identified
goal_identified
goal_identified
=== ep: 1561, time 26.377819776535034, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1561
goal_identified
=== ep: 1562, time 26.599093437194824, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1562
goal_identified
goal_identified
goal_identified
=== ep: 1563, time 26.60169267654419, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1563
goal_identified
goal_identified
=== ep: 1564, time 26.53165555000305, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1564
=== ep: 1565, time 26.691288709640503, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1565
=== ep: 1566, time 26.589638233184814, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1566
=== ep: 1567, time 26.94620633125305, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1567
goal_identified
goal_identified
goal_identified
=== ep: 1568, time 26.47406029701233, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1568
goal_identified
=== ep: 1569, time 30.985431671142578, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1569
goal_identified
goal_identified
goal_identified
=== ep: 1570, time 26.840081930160522, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1570
goal_identified
=== ep: 1571, time 26.214507818222046, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1571
goal_identified
=== ep: 1572, time 26.84183382987976, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1572
goal_identified
goal_identified
goal_identified
=== ep: 1573, time 26.79942798614502, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1573
goal_identified
=== ep: 1574, time 26.593281507492065, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1574
=== ep: 1575, time 26.886412620544434, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1575
goal_identified
goal_identified
=== ep: 1576, time 26.439955234527588, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1576
=== ep: 1577, time 26.793951511383057, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1577
=== ep: 1578, time 26.249065160751343, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1578
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1579, time 31.20937991142273, eps 0.001, sum reward: 7, score_diff 7, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 806
=== ep: 1580, time 27.079715967178345, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1580
goal_identified
=== ep: 1581, time 27.144273042678833, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1581
goal_identified
goal_identified
=== ep: 1582, time 26.83673930168152, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1582
goal_identified
goal_identified
=== ep: 1583, time 26.227370262145996, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1583
goal_identified
goal_identified
goal_identified
=== ep: 1584, time 27.098220348358154, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1584
goal_identified
goal_identified
=== ep: 1585, time 26.39969038963318, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1585
goal_identified
goal_identified
=== ep: 1586, time 26.427944898605347, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1586
=== ep: 1587, time 26.540575981140137, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1587
goal_identified
=== ep: 1588, time 26.349729537963867, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1588
goal_identified
goal_identified
=== ep: 1589, time 31.39247488975525, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1589
=== ep: 1590, time 26.824356079101562, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1590
=== ep: 1591, time 26.69393515586853, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1591
=== ep: 1592, time 26.64474081993103, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1592
goal_identified
goal_identified
=== ep: 1593, time 26.30893850326538, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1593
goal_identified
goal_identified
goal_identified
=== ep: 1594, time 26.55585026741028, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1594
goal_identified
=== ep: 1595, time 26.55163049697876, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1595
=== ep: 1596, time 26.570663690567017, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1596
goal_identified
goal_identified
=== ep: 1597, time 26.565715074539185, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1597
=== ep: 1598, time 26.733651161193848, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1598
goal_identified
=== ep: 1599, time 31.58678674697876, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1599
=== ep: 1600, time 26.86499261856079, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1600
goal_identified
=== ep: 1601, time 26.402421951293945, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1601
goal_identified
goal_identified
goal_identified
=== ep: 1602, time 26.37150001525879, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1602
goal_identified
goal_identified
=== ep: 1603, time 26.991158723831177, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1603
=== ep: 1604, time 26.756251573562622, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1604
goal_identified
=== ep: 1605, time 26.593308687210083, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1605
goal_identified
goal_identified
goal_identified
=== ep: 1606, time 26.530616760253906, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1606
goal_identified
=== ep: 1607, time 26.913487434387207, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1607
goal_identified
goal_identified
=== ep: 1608, time 26.098537921905518, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1608
goal_identified
goal_identified
=== ep: 1609, time 31.012374877929688, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1609
=== ep: 1610, time 26.78328776359558, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1610
goal_identified
goal_identified
=== ep: 1611, time 26.652432918548584, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1611
goal_identified
=== ep: 1612, time 26.717697858810425, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1612
goal_identified
=== ep: 1613, time 26.13767385482788, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1613
goal_identified
=== ep: 1614, time 26.91455364227295, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1614
=== ep: 1615, time 26.20850920677185, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1615
=== ep: 1616, time 27.119366884231567, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1616
=== ep: 1617, time 26.346444606781006, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1617
=== ep: 1618, time 26.12514090538025, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1618
goal_identified
goal_identified
goal_identified
=== ep: 1619, time 31.478885889053345, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1619
goal_identified
goal_identified
goal_identified
=== ep: 1620, time 26.446918487548828, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1620
=== ep: 1621, time 26.672680139541626, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1621
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1622, time 26.773049354553223, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1622
goal_identified
=== ep: 1623, time 27.061500549316406, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1623
goal_identified
goal_identified
=== ep: 1624, time 26.46353769302368, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1624
=== ep: 1625, time 26.75796890258789, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1625
goal_identified
goal_identified
=== ep: 1626, time 26.846667051315308, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1626
goal_identified
goal_identified
=== ep: 1627, time 26.53212261199951, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1627
goal_identified
=== ep: 1628, time 26.85218596458435, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1628
goal_identified
goal_identified
=== ep: 1629, time 31.205365419387817, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1629
goal_identified
goal_identified
goal_identified
=== ep: 1630, time 26.39500617980957, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1630
goal_identified
goal_identified
=== ep: 1631, time 26.101996421813965, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1631
goal_identified
goal_identified
=== ep: 1632, time 26.522618770599365, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1632
goal_identified
goal_identified
=== ep: 1633, time 26.683882236480713, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1633
goal_identified
goal_identified
=== ep: 1634, time 26.527743816375732, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1634
goal_identified
goal_identified
=== ep: 1635, time 26.66175675392151, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1635
goal_identified
=== ep: 1636, time 26.310089826583862, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1636
goal_identified
=== ep: 1637, time 26.705803871154785, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1637
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1638, time 26.414454460144043, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1638
goal_identified
goal_identified
goal_identified
=== ep: 1639, time 31.69568133354187, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1639
goal_identified
goal_identified
goal_identified
=== ep: 1640, time 26.290088415145874, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1640
=== ep: 1641, time 26.669921398162842, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1641
goal_identified
goal_identified
=== ep: 1642, time 26.562292337417603, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1642
=== ep: 1643, time 26.437230348587036, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1643
=== ep: 1644, time 26.57538914680481, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1644
goal_identified
=== ep: 1645, time 26.342668771743774, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1645
goal_identified
goal_identified
=== ep: 1646, time 26.50338840484619, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1646
=== ep: 1647, time 26.726154088974, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1647
=== ep: 1648, time 26.727118968963623, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1648
goal_identified
goal_identified
=== ep: 1649, time 31.382436990737915, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1649
goal_identified
goal_identified
=== ep: 1650, time 26.523627758026123, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1650
=== ep: 1651, time 26.62180805206299, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1651
goal_identified
goal_identified
goal_identified
=== ep: 1652, time 26.29822301864624, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1652
goal_identified
=== ep: 1653, time 26.617583513259888, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1653
goal_identified
goal_identified
=== ep: 1654, time 26.44210648536682, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1654
goal_identified
=== ep: 1655, time 26.9481041431427, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1655
goal_identified
goal_identified
=== ep: 1656, time 26.524845123291016, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1656
goal_identified
=== ep: 1657, time 26.48254704475403, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1657
goal_identified
goal_identified
goal_identified
=== ep: 1658, time 26.585872411727905, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1658
goal_identified
=== ep: 1659, time 31.017214059829712, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1659
goal_identified
goal_identified
goal_identified
=== ep: 1660, time 26.54126811027527, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1660
=== ep: 1661, time 26.86546039581299, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1661
goal_identified
=== ep: 1662, time 26.431509256362915, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1662
goal_identified
=== ep: 1663, time 26.429834604263306, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1663
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1664, time 26.457383394241333, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1664
=== ep: 1665, time 26.4937801361084, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1665
goal_identified
=== ep: 1666, time 26.449340105056763, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1666
=== ep: 1667, time 26.939446210861206, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1667
goal_identified
goal_identified
goal_identified
=== ep: 1668, time 26.27393651008606, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1668
goal_identified
=== ep: 1669, time 31.0966374874115, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1669
goal_identified
goal_identified
=== ep: 1670, time 26.597174882888794, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1670
=== ep: 1671, time 26.89856481552124, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1671
goal_identified
=== ep: 1672, time 26.4420485496521, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1672
goal_identified
=== ep: 1673, time 26.356417894363403, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1673
=== ep: 1674, time 26.284658908843994, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1674
=== ep: 1675, time 26.59850573539734, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1675
goal_identified
goal_identified
goal_identified
=== ep: 1676, time 26.50938582420349, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1676
goal_identified
goal_identified
goal_identified
=== ep: 1677, time 26.633379459381104, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1677
goal_identified
goal_identified
=== ep: 1678, time 26.663703680038452, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1678
goal_identified
=== ep: 1679, time 30.970818281173706, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1679
=== ep: 1680, time 26.468945264816284, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1680
goal_identified
=== ep: 1681, time 26.48968744277954, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1681
goal_identified
goal_identified
=== ep: 1682, time 26.219262838363647, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1682
goal_identified
goal_identified
=== ep: 1683, time 26.856095790863037, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1683
=== ep: 1684, time 26.546496152877808, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1684
=== ep: 1685, time 26.35125970840454, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1685
=== ep: 1686, time 26.601988315582275, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1686
goal_identified
=== ep: 1687, time 26.54199767112732, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1687
goal_identified
goal_identified
=== ep: 1688, time 26.509562015533447, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1688
goal_identified
goal_identified
goal_identified
=== ep: 1689, time 31.6560161113739, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1689
goal_identified
goal_identified
goal_identified
=== ep: 1690, time 26.68541407585144, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1690
goal_identified
goal_identified
=== ep: 1691, time 26.47933602333069, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1691
goal_identified
=== ep: 1692, time 26.435742616653442, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1692
goal_identified
goal_identified
goal_identified
=== ep: 1693, time 26.367326259613037, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1693
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1694, time 26.477128505706787, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1694
=== ep: 1695, time 26.394604206085205, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1695
=== ep: 1696, time 26.27988290786743, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1696
goal_identified
goal_identified
=== ep: 1697, time 26.41530442237854, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1697
goal_identified
=== ep: 1698, time 26.231950044631958, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1698
goal_identified
=== ep: 1699, time 31.318292140960693, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1699
goal_identified
=== ep: 1700, time 26.44206166267395, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1700
goal_identified
goal_identified
goal_identified
=== ep: 1701, time 26.48448371887207, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1701
goal_identified
goal_identified
=== ep: 1702, time 26.38031816482544, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1702
goal_identified
goal_identified
=== ep: 1703, time 26.791120529174805, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1703
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1704, time 26.443671464920044, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1704
goal_identified
=== ep: 1705, time 26.567075967788696, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1705
=== ep: 1706, time 26.911882877349854, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1706
=== ep: 1707, time 26.854671478271484, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1707
goal_identified
=== ep: 1708, time 26.71010971069336, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1708
=== ep: 1709, time 31.169797658920288, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1709
goal_identified
goal_identified
=== ep: 1710, time 26.148606777191162, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1710
goal_identified
=== ep: 1711, time 26.898955583572388, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1711
goal_identified
=== ep: 1712, time 26.489336490631104, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1712
goal_identified
goal_identified
=== ep: 1713, time 26.356003761291504, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1713
goal_identified
goal_identified
=== ep: 1714, time 26.41613483428955, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1714
goal_identified
=== ep: 1715, time 26.399402141571045, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1715
goal_identified
=== ep: 1716, time 26.643258333206177, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1716
goal_identified
goal_identified
=== ep: 1717, time 26.318756341934204, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1717
goal_identified
=== ep: 1718, time 26.323656797409058, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1718
goal_identified
=== ep: 1719, time 31.127745389938354, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1719
goal_identified
=== ep: 1720, time 26.3351092338562, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1720
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1721, time 26.415446996688843, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1721
goal_identified
=== ep: 1722, time 26.83715057373047, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1722
goal_identified
goal_identified
=== ep: 1723, time 26.48075580596924, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1723
=== ep: 1724, time 26.30129051208496, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1724
goal_identified
=== ep: 1725, time 26.54825448989868, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1725
goal_identified
goal_identified
goal_identified
=== ep: 1726, time 26.565985918045044, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1726
goal_identified
=== ep: 1727, time 26.542632579803467, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1727
=== ep: 1728, time 26.653987169265747, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1728
=== ep: 1729, time 31.14157509803772, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1729
goal_identified
goal_identified
=== ep: 1730, time 26.524572134017944, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1730
goal_identified
goal_identified
goal_identified
=== ep: 1731, time 26.984891176223755, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1731
=== ep: 1732, time 26.491042375564575, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1732
goal_identified
=== ep: 1733, time 26.307324647903442, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1733
=== ep: 1734, time 26.705043077468872, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1734
goal_identified
goal_identified
goal_identified
=== ep: 1735, time 26.60180139541626, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1735
goal_identified
=== ep: 1736, time 26.47507119178772, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1736
goal_identified
=== ep: 1737, time 26.374835968017578, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1737
goal_identified
goal_identified
goal_identified
=== ep: 1738, time 26.548198461532593, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1738
goal_identified
goal_identified
goal_identified
=== ep: 1739, time 31.190917491912842, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1739
goal_identified
goal_identified
=== ep: 1740, time 26.551673889160156, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1740
goal_identified
=== ep: 1741, time 26.067166805267334, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1741
goal_identified
=== ep: 1742, time 26.396576166152954, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1742
goal_identified
=== ep: 1743, time 26.09999704360962, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1743
goal_identified
=== ep: 1744, time 26.340091943740845, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1744
=== ep: 1745, time 26.5725998878479, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1745
goal_identified
goal_identified
goal_identified
=== ep: 1746, time 26.531824588775635, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1746
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1747, time 26.732867002487183, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1747
goal_identified
goal_identified
=== ep: 1748, time 26.490649700164795, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1748
=== ep: 1749, time 31.573731184005737, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1749
goal_identified
=== ep: 1750, time 26.112736225128174, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1750
goal_identified
=== ep: 1751, time 26.35053324699402, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1751
goal_identified
=== ep: 1752, time 26.24340510368347, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1752
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1753, time 26.560240745544434, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1016
=== ep: 1754, time 26.728036880493164, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1754
goal_identified
goal_identified
=== ep: 1755, time 26.745799779891968, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1755
goal_identified
=== ep: 1756, time 26.418351411819458, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1756
goal_identified
goal_identified
=== ep: 1757, time 26.64280676841736, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1757
goal_identified
=== ep: 1758, time 26.973323345184326, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1758
goal_identified
=== ep: 1759, time 31.49792170524597, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1759
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1760, time 26.46632146835327, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1760
goal_identified
goal_identified
=== ep: 1761, time 26.29242753982544, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1761
=== ep: 1762, time 27.282134771347046, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1762
goal_identified
=== ep: 1763, time 26.354079723358154, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1763
goal_identified
=== ep: 1764, time 26.608415603637695, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1764
=== ep: 1765, time 26.819743156433105, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1765
=== ep: 1766, time 26.313663482666016, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1766
goal_identified
goal_identified
=== ep: 1767, time 26.522560834884644, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1767
goal_identified
=== ep: 1768, time 26.495410680770874, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1768
goal_identified
=== ep: 1769, time 31.349224090576172, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1769
goal_identified
=== ep: 1770, time 26.3407142162323, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1770
goal_identified
goal_identified
goal_identified
=== ep: 1771, time 26.862475395202637, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1771
goal_identified
goal_identified
=== ep: 1772, time 26.28638768196106, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1772
=== ep: 1773, time 26.5325608253479, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1773
=== ep: 1774, time 26.314369440078735, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1774
goal_identified
goal_identified
=== ep: 1775, time 26.39112377166748, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1775
goal_identified
=== ep: 1776, time 26.219674825668335, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1776
=== ep: 1777, time 26.426241397857666, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1777
goal_identified
=== ep: 1778, time 26.85620903968811, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1778
goal_identified
goal_identified
goal_identified
=== ep: 1779, time 31.19174289703369, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1779
=== ep: 1780, time 26.600991249084473, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1780
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1781, time 26.47626805305481, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1781
goal_identified
=== ep: 1782, time 26.82642936706543, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1782
goal_identified
goal_identified
goal_identified
=== ep: 1783, time 26.7570161819458, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1783
goal_identified
goal_identified
=== ep: 1784, time 26.406808614730835, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1784
goal_identified
goal_identified
=== ep: 1785, time 26.517879724502563, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1785
goal_identified
goal_identified
=== ep: 1786, time 26.395548820495605, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1786
goal_identified
goal_identified
=== ep: 1787, time 26.297088623046875, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1787
goal_identified
goal_identified
=== ep: 1788, time 26.59129571914673, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1788
goal_identified
goal_identified
goal_identified
=== ep: 1789, time 31.475375175476074, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1789
goal_identified
=== ep: 1790, time 26.77706289291382, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1790
goal_identified
goal_identified
goal_identified
=== ep: 1791, time 26.72119927406311, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1791
goal_identified
=== ep: 1792, time 26.726341485977173, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1792
goal_identified
=== ep: 1793, time 26.558173656463623, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1793
goal_identified
goal_identified
=== ep: 1794, time 26.614930152893066, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1794
=== ep: 1795, time 26.72798776626587, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1795
goal_identified
=== ep: 1796, time 26.386581420898438, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1796
=== ep: 1797, time 26.931787729263306, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1797
goal_identified
goal_identified
goal_identified
=== ep: 1798, time 26.70001769065857, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1798
goal_identified
=== ep: 1799, time 31.58131241798401, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1799
goal_identified
=== ep: 1800, time 26.950507640838623, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1800
goal_identified
=== ep: 1801, time 26.967103719711304, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1801
goal_identified
=== ep: 1802, time 26.49908709526062, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1802
goal_identified
goal_identified
=== ep: 1803, time 26.41740870475769, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1803
goal_identified
=== ep: 1804, time 26.48139500617981, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1804
goal_identified
=== ep: 1805, time 26.169028997421265, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1805
=== ep: 1806, time 26.764025688171387, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1806
goal_identified
goal_identified
=== ep: 1807, time 26.891160249710083, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1807
goal_identified
goal_identified
goal_identified
=== ep: 1808, time 26.576672554016113, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1808
goal_identified
=== ep: 1809, time 31.11465549468994, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1809
goal_identified
goal_identified
=== ep: 1810, time 26.320005655288696, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1810
=== ep: 1811, time 27.021929502487183, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1811
=== ep: 1812, time 26.71256732940674, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1812
=== ep: 1813, time 26.252896070480347, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1813
=== ep: 1814, time 26.596068143844604, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1814
goal_identified
=== ep: 1815, time 26.77941131591797, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1815
goal_identified
goal_identified
=== ep: 1816, time 26.937642335891724, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1816
goal_identified
goal_identified
=== ep: 1817, time 26.611470699310303, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 128/128)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1817
goal_identified
goal_identified
goal_identified
=== ep: 1818, time 26.413071393966675, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1818
goal_identified
=== ep: 1819, time 31.32185411453247, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1819
goal_identified
=== ep: 1820, time 26.47597622871399, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1820
goal_identified
goal_identified
=== ep: 1821, time 26.629245281219482, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1821
goal_identified
=== ep: 1822, time 26.538365840911865, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1822
=== ep: 1823, time 26.664930820465088, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1823
goal_identified
goal_identified
goal_identified
=== ep: 1824, time 26.59538769721985, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1824
goal_identified
=== ep: 1825, time 26.756997108459473, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1825
goal_identified
goal_identified
goal_identified
=== ep: 1826, time 26.560993671417236, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1826
goal_identified
goal_identified
=== ep: 1827, time 26.764301538467407, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1827
=== ep: 1828, time 27.041076183319092, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1828
goal_identified
=== ep: 1829, time 31.259660005569458, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1829
goal_identified
goal_identified
=== ep: 1830, time 26.53454065322876, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1830
goal_identified
=== ep: 1831, time 27.060463428497314, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1831
=== ep: 1832, time 26.86902356147766, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1832
goal_identified
=== ep: 1833, time 26.601983308792114, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1833
goal_identified
=== ep: 1834, time 26.590664386749268, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1834
=== ep: 1835, time 26.684932708740234, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1835
goal_identified
goal_identified
goal_identified
=== ep: 1836, time 26.302741765975952, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1836
goal_identified
goal_identified
goal_identified
=== ep: 1837, time 26.64484477043152, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1837
goal_identified
=== ep: 1838, time 26.438910484313965, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1838
=== ep: 1839, time 31.709040641784668, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1839
=== ep: 1840, time 26.994617223739624, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1840
goal_identified
=== ep: 1841, time 26.434108018875122, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1841
goal_identified
=== ep: 1842, time 26.55646300315857, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1842
goal_identified
=== ep: 1843, time 26.799623012542725, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1843
=== ep: 1844, time 26.336089611053467, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1844
goal_identified
goal_identified
goal_identified
=== ep: 1845, time 26.723360061645508, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1845
=== ep: 1846, time 27.19110369682312, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1846
goal_identified
=== ep: 1847, time 26.783555030822754, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1847
goal_identified
=== ep: 1848, time 26.467755794525146, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1848
=== ep: 1849, time 31.15469789505005, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1849
goal_identified
=== ep: 1850, time 26.60353684425354, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1850
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1851, time 26.497249364852905, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1851
=== ep: 1852, time 26.408077239990234, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1852
goal_identified
goal_identified
=== ep: 1853, time 26.849020957946777, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1853
goal_identified
goal_identified
=== ep: 1854, time 26.463900804519653, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1854
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1855, time 26.551819801330566, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1855
goal_identified
goal_identified
goal_identified
=== ep: 1856, time 26.403044939041138, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1856
=== ep: 1857, time 26.55788254737854, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1857
goal_identified
=== ep: 1858, time 26.711118936538696, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1858
goal_identified
=== ep: 1859, time 31.24259090423584, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1859
goal_identified
=== ep: 1860, time 26.486279726028442, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1860
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1861, time 26.6091046333313, eps 0.001, sum reward: 6, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1049
goal_identified
=== ep: 1862, time 26.965030908584595, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1862
=== ep: 1863, time 26.926612377166748, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1863
goal_identified
=== ep: 1864, time 26.58176279067993, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1864
=== ep: 1865, time 26.528253078460693, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1865
goal_identified
goal_identified
=== ep: 1866, time 26.892629623413086, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1866
goal_identified
goal_identified
=== ep: 1867, time 26.570632696151733, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1867
=== ep: 1868, time 26.656917572021484, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1868
goal_identified
=== ep: 1869, time 31.405073642730713, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1869
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1870, time 26.537948608398438, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1116
goal_identified
=== ep: 1871, time 26.80133056640625, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1871
goal_identified
goal_identified
=== ep: 1872, time 27.159477472305298, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1872
goal_identified
goal_identified
=== ep: 1873, time 26.374890327453613, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1873
goal_identified
=== ep: 1874, time 26.521318912506104, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1874
goal_identified
=== ep: 1875, time 26.66362738609314, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1875
goal_identified
=== ep: 1876, time 26.65809917449951, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1876
goal_identified
goal_identified
=== ep: 1877, time 26.623160123825073, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1877
goal_identified
=== ep: 1878, time 26.609022617340088, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1878
goal_identified
goal_identified
goal_identified
=== ep: 1879, time 31.819028615951538, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1879
goal_identified
=== ep: 1880, time 26.89370608329773, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1880
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1881, time 26.529736757278442, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1881
goal_identified
=== ep: 1882, time 26.358272790908813, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1882
goal_identified
goal_identified
=== ep: 1883, time 26.595337867736816, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1883
goal_identified
goal_identified
=== ep: 1884, time 27.108805418014526, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1884
goal_identified
goal_identified
goal_identified
=== ep: 1885, time 26.596973180770874, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1885
goal_identified
goal_identified
=== ep: 1886, time 26.81927466392517, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1886
goal_identified
goal_identified
=== ep: 1887, time 26.573025465011597, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1887
goal_identified
goal_identified
=== ep: 1888, time 26.636080503463745, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1888
goal_identified
goal_identified
=== ep: 1889, time 31.633549213409424, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1889
goal_identified
goal_identified
goal_identified
=== ep: 1890, time 26.56688904762268, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1890
goal_identified
goal_identified
goal_identified
=== ep: 1891, time 26.377349138259888, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1891
goal_identified
=== ep: 1892, time 26.658020496368408, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1892
goal_identified
=== ep: 1893, time 26.661489963531494, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1893
goal_identified
goal_identified
goal_identified
=== ep: 1894, time 26.34651231765747, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1894
=== ep: 1895, time 26.68509292602539, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1895
=== ep: 1896, time 26.803868293762207, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1896
goal_identified
=== ep: 1897, time 26.642534971237183, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1897
goal_identified
=== ep: 1898, time 26.618178367614746, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1898
goal_identified
=== ep: 1899, time 31.374531030654907, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1899
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1900, time 26.330118417739868, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1900
=== ep: 1901, time 26.61479377746582, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1901
goal_identified
goal_identified
goal_identified
=== ep: 1902, time 26.82946515083313, eps 0.001, sum reward: 3, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1902
goal_identified
goal_identified
=== ep: 1903, time 26.428500175476074, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1903
goal_identified
=== ep: 1904, time 26.375080823898315, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1904
=== ep: 1905, time 26.513317346572876, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1905
goal_identified
goal_identified
goal_identified
=== ep: 1906, time 26.479719400405884, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1906
goal_identified
goal_identified
=== ep: 1907, time 26.347544193267822, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1907
goal_identified
goal_identified
=== ep: 1908, time 26.72926163673401, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1908
goal_identified
goal_identified
=== ep: 1909, time 31.48077154159546, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1909
=== ep: 1910, time 26.771854400634766, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1910
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1911, time 26.301312923431396, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1911
goal_identified
=== ep: 1912, time 26.739156484603882, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1912
goal_identified
=== ep: 1913, time 26.592521905899048, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1913
goal_identified
goal_identified
=== ep: 1914, time 26.596800804138184, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1914
goal_identified
goal_identified
goal_identified
=== ep: 1915, time 26.494359254837036, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1915
=== ep: 1916, time 29.648011922836304, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1916
goal_identified
goal_identified
goal_identified
=== ep: 1917, time 26.48069930076599, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1917
=== ep: 1918, time 26.414002656936646, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1918
goal_identified
=== ep: 1919, time 31.482709646224976, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1919
goal_identified
goal_identified
=== ep: 1920, time 26.381200075149536, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1920
goal_identified
=== ep: 1921, time 26.231396436691284, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1921
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1922, time 26.400700092315674, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1922
goal_identified
=== ep: 1923, time 26.40989327430725, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1923
goal_identified
goal_identified
goal_identified
=== ep: 1924, time 26.22730565071106, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1924
goal_identified
=== ep: 1925, time 26.77307963371277, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1925
goal_identified
goal_identified
=== ep: 1926, time 26.197582721710205, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1926
goal_identified
goal_identified
=== ep: 1927, time 26.83455181121826, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1927
=== ep: 1928, time 27.168421268463135, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1928
goal_identified
=== ep: 1929, time 31.561616897583008, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1929
=== ep: 1930, time 26.789655447006226, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1930
goal_identified
goal_identified
=== ep: 1931, time 26.55308961868286, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1931
goal_identified
goal_identified
=== ep: 1932, time 26.748518705368042, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1932
goal_identified
=== ep: 1933, time 26.59484601020813, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1933
goal_identified
goal_identified
=== ep: 1934, time 26.75926184654236, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1934
goal_identified
=== ep: 1935, time 26.56854772567749, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1935
goal_identified
goal_identified
=== ep: 1936, time 26.833441019058228, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1936
goal_identified
goal_identified
goal_identified
=== ep: 1937, time 26.777608156204224, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1937
goal_identified
=== ep: 1938, time 26.72656536102295, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1938
goal_identified
=== ep: 1939, time 31.43386673927307, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1939
goal_identified
goal_identified
goal_identified
=== ep: 1940, time 26.8098361492157, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1940
goal_identified
goal_identified
goal_identified
=== ep: 1941, time 26.541815280914307, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1941
goal_identified
=== ep: 1942, time 26.88244891166687, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1942
goal_identified
goal_identified
goal_identified
=== ep: 1943, time 26.774605751037598, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1943
goal_identified
goal_identified
=== ep: 1944, time 26.520880937576294, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1944
=== ep: 1945, time 26.652466773986816, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1945
goal_identified
=== ep: 1946, time 26.188812494277954, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1946
goal_identified
=== ep: 1947, time 26.966195821762085, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1947
goal_identified
=== ep: 1948, time 26.518555641174316, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1948
goal_identified
=== ep: 1949, time 31.523618698120117, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1949
goal_identified
goal_identified
goal_identified
=== ep: 1950, time 26.45452070236206, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1950
goal_identified
goal_identified
=== ep: 1951, time 26.546663761138916, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1951
goal_identified
=== ep: 1952, time 26.61362934112549, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1952
goal_identified
goal_identified
goal_identified
=== ep: 1953, time 26.04900026321411, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1953
goal_identified
goal_identified
=== ep: 1954, time 26.634554386138916, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1954
goal_identified
goal_identified
=== ep: 1955, time 26.389415979385376, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1955
goal_identified
goal_identified
=== ep: 1956, time 26.489412784576416, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 58/58)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1956
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 1957, time 26.462639093399048, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1957
goal_identified
goal_identified
=== ep: 1958, time 26.4338161945343, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1958
goal_identified
goal_identified
=== ep: 1959, time 31.8974552154541, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1959
goal_identified
goal_identified
=== ep: 1960, time 26.668150424957275, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1960
goal_identified
=== ep: 1961, time 26.341315746307373, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1961
goal_identified
goal_identified
=== ep: 1962, time 26.590598106384277, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1962
goal_identified
goal_identified
goal_identified
=== ep: 1963, time 26.554771900177002, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1963
goal_identified
goal_identified
=== ep: 1964, time 26.517245292663574, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1964
goal_identified
=== ep: 1965, time 26.545917749404907, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1965
goal_identified
=== ep: 1966, time 26.67570400238037, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1966
goal_identified
goal_identified
=== ep: 1967, time 26.248055458068848, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1967
goal_identified
goal_identified
=== ep: 1968, time 26.27408480644226, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1968
goal_identified
=== ep: 1969, time 31.12340521812439, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1969
goal_identified
goal_identified
=== ep: 1970, time 26.349984407424927, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1970
goal_identified
goal_identified
goal_identified
=== ep: 1971, time 26.94258213043213, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1971
goal_identified
=== ep: 1972, time 26.19513726234436, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1972
goal_identified
=== ep: 1973, time 26.42402458190918, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1973
=== ep: 1974, time 26.600916147232056, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1974
goal_identified
=== ep: 1975, time 26.457174062728882, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1975
goal_identified
=== ep: 1976, time 26.81388211250305, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1976
=== ep: 1977, time 26.831947565078735, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1977
goal_identified
=== ep: 1978, time 26.31902575492859, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1978
goal_identified
=== ep: 1979, time 32.08037281036377, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1979
goal_identified
=== ep: 1980, time 26.772066354751587, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1980
=== ep: 1981, time 26.82166314125061, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1981
goal_identified
goal_identified
=== ep: 1982, time 26.588671684265137, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1982
=== ep: 1983, time 26.55170488357544, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1983
goal_identified
=== ep: 1984, time 26.84281826019287, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1984
goal_identified
goal_identified
=== ep: 1985, time 26.3291757106781, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1985
goal_identified
=== ep: 1986, time 26.488947868347168, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1986
=== ep: 1987, time 26.528738498687744, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1987
goal_identified
=== ep: 1988, time 26.439505577087402, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1988
goal_identified
goal_identified
=== ep: 1989, time 31.54047155380249, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1989
goal_identified
goal_identified
=== ep: 1990, time 26.853695392608643, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1990
goal_identified
=== ep: 1991, time 27.008751392364502, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1991
goal_identified
goal_identified
=== ep: 1992, time 26.373066663742065, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1992
goal_identified
=== ep: 1993, time 26.60112762451172, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1993
goal_identified
=== ep: 1994, time 26.991057872772217, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1994
goal_identified
goal_identified
=== ep: 1995, time 26.820035696029663, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1995
goal_identified
goal_identified
=== ep: 1996, time 26.616750717163086, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1996
=== ep: 1997, time 26.412391662597656, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1997
goal_identified
=== ep: 1998, time 26.571064710617065, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1998
=== ep: 1999, time 32.198447465896606, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1999
=== ep: 2000, time 26.39134168624878, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2000
=== ep: 2001, time 26.67642116546631, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2001
goal_identified
goal_identified
=== ep: 2002, time 26.282570362091064, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2002
goal_identified
goal_identified
=== ep: 2003, time 26.80617928504944, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2003
goal_identified
goal_identified
goal_identified
=== ep: 2004, time 26.35182213783264, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2004
goal_identified
=== ep: 2005, time 26.554206371307373, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2005
goal_identified
=== ep: 2006, time 26.823267698287964, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2006
=== ep: 2007, time 26.553160667419434, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2007
goal_identified
goal_identified
=== ep: 2008, time 26.43785786628723, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2008
goal_identified
=== ep: 2009, time 31.52859616279602, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2009
goal_identified
=== ep: 2010, time 26.582777976989746, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2010
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2011, time 26.229939460754395, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2011
goal_identified
=== ep: 2012, time 26.518162488937378, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2012
=== ep: 2013, time 26.445862770080566, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2013
goal_identified
=== ep: 2014, time 26.39882779121399, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2014
goal_identified
goal_identified
=== ep: 2015, time 26.378967761993408, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2015
goal_identified
=== ep: 2016, time 26.63462781906128, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2016
goal_identified
goal_identified
=== ep: 2017, time 26.34033966064453, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2017
=== ep: 2018, time 26.45592975616455, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2018
goal_identified
=== ep: 2019, time 31.352481603622437, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2019
goal_identified
goal_identified
goal_identified
=== ep: 2020, time 26.222335815429688, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2020
=== ep: 2021, time 26.54120135307312, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2021
goal_identified
=== ep: 2022, time 26.74091148376465, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2022
goal_identified
goal_identified
goal_identified
=== ep: 2023, time 26.24804449081421, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2023
goal_identified
=== ep: 2024, time 26.5941801071167, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2024
goal_identified
=== ep: 2025, time 26.533036708831787, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2025
goal_identified
=== ep: 2026, time 26.71906614303589, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2026
goal_identified
=== ep: 2027, time 26.338573455810547, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2027
goal_identified
goal_identified
=== ep: 2028, time 26.61424732208252, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2028
goal_identified
=== ep: 2029, time 32.49099898338318, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2029
goal_identified
=== ep: 2030, time 26.6624493598938, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2030
=== ep: 2031, time 26.636865615844727, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2031
goal_identified
=== ep: 2032, time 26.886831045150757, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2032
goal_identified
=== ep: 2033, time 26.527887105941772, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2033
=== ep: 2034, time 26.63029456138611, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2034
goal_identified
=== ep: 2035, time 26.703981161117554, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2035
=== ep: 2036, time 26.509065866470337, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2036
goal_identified
goal_identified
goal_identified
=== ep: 2037, time 26.334101676940918, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2037
goal_identified
=== ep: 2038, time 26.419750452041626, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2038
goal_identified
=== ep: 2039, time 31.317116498947144, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2039
=== ep: 2040, time 26.74173617362976, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2040
goal_identified
=== ep: 2041, time 26.479984283447266, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2041
goal_identified
goal_identified
=== ep: 2042, time 26.593700647354126, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2042
goal_identified
=== ep: 2043, time 26.58669352531433, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2043
goal_identified
=== ep: 2044, time 24.89879274368286, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 43/43)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2044
goal_identified
goal_identified
goal_identified
=== ep: 2045, time 26.37367820739746, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2045
goal_identified
=== ep: 2046, time 26.185889959335327, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2046
goal_identified
goal_identified
goal_identified
=== ep: 2047, time 26.790964126586914, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2047
goal_identified
=== ep: 2048, time 27.18095827102661, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2048
goal_identified
goal_identified
=== ep: 2049, time 31.506956577301025, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2049
goal_identified
goal_identified
goal_identified
=== ep: 2050, time 26.4257493019104, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2050
goal_identified
=== ep: 2051, time 26.604090213775635, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2051
=== ep: 2052, time 27.064062118530273, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2052
goal_identified
goal_identified
goal_identified
=== ep: 2053, time 26.42415714263916, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2053
=== ep: 2054, time 26.647096872329712, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2054
=== ep: 2055, time 26.587706565856934, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2055
=== ep: 2056, time 26.49787473678589, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2056
goal_identified
goal_identified
=== ep: 2057, time 26.469666719436646, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2057
goal_identified
=== ep: 2058, time 26.726388454437256, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2058
goal_identified
goal_identified
=== ep: 2059, time 32.09952521324158, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2059
goal_identified
=== ep: 2060, time 26.803719520568848, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2060
goal_identified
goal_identified
=== ep: 2061, time 26.624648094177246, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2061
goal_identified
goal_identified
=== ep: 2062, time 26.561923503875732, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2062
goal_identified
=== ep: 2063, time 26.484643697738647, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2063
goal_identified
=== ep: 2064, time 26.535807371139526, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2064
goal_identified
goal_identified
=== ep: 2065, time 26.817469835281372, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2065
goal_identified
goal_identified
=== ep: 2066, time 27.05002784729004, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2066
=== ep: 2067, time 26.496745824813843, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2067
goal_identified
goal_identified
=== ep: 2068, time 26.711607933044434, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2068
goal_identified
goal_identified
=== ep: 2069, time 32.142817735672, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2069
goal_identified
goal_identified
=== ep: 2070, time 26.517361164093018, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2070
goal_identified
=== ep: 2071, time 26.763519048690796, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2071
=== ep: 2072, time 26.507577657699585, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2072
goal_identified
goal_identified
=== ep: 2073, time 26.52920699119568, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2073
goal_identified
goal_identified
=== ep: 2074, time 26.431949853897095, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2074
goal_identified
=== ep: 2075, time 26.85968852043152, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2075
goal_identified
=== ep: 2076, time 26.69623351097107, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2076
goal_identified
=== ep: 2077, time 27.070215702056885, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2077
goal_identified
=== ep: 2078, time 26.52517557144165, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2078
goal_identified
=== ep: 2079, time 31.33715295791626, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2079
=== ep: 2080, time 26.607229948043823, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2080
=== ep: 2081, time 26.78369903564453, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2081
goal_identified
goal_identified
=== ep: 2082, time 26.195852518081665, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2082
goal_identified
goal_identified
=== ep: 2083, time 26.816155195236206, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2083
goal_identified
goal_identified
goal_identified
=== ep: 2084, time 26.543781042099, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2084
goal_identified
=== ep: 2085, time 26.882169246673584, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2085
goal_identified
goal_identified
=== ep: 2086, time 26.590027332305908, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2086
=== ep: 2087, time 26.813477993011475, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2087
goal_identified
=== ep: 2088, time 26.601093292236328, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2088
goal_identified
goal_identified
=== ep: 2089, time 31.6034414768219, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2089
goal_identified
=== ep: 2090, time 26.46505355834961, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2090
goal_identified
=== ep: 2091, time 26.81027603149414, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2091
goal_identified
=== ep: 2092, time 26.491923332214355, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2092
=== ep: 2093, time 26.751841068267822, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2093
goal_identified
goal_identified
goal_identified
=== ep: 2094, time 26.566165685653687, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2094
goal_identified
=== ep: 2095, time 26.565244436264038, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2095
goal_identified
=== ep: 2096, time 26.15193510055542, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2096
goal_identified
goal_identified
=== ep: 2097, time 26.358760356903076, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2097
goal_identified
goal_identified
=== ep: 2098, time 26.30381679534912, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2098
goal_identified
goal_identified
=== ep: 2099, time 31.860363245010376, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2099
goal_identified
goal_identified
=== ep: 2100, time 27.031133890151978, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2100
goal_identified
=== ep: 2101, time 26.631694793701172, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2101
goal_identified
=== ep: 2102, time 26.439823389053345, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2102
goal_identified
=== ep: 2103, time 26.968494653701782, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2103
goal_identified
goal_identified
=== ep: 2104, time 26.565752506256104, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2104
goal_identified
=== ep: 2105, time 27.019030570983887, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2105
goal_identified
goal_identified
=== ep: 2106, time 26.514259099960327, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2106
goal_identified
=== ep: 2107, time 26.57398796081543, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2107
=== ep: 2108, time 26.33495855331421, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2108
goal_identified
goal_identified
goal_identified
=== ep: 2109, time 31.866395950317383, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2109
goal_identified
goal_identified
=== ep: 2110, time 26.70397114753723, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2110
goal_identified
goal_identified
=== ep: 2111, time 26.744184732437134, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2111
goal_identified
goal_identified
goal_identified
=== ep: 2112, time 26.415863752365112, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2112
goal_identified
=== ep: 2113, time 26.587419509887695, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2113
goal_identified
=== ep: 2114, time 26.291727542877197, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2114
goal_identified
goal_identified
=== ep: 2115, time 26.782724142074585, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2115
goal_identified
=== ep: 2116, time 26.74109172821045, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2116
goal_identified
goal_identified
=== ep: 2117, time 26.682300329208374, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2117
goal_identified
goal_identified
=== ep: 2118, time 26.834158420562744, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2118
goal_identified
=== ep: 2119, time 31.381656885147095, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2119
goal_identified
goal_identified
=== ep: 2120, time 26.855866193771362, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2120
goal_identified
goal_identified
goal_identified
=== ep: 2121, time 26.850216388702393, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2121
goal_identified
=== ep: 2122, time 26.609360933303833, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2122
=== ep: 2123, time 26.905763149261475, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2123
goal_identified
=== ep: 2124, time 26.921398878097534, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2124
=== ep: 2125, time 26.49058985710144, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2125
=== ep: 2126, time 26.953826904296875, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2126
goal_identified
=== ep: 2127, time 26.615389585494995, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2127
goal_identified
=== ep: 2128, time 26.347209453582764, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2128
goal_identified
goal_identified
=== ep: 2129, time 31.547848224639893, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2129
=== ep: 2130, time 26.64179754257202, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2130
goal_identified
goal_identified
=== ep: 2131, time 26.89291787147522, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2131
goal_identified
goal_identified
goal_identified
=== ep: 2132, time 26.019060373306274, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2132
goal_identified
=== ep: 2133, time 26.335087776184082, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2133
=== ep: 2134, time 26.25201153755188, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2134
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2135, time 26.631670236587524, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2135
goal_identified
=== ep: 2136, time 26.417318105697632, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2136
goal_identified
goal_identified
=== ep: 2137, time 26.5924015045166, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2137
goal_identified
=== ep: 2138, time 26.593563079833984, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2138
goal_identified
=== ep: 2139, time 31.88850450515747, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2139
=== ep: 2140, time 26.58160901069641, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2140
goal_identified
=== ep: 2141, time 26.604833126068115, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2141
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2142, time 26.737117052078247, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1212
goal_identified
goal_identified
=== ep: 2143, time 27.21175456047058, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2143
goal_identified
=== ep: 2144, time 26.68208622932434, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2144
goal_identified
goal_identified
goal_identified
=== ep: 2145, time 26.909606218338013, eps 0.001, sum reward: 3, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2145
=== ep: 2146, time 26.791428804397583, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2146
goal_identified
=== ep: 2147, time 26.805952548980713, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2147
goal_identified
=== ep: 2148, time 26.839328289031982, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2148
=== ep: 2149, time 31.902385711669922, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2149
goal_identified
=== ep: 2150, time 27.03064465522766, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2150
=== ep: 2151, time 27.028336763381958, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2151
=== ep: 2152, time 27.15470576286316, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2152
=== ep: 2153, time 26.610471963882446, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2153
goal_identified
goal_identified
=== ep: 2154, time 26.5738365650177, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2154
goal_identified
=== ep: 2155, time 26.674006462097168, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2155
=== ep: 2156, time 26.698302745819092, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2156
goal_identified
goal_identified
=== ep: 2157, time 26.638468980789185, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2157
goal_identified
=== ep: 2158, time 26.587206602096558, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2158
goal_identified
=== ep: 2159, time 31.929439067840576, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2159
=== ep: 2160, time 26.73568844795227, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2160
goal_identified
goal_identified
goal_identified
=== ep: 2161, time 26.993202209472656, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2161
goal_identified
goal_identified
=== ep: 2162, time 26.589614152908325, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2162
goal_identified
=== ep: 2163, time 26.76814293861389, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2163
=== ep: 2164, time 26.96835947036743, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2164
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2165, time 26.494841814041138, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2165
=== ep: 2166, time 26.393319129943848, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2166
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2167, time 26.61738657951355, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2167
goal_identified
=== ep: 2168, time 26.617573022842407, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2168
goal_identified
goal_identified
=== ep: 2169, time 31.7255220413208, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2169
=== ep: 2170, time 26.84921669960022, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2170
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2171, time 26.45012068748474, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2171
goal_identified
=== ep: 2172, time 26.500410079956055, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2172
goal_identified
goal_identified
goal_identified
=== ep: 2173, time 26.636632204055786, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2173
=== ep: 2174, time 26.567848205566406, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2174
goal_identified
=== ep: 2175, time 27.07785153388977, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2175
goal_identified
goal_identified
goal_identified
=== ep: 2176, time 26.643017292022705, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2176
=== ep: 2177, time 26.676480770111084, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2177
goal_identified
goal_identified
=== ep: 2178, time 26.503798246383667, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2178
goal_identified
goal_identified
=== ep: 2179, time 31.546478033065796, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2179
goal_identified
goal_identified
goal_identified
=== ep: 2180, time 27.16654944419861, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2180
=== ep: 2181, time 26.804343223571777, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2181
=== ep: 2182, time 26.674455404281616, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2182
goal_identified
=== ep: 2183, time 26.453235626220703, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2183
goal_identified
goal_identified
goal_identified
=== ep: 2184, time 26.3239848613739, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2184
goal_identified
=== ep: 2185, time 26.590234756469727, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2185
=== ep: 2186, time 26.443541288375854, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2186
goal_identified
goal_identified
=== ep: 2187, time 26.851516008377075, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2187
goal_identified
goal_identified
=== ep: 2188, time 26.445383071899414, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2188
goal_identified
=== ep: 2189, time 31.379783153533936, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2189
goal_identified
goal_identified
=== ep: 2190, time 26.562355995178223, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2190
goal_identified
=== ep: 2191, time 26.27544331550598, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2191
goal_identified
=== ep: 2192, time 26.861677408218384, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2192
goal_identified
=== ep: 2193, time 26.377837657928467, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2193
goal_identified
goal_identified
=== ep: 2194, time 26.55845046043396, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2194
goal_identified
goal_identified
=== ep: 2195, time 26.51979374885559, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2195
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2196, time 26.5555100440979, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2196
goal_identified
goal_identified
=== ep: 2197, time 26.575400352478027, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2197
goal_identified
goal_identified
=== ep: 2198, time 26.472158908843994, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2198
goal_identified
=== ep: 2199, time 31.53814458847046, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2199
goal_identified
goal_identified
=== ep: 2200, time 26.640160083770752, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2200
goal_identified
goal_identified
goal_identified
=== ep: 2201, time 26.766756057739258, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2201
goal_identified
goal_identified
=== ep: 2202, time 26.82477617263794, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2202
=== ep: 2203, time 26.85271644592285, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2203
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2204, time 26.623225212097168, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2204
goal_identified
goal_identified
=== ep: 2205, time 26.250893592834473, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2205
goal_identified
goal_identified
=== ep: 2206, time 26.538987159729004, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2206
goal_identified
goal_identified
goal_identified
=== ep: 2207, time 26.52806282043457, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2207
goal_identified
goal_identified
goal_identified
=== ep: 2208, time 26.549925088882446, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2208
goal_identified
goal_identified
=== ep: 2209, time 31.628976821899414, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2209
goal_identified
goal_identified
=== ep: 2210, time 26.455223321914673, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2210
goal_identified
goal_identified
goal_identified
=== ep: 2211, time 26.412183046340942, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2211
=== ep: 2212, time 26.71985936164856, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2212
=== ep: 2213, time 26.395548820495605, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2213
goal_identified
=== ep: 2214, time 26.605797290802002, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2214
goal_identified
goal_identified
goal_identified
=== ep: 2215, time 26.47383713722229, eps 0.001, sum reward: 3, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2215
goal_identified
goal_identified
=== ep: 2216, time 26.632843017578125, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2216
goal_identified
=== ep: 2217, time 27.023091793060303, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2217
goal_identified
=== ep: 2218, time 26.832013368606567, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2218
goal_identified
=== ep: 2219, time 31.650152683258057, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2219
goal_identified
goal_identified
goal_identified
=== ep: 2220, time 26.458772897720337, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2220
goal_identified
goal_identified
=== ep: 2221, time 26.481701612472534, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2221
goal_identified
goal_identified
=== ep: 2222, time 26.449050426483154, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2222
=== ep: 2223, time 26.6152286529541, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2223
goal_identified
goal_identified
goal_identified
=== ep: 2224, time 26.478649616241455, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2224
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2225, time 26.534754276275635, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2225
goal_identified
=== ep: 2226, time 27.176291704177856, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2226
=== ep: 2227, time 26.788262844085693, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2227
goal_identified
=== ep: 2228, time 26.613726377487183, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2228
=== ep: 2229, time 31.272815465927124, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2229
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2230, time 26.342446088790894, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2230
goal_identified
=== ep: 2231, time 26.638620853424072, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2231
goal_identified
=== ep: 2232, time 26.498035192489624, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2232
=== ep: 2233, time 26.9577374458313, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2233
goal_identified
=== ep: 2234, time 26.732048273086548, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2234
goal_identified
=== ep: 2235, time 26.787115335464478, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2235
goal_identified
=== ep: 2236, time 26.320839166641235, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2236
goal_identified
goal_identified
=== ep: 2237, time 26.160181045532227, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2237
goal_identified
=== ep: 2238, time 26.56239891052246, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2238
goal_identified
=== ep: 2239, time 31.521302938461304, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2239
=== ep: 2240, time 26.446908235549927, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2240
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2241, time 26.66232180595398, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2241
goal_identified
=== ep: 2242, time 26.639818906784058, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2242
goal_identified
goal_identified
goal_identified
=== ep: 2243, time 26.92119264602661, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2243
=== ep: 2244, time 26.718825817108154, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2244
goal_identified
goal_identified
goal_identified
=== ep: 2245, time 26.787189722061157, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2245
goal_identified
=== ep: 2246, time 26.287440538406372, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2246
=== ep: 2247, time 26.13201665878296, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2247
goal_identified
=== ep: 2248, time 26.8860182762146, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2248
goal_identified
=== ep: 2249, time 31.296818733215332, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2249
goal_identified
=== ep: 2250, time 26.861243724822998, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2250
=== ep: 2251, time 26.671308279037476, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2251
goal_identified
=== ep: 2252, time 26.92172122001648, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2252
goal_identified
goal_identified
=== ep: 2253, time 26.606210231781006, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2253
=== ep: 2254, time 26.556814432144165, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2254
=== ep: 2255, time 26.868668794631958, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2255
goal_identified
=== ep: 2256, time 26.757232189178467, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2256
goal_identified
=== ep: 2257, time 26.402836322784424, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2257
=== ep: 2258, time 26.48057222366333, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2258
goal_identified
=== ep: 2259, time 31.382408380508423, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2259
goal_identified
=== ep: 2260, time 26.296769380569458, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2260
=== ep: 2261, time 26.55798101425171, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2261
goal_identified
=== ep: 2262, time 26.966418027877808, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2262
goal_identified
goal_identified
=== ep: 2263, time 26.766441106796265, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2263
goal_identified
goal_identified
=== ep: 2264, time 26.68226671218872, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2264
goal_identified
goal_identified
goal_identified
=== ep: 2265, time 26.283496141433716, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2265
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2266, time 26.563969135284424, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2266
=== ep: 2267, time 26.31742000579834, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2267
goal_identified
=== ep: 2268, time 26.549132108688354, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2268
goal_identified
=== ep: 2269, time 31.535257577896118, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2269
goal_identified
=== ep: 2270, time 26.448984146118164, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2270
goal_identified
=== ep: 2271, time 27.018147468566895, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2271
goal_identified
goal_identified
=== ep: 2272, time 26.251446962356567, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2272
goal_identified
goal_identified
goal_identified
=== ep: 2273, time 26.54083490371704, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2273
goal_identified
goal_identified
=== ep: 2274, time 26.783580780029297, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2274
goal_identified
=== ep: 2275, time 26.58984661102295, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2275
goal_identified
goal_identified
=== ep: 2276, time 26.537193298339844, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2276
goal_identified
goal_identified
goal_identified
=== ep: 2277, time 26.681208610534668, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2277
goal_identified
goal_identified
=== ep: 2278, time 26.71861457824707, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 127/127)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2278
=== ep: 2279, time 31.71026921272278, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2279
goal_identified
=== ep: 2280, time 26.30914330482483, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2280
=== ep: 2281, time 26.652869701385498, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2281
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2282, time 26.397144556045532, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2282
=== ep: 2283, time 26.72607970237732, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2283
goal_identified
=== ep: 2284, time 27.10458493232727, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2284
goal_identified
goal_identified
=== ep: 2285, time 26.791104078292847, eps 0.001, sum reward: 2, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2285
=== ep: 2286, time 26.750566482543945, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2286
goal_identified
goal_identified
=== ep: 2287, time 26.501306533813477, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2287
goal_identified
goal_identified
=== ep: 2288, time 26.35230255126953, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2288
goal_identified
goal_identified
=== ep: 2289, time 31.3851375579834, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2289
=== ep: 2290, time 26.873589038848877, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2290
=== ep: 2291, time 26.39137291908264, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2291
goal_identified
goal_identified
=== ep: 2292, time 26.387115240097046, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2292
goal_identified
=== ep: 2293, time 26.556114435195923, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2293
=== ep: 2294, time 27.147534370422363, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2294
=== ep: 2295, time 26.72542715072632, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2295
=== ep: 2296, time 26.463006019592285, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2296
=== ep: 2297, time 26.730392456054688, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2297
goal_identified
goal_identified
=== ep: 2298, time 26.695902824401855, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2298
goal_identified
goal_identified
=== ep: 2299, time 31.338617086410522, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2299
goal_identified
goal_identified
=== ep: 2300, time 26.680880784988403, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2300
goal_identified
goal_identified
=== ep: 2301, time 26.769121646881104, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2301
goal_identified
goal_identified
=== ep: 2302, time 26.63945722579956, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2302
=== ep: 2303, time 26.369264841079712, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2303
goal_identified
=== ep: 2304, time 26.704248905181885, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2304
goal_identified
=== ep: 2305, time 26.63335609436035, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2305
goal_identified
=== ep: 2306, time 26.69569420814514, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2306
=== ep: 2307, time 26.854512214660645, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2307
goal_identified
=== ep: 2308, time 26.459988117218018, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2308
=== ep: 2309, time 30.916273832321167, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2309
goal_identified
goal_identified
=== ep: 2310, time 26.498857021331787, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2310
=== ep: 2311, time 27.192403078079224, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2311
goal_identified
goal_identified
goal_identified
=== ep: 2312, time 26.47931170463562, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2312
=== ep: 2313, time 26.89089822769165, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2313
=== ep: 2314, time 26.834460496902466, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2314
=== ep: 2315, time 26.68940782546997, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2315
=== ep: 2316, time 26.571606636047363, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2316
=== ep: 2317, time 26.902340412139893, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2317
=== ep: 2318, time 26.49701499938965, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2318
goal_identified
=== ep: 2319, time 31.766616582870483, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2319
=== ep: 2320, time 26.795583724975586, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2320
goal_identified
=== ep: 2321, time 26.48397946357727, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2321
goal_identified
goal_identified
goal_identified
=== ep: 2322, time 26.69923496246338, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2322
=== ep: 2323, time 28.453871726989746, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2323
goal_identified
=== ep: 2324, time 26.65182137489319, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2324
goal_identified
=== ep: 2325, time 26.897810459136963, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2325
goal_identified
=== ep: 2326, time 26.418261289596558, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2326
goal_identified
goal_identified
=== ep: 2327, time 26.418251991271973, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2327
goal_identified
goal_identified
=== ep: 2328, time 26.654576301574707, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2328
goal_identified
=== ep: 2329, time 31.2715106010437, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2329
goal_identified
=== ep: 2330, time 26.668164491653442, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2330
goal_identified
goal_identified
goal_identified
=== ep: 2331, time 26.467522859573364, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2331
goal_identified
goal_identified
=== ep: 2332, time 27.04587197303772, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2332
goal_identified
goal_identified
=== ep: 2333, time 26.439857482910156, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2333
=== ep: 2334, time 26.651742696762085, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2334
goal_identified
goal_identified
goal_identified
=== ep: 2335, time 26.46313977241516, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2335
=== ep: 2336, time 26.6860089302063, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2336
goal_identified
goal_identified
=== ep: 2337, time 26.141263961791992, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2337
goal_identified
goal_identified
=== ep: 2338, time 26.250364065170288, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2338
goal_identified
goal_identified
=== ep: 2339, time 30.97593402862549, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2339
=== ep: 2340, time 26.964263200759888, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2340
=== ep: 2341, time 26.443666696548462, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2341
=== ep: 2342, time 26.81747317314148, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2342
goal_identified
=== ep: 2343, time 26.57794165611267, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2343
goal_identified
goal_identified
goal_identified
=== ep: 2344, time 26.37832498550415, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2344
goal_identified
=== ep: 2345, time 26.47016978263855, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2345
goal_identified
=== ep: 2346, time 26.573302268981934, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2346
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2347, time 26.761738300323486, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2347
goal_identified
goal_identified
=== ep: 2348, time 26.487832069396973, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2348
=== ep: 2349, time 31.55978298187256, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2349
=== ep: 2350, time 26.408756017684937, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2350
goal_identified
goal_identified
goal_identified
=== ep: 2351, time 26.62433886528015, eps 0.001, sum reward: 3, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2351
goal_identified
goal_identified
=== ep: 2352, time 26.342540979385376, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2352
goal_identified
goal_identified
=== ep: 2353, time 26.474673986434937, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2353
=== ep: 2354, time 26.451736450195312, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2354
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2355, time 26.93139100074768, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2355
=== ep: 2356, time 26.31541419029236, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2356
goal_identified
=== ep: 2357, time 26.47916841506958, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2357
goal_identified
goal_identified
=== ep: 2358, time 26.584784746170044, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2358
=== ep: 2359, time 31.566725969314575, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2359
goal_identified
=== ep: 2360, time 26.159083604812622, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2360
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2361, time 26.73888611793518, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2361
=== ep: 2362, time 26.60292363166809, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2362
goal_identified
=== ep: 2363, time 26.42062735557556, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2363
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2364, time 26.54570508003235, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2364
=== ep: 2365, time 26.545166969299316, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2365
goal_identified
goal_identified
=== ep: 2366, time 26.40867257118225, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2366
goal_identified
goal_identified
goal_identified
=== ep: 2367, time 26.499696254730225, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2367
goal_identified
=== ep: 2368, time 26.609398365020752, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2368
goal_identified
goal_identified
goal_identified
=== ep: 2369, time 31.740419387817383, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2369
goal_identified
goal_identified
=== ep: 2370, time 26.819778442382812, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2370
goal_identified
goal_identified
=== ep: 2371, time 26.63619351387024, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2371
goal_identified
goal_identified
goal_identified
=== ep: 2372, time 26.443764209747314, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2372
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2373, time 26.33548402786255, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1527
=== ep: 2374, time 26.57001256942749, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2374
=== ep: 2375, time 26.836538553237915, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2375
=== ep: 2376, time 27.002696752548218, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2376
goal_identified
=== ep: 2377, time 26.821250200271606, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2377
goal_identified
goal_identified
=== ep: 2378, time 26.66240119934082, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2378
goal_identified
goal_identified
goal_identified
=== ep: 2379, time 31.24860906600952, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2379
goal_identified
goal_identified
goal_identified
=== ep: 2380, time 26.599470376968384, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2380
goal_identified
=== ep: 2381, time 26.50172257423401, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2381
=== ep: 2382, time 26.795568466186523, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2382
goal_identified
goal_identified
=== ep: 2383, time 27.372100353240967, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2383
goal_identified
=== ep: 2384, time 26.905818462371826, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2384
goal_identified
=== ep: 2385, time 26.70732855796814, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2385
goal_identified
=== ep: 2386, time 26.294230699539185, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2386
goal_identified
goal_identified
=== ep: 2387, time 26.450342893600464, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2387
goal_identified
goal_identified
=== ep: 2388, time 26.624858140945435, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2388
goal_identified
=== ep: 2389, time 31.50561499595642, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2389
goal_identified
=== ep: 2390, time 26.66776752471924, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2390
=== ep: 2391, time 26.81525754928589, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2391
goal_identified
goal_identified
=== ep: 2392, time 26.518038034439087, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2392
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2393, time 26.873998880386353, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1753
goal_identified
goal_identified
=== ep: 2394, time 26.521634817123413, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2394
=== ep: 2395, time 26.652193069458008, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2395
=== ep: 2396, time 27.145879983901978, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2396
goal_identified
=== ep: 2397, time 27.100577116012573, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2397
goal_identified
=== ep: 2398, time 26.89151358604431, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2398
goal_identified
goal_identified
=== ep: 2399, time 31.209941864013672, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2399
goal_identified
=== ep: 2400, time 26.65361523628235, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2400
goal_identified
=== ep: 2401, time 26.592592000961304, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2401
=== ep: 2402, time 26.953534364700317, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2402
goal_identified
=== ep: 2403, time 26.930061101913452, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2403
goal_identified
goal_identified
goal_identified
=== ep: 2404, time 26.377861261367798, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2404
goal_identified
goal_identified
=== ep: 2405, time 26.368988513946533, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2405
goal_identified
goal_identified
=== ep: 2406, time 26.977333784103394, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2406
goal_identified
goal_identified
=== ep: 2407, time 26.500667572021484, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2407
goal_identified
goal_identified
goal_identified
=== ep: 2408, time 26.333837032318115, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2408
goal_identified
goal_identified
=== ep: 2409, time 31.614506006240845, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2409
=== ep: 2410, time 27.255138635635376, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2410
goal_identified
=== ep: 2411, time 26.469295501708984, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2411
goal_identified
goal_identified
goal_identified
=== ep: 2412, time 26.620800495147705, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2412
goal_identified
goal_identified
=== ep: 2413, time 26.62718939781189, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2413
=== ep: 2414, time 26.596758365631104, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2414
goal_identified
=== ep: 2415, time 26.37677311897278, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2415
goal_identified
goal_identified
=== ep: 2416, time 27.092833042144775, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 129/129)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2416
goal_identified
goal_identified
goal_identified
=== ep: 2417, time 26.412416219711304, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2417
goal_identified
goal_identified
goal_identified
=== ep: 2418, time 26.40354824066162, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2418
goal_identified
=== ep: 2419, time 31.452802658081055, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2419
goal_identified
goal_identified
=== ep: 2420, time 27.37263798713684, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2420
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2421, time 26.327834367752075, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2421
goal_identified
goal_identified
=== ep: 2422, time 26.62536358833313, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2422
goal_identified
goal_identified
goal_identified
=== ep: 2423, time 26.53039312362671, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2423
goal_identified
=== ep: 2424, time 26.745323657989502, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2424
goal_identified
=== ep: 2425, time 26.57240056991577, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2425
goal_identified
=== ep: 2426, time 26.461918592453003, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2426
=== ep: 2427, time 26.594670295715332, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2427
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2428, time 26.699031114578247, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2428
goal_identified
=== ep: 2429, time 31.34664249420166, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2429
goal_identified
goal_identified
=== ep: 2430, time 26.478400945663452, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2430
goal_identified
=== ep: 2431, time 26.629613161087036, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2431
goal_identified
goal_identified
goal_identified
=== ep: 2432, time 26.73296093940735, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2432
goal_identified
=== ep: 2433, time 26.576750993728638, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2433
goal_identified
goal_identified
=== ep: 2434, time 26.181018829345703, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2434
=== ep: 2435, time 26.771212816238403, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2435
goal_identified
=== ep: 2436, time 26.6066255569458, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2436
=== ep: 2437, time 26.52605438232422, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2437
goal_identified
=== ep: 2438, time 26.250965356826782, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 49/49)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2438
goal_identified
goal_identified
=== ep: 2439, time 31.4637348651886, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2439
goal_identified
=== ep: 2440, time 26.32882308959961, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2440
goal_identified
goal_identified
=== ep: 2441, time 26.91100311279297, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2441
goal_identified
goal_identified
=== ep: 2442, time 26.41989541053772, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2442
=== ep: 2443, time 26.673487901687622, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2443
goal_identified
=== ep: 2444, time 26.183903455734253, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2444
goal_identified
=== ep: 2445, time 26.6690673828125, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2445
goal_identified
goal_identified
=== ep: 2446, time 26.456830263137817, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2446
goal_identified
goal_identified
=== ep: 2447, time 26.67941379547119, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2447
goal_identified
goal_identified
=== ep: 2448, time 26.549095630645752, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2448
=== ep: 2449, time 31.433111906051636, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2449
goal_identified
=== ep: 2450, time 26.734211683273315, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2450
=== ep: 2451, time 26.509750366210938, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2451
goal_identified
goal_identified
=== ep: 2452, time 26.785682678222656, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2452
goal_identified
=== ep: 2453, time 26.353700637817383, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2453
goal_identified
goal_identified
=== ep: 2454, time 26.44033718109131, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2454
goal_identified
=== ep: 2455, time 26.441931009292603, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2455
goal_identified
=== ep: 2456, time 26.894192695617676, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2456
=== ep: 2457, time 26.643155574798584, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2457
=== ep: 2458, time 26.71869683265686, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2458
=== ep: 2459, time 31.549234628677368, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2459
goal_identified
=== ep: 2460, time 26.673781156539917, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2460
goal_identified
=== ep: 2461, time 26.672931671142578, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2461
goal_identified
goal_identified
=== ep: 2462, time 27.01410698890686, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2462
=== ep: 2463, time 26.66653275489807, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2463
goal_identified
=== ep: 2464, time 27.017307996749878, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2464
goal_identified
=== ep: 2465, time 27.281461000442505, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2465
goal_identified
=== ep: 2466, time 26.561042547225952, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2466
goal_identified
=== ep: 2467, time 26.526483297348022, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2467
=== ep: 2468, time 26.872533082962036, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2468
goal_identified
=== ep: 2469, time 31.882957220077515, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2469
goal_identified
goal_identified
=== ep: 2470, time 26.907010316848755, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2470
goal_identified
=== ep: 2471, time 26.673001289367676, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2471
goal_identified
=== ep: 2472, time 26.27614688873291, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2472
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2473, time 26.39314079284668, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2473
=== ep: 2474, time 26.71077299118042, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2474
goal_identified
goal_identified
goal_identified
=== ep: 2475, time 26.65784788131714, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2475
goal_identified
goal_identified
=== ep: 2476, time 26.663373708724976, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2476
goal_identified
goal_identified
=== ep: 2477, time 26.56782603263855, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2477
=== ep: 2478, time 26.958986043930054, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2478
goal_identified
=== ep: 2479, time 31.448143482208252, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2479
goal_identified
goal_identified
=== ep: 2480, time 26.871180057525635, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2480
goal_identified
=== ep: 2481, time 26.851744651794434, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2481
=== ep: 2482, time 26.54511594772339, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2482
goal_identified
=== ep: 2483, time 26.96219301223755, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2483
goal_identified
=== ep: 2484, time 26.598442316055298, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2484
goal_identified
=== ep: 2485, time 26.479932069778442, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2485
goal_identified
goal_identified
=== ep: 2486, time 26.759230375289917, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2486
goal_identified
=== ep: 2487, time 26.904014825820923, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2487
goal_identified
=== ep: 2488, time 26.681782007217407, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2488
goal_identified
goal_identified
=== ep: 2489, time 31.534279346466064, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2489
goal_identified
goal_identified
=== ep: 2490, time 26.66652822494507, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2490
=== ep: 2491, time 26.472408056259155, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2491
goal_identified
=== ep: 2492, time 26.64235281944275, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2492
=== ep: 2493, time 26.582650184631348, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2493
goal_identified
goal_identified
=== ep: 2494, time 26.838478565216064, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2494
=== ep: 2495, time 26.43715262413025, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2495
goal_identified
goal_identified
=== ep: 2496, time 26.363224506378174, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2496
goal_identified
goal_identified
=== ep: 2497, time 26.296391487121582, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2497
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2498, time 26.520288944244385, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2498
=== ep: 2499, time 31.050949811935425, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2499
=== ep: 2500, time 26.5534451007843, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2500
goal_identified
goal_identified
=== ep: 2501, time 26.26312518119812, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2501
goal_identified
goal_identified
=== ep: 2502, time 26.697949647903442, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2502
goal_identified
goal_identified
=== ep: 2503, time 26.402178049087524, eps 0.001, sum reward: 2, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2503
goal_identified
goal_identified
=== ep: 2504, time 26.799964904785156, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2504
goal_identified
goal_identified
=== ep: 2505, time 26.552120447158813, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2505
goal_identified
=== ep: 2506, time 26.908564567565918, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2506
goal_identified
=== ep: 2507, time 26.745134830474854, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2507
goal_identified
goal_identified
=== ep: 2508, time 26.95719003677368, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2508
goal_identified
goal_identified
=== ep: 2509, time 31.274991273880005, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2509
goal_identified
goal_identified
=== ep: 2510, time 26.54136610031128, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2510
goal_identified
goal_identified
=== ep: 2511, time 26.50567078590393, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2511
=== ep: 2512, time 26.54639768600464, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2512
=== ep: 2513, time 26.85451364517212, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2513
goal_identified
goal_identified
goal_identified
=== ep: 2514, time 26.587589025497437, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2514
goal_identified
=== ep: 2515, time 26.404778957366943, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2515
goal_identified
=== ep: 2516, time 26.538103818893433, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2516
goal_identified
goal_identified
=== ep: 2517, time 26.8528835773468, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2517
=== ep: 2518, time 26.520013093948364, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2518
goal_identified
goal_identified
goal_identified
=== ep: 2519, time 31.640573024749756, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2519
goal_identified
goal_identified
=== ep: 2520, time 26.66375708580017, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2520
=== ep: 2521, time 26.47426962852478, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2521
=== ep: 2522, time 26.48895502090454, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2522
=== ep: 2523, time 26.62393546104431, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2523
goal_identified
=== ep: 2524, time 26.962517023086548, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2524
goal_identified
goal_identified
=== ep: 2525, time 26.57130765914917, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2525
=== ep: 2526, time 26.494274616241455, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2526
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2527, time 26.54046630859375, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2527
goal_identified
=== ep: 2528, time 26.737746238708496, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2528
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2529, time 31.26996374130249, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2529
goal_identified
=== ep: 2530, time 26.33442974090576, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2530
goal_identified
=== ep: 2531, time 26.63096594810486, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2531
=== ep: 2532, time 26.581027507781982, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2532
=== ep: 2533, time 26.34685182571411, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2533
goal_identified
=== ep: 2534, time 26.524454355239868, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2534
=== ep: 2535, time 26.607568740844727, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2535
goal_identified
=== ep: 2536, time 26.17187786102295, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2536
goal_identified
goal_identified
=== ep: 2537, time 26.340867280960083, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2537
=== ep: 2538, time 26.573235750198364, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2538
=== ep: 2539, time 31.465479135513306, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2539
goal_identified
=== ep: 2540, time 26.30044913291931, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2540
=== ep: 2541, time 26.520996809005737, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2541
=== ep: 2542, time 26.738222122192383, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2542
goal_identified
=== ep: 2543, time 26.5801682472229, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2543
goal_identified
goal_identified
=== ep: 2544, time 26.850411415100098, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2544
goal_identified
=== ep: 2545, time 26.586771249771118, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2545
goal_identified
goal_identified
goal_identified
=== ep: 2546, time 26.331517219543457, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2546
goal_identified
=== ep: 2547, time 26.830254793167114, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2547
=== ep: 2548, time 26.63196849822998, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2548
goal_identified
goal_identified
=== ep: 2549, time 31.014610767364502, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2549
goal_identified
goal_identified
=== ep: 2550, time 26.130152225494385, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2550
goal_identified
=== ep: 2551, time 26.691446542739868, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2551
goal_identified
=== ep: 2552, time 26.545313596725464, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2552
=== ep: 2553, time 26.866156578063965, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2553
goal_identified
goal_identified
=== ep: 2554, time 26.429775714874268, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2554
=== ep: 2555, time 26.217138528823853, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2555
=== ep: 2556, time 26.554277181625366, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2556
goal_identified
goal_identified
=== ep: 2557, time 26.628851413726807, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2557
=== ep: 2558, time 26.367141008377075, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2558
goal_identified
=== ep: 2559, time 31.388282537460327, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2559
=== ep: 2560, time 28.66523814201355, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2560
goal_identified
goal_identified
=== ep: 2561, time 26.513766765594482, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2561
goal_identified
goal_identified
=== ep: 2562, time 26.314098834991455, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2562
=== ep: 2563, time 26.67353057861328, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2563
goal_identified
=== ep: 2564, time 26.56923508644104, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2564
goal_identified
goal_identified
=== ep: 2565, time 26.36136746406555, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2565
goal_identified
=== ep: 2566, time 26.03675127029419, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2566
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2567, time 26.261385679244995, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2567
goal_identified
goal_identified
goal_identified
=== ep: 2568, time 26.588378429412842, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2568
=== ep: 2569, time 31.588444232940674, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2569
goal_identified
goal_identified
goal_identified
=== ep: 2570, time 26.714267015457153, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2570
goal_identified
goal_identified
=== ep: 2571, time 26.595890522003174, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2571
goal_identified
=== ep: 2572, time 26.73260736465454, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2572
goal_identified
=== ep: 2573, time 26.420711278915405, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2573
goal_identified
goal_identified
goal_identified
=== ep: 2574, time 26.17801070213318, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2574
goal_identified
goal_identified
=== ep: 2575, time 26.217292547225952, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2575
goal_identified
goal_identified
=== ep: 2576, time 26.669114351272583, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2576
=== ep: 2577, time 26.27659296989441, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2577
goal_identified
=== ep: 2578, time 26.33855390548706, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2578
goal_identified
=== ep: 2579, time 31.072442054748535, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2579
goal_identified
goal_identified
goal_identified
=== ep: 2580, time 26.118353843688965, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2580
=== ep: 2581, time 26.385480403900146, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2581
goal_identified
goal_identified
=== ep: 2582, time 26.211950063705444, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2582
goal_identified
goal_identified
goal_identified
=== ep: 2583, time 26.30294966697693, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2583
goal_identified
=== ep: 2584, time 26.11263418197632, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2584
goal_identified
=== ep: 2585, time 26.013675928115845, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2585
goal_identified
=== ep: 2586, time 26.300828218460083, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2586
goal_identified
=== ep: 2587, time 26.1725857257843, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2587
=== ep: 2588, time 26.269084215164185, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2588
goal_identified
=== ep: 2589, time 30.639023542404175, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2589
goal_identified
=== ep: 2590, time 26.37283492088318, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2590
=== ep: 2591, time 26.23793601989746, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2591
goal_identified
goal_identified
=== ep: 2592, time 25.798065900802612, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2592
=== ep: 2593, time 26.212585926055908, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2593
goal_identified
=== ep: 2594, time 25.821038722991943, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2594
goal_identified
goal_identified
=== ep: 2595, time 25.744667053222656, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2595
=== ep: 2596, time 26.644253253936768, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2596
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2597, time 26.03245210647583, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2597
goal_identified
goal_identified
=== ep: 2598, time 25.917568922042847, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2598
goal_identified
=== ep: 2599, time 30.7974693775177, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2599
goal_identified
=== ep: 2600, time 25.706652641296387, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2600
=== ep: 2601, time 26.029985189437866, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2601
goal_identified
goal_identified
=== ep: 2602, time 25.48489212989807, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2602
goal_identified
=== ep: 2603, time 25.45358419418335, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2603
goal_identified
=== ep: 2604, time 25.922983646392822, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2604
goal_identified
=== ep: 2605, time 25.66554307937622, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2605
=== ep: 2606, time 25.468181848526, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2606
=== ep: 2607, time 25.63725972175598, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2607
goal_identified
=== ep: 2608, time 25.494957208633423, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2608
goal_identified
=== ep: 2609, time 30.094063997268677, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2609
=== ep: 2610, time 25.454041957855225, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2610
=== ep: 2611, time 25.265860557556152, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2611
goal_identified
=== ep: 2612, time 25.57057476043701, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2612
goal_identified
goal_identified
=== ep: 2613, time 25.347840547561646, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2613
goal_identified
goal_identified
=== ep: 2614, time 25.193190574645996, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2614
goal_identified
=== ep: 2615, time 25.32584500312805, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2615
goal_identified
=== ep: 2616, time 25.48110580444336, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2616
goal_identified
goal_identified
=== ep: 2617, time 25.277647018432617, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2617
=== ep: 2618, time 24.892805099487305, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2618
goal_identified
goal_identified
=== ep: 2619, time 30.23039174079895, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2619
goal_identified
goal_identified
=== ep: 2620, time 25.310463190078735, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2620
goal_identified
=== ep: 2621, time 25.010826110839844, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2621
goal_identified
goal_identified
goal_identified
=== ep: 2622, time 24.988996267318726, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2622
goal_identified
goal_identified
=== ep: 2623, time 25.21925711631775, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2623
goal_identified
=== ep: 2624, time 25.142045259475708, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2624
goal_identified
=== ep: 2625, time 24.961833715438843, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2625
=== ep: 2626, time 25.273579597473145, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2626
goal_identified
goal_identified
=== ep: 2627, time 25.24214267730713, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2627
=== ep: 2628, time 25.10875391960144, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2628
=== ep: 2629, time 29.677037239074707, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2629
goal_identified
=== ep: 2630, time 24.93353581428528, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2630
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2631, time 25.596518754959106, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2631
goal_identified
goal_identified
=== ep: 2632, time 25.792673349380493, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2632
goal_identified
goal_identified
=== ep: 2633, time 25.40417790412903, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2633
=== ep: 2634, time 25.28728985786438, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2634
goal_identified
goal_identified
=== ep: 2635, time 25.116040468215942, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2635
goal_identified
=== ep: 2636, time 25.165579080581665, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2636
goal_identified
goal_identified
=== ep: 2637, time 25.327321767807007, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2637
=== ep: 2638, time 25.22548794746399, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2638
=== ep: 2639, time 29.785093545913696, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2639
goal_identified
=== ep: 2640, time 25.497191190719604, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2640
=== ep: 2641, time 25.05022144317627, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2641
=== ep: 2642, time 25.025030374526978, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2642
=== ep: 2643, time 25.40979266166687, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2643
goal_identified
=== ep: 2644, time 25.177067756652832, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2644
=== ep: 2645, time 25.245279550552368, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2645
goal_identified
goal_identified
goal_identified
=== ep: 2646, time 25.190500020980835, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2646
=== ep: 2647, time 25.335116863250732, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2647
goal_identified
=== ep: 2648, time 24.84381675720215, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2648
goal_identified
=== ep: 2649, time 30.909021139144897, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2649
goal_identified
goal_identified
=== ep: 2650, time 25.38495945930481, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2650
goal_identified
goal_identified
=== ep: 2651, time 25.129945516586304, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2651
goal_identified
goal_identified
=== ep: 2652, time 25.847461223602295, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 121/121)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2652
goal_identified
=== ep: 2653, time 25.55741810798645, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2653
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2654, time 25.166298627853394, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2654
goal_identified
=== ep: 2655, time 25.45435929298401, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2655
goal_identified
goal_identified
goal_identified
=== ep: 2656, time 25.336212158203125, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2656
goal_identified
=== ep: 2657, time 25.325628519058228, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2657
goal_identified
goal_identified
goal_identified
=== ep: 2658, time 25.331440687179565, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2658
goal_identified
goal_identified
goal_identified
=== ep: 2659, time 30.224619150161743, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2659
goal_identified
goal_identified
=== ep: 2660, time 25.24276041984558, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2660
=== ep: 2661, time 25.28549861907959, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2661
=== ep: 2662, time 25.779808044433594, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2662
goal_identified
=== ep: 2663, time 25.29667854309082, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2663
=== ep: 2664, time 25.63072633743286, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2664
goal_identified
=== ep: 2665, time 25.769157648086548, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2665
goal_identified
=== ep: 2666, time 25.85721778869629, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2666
goal_identified
goal_identified
=== ep: 2667, time 25.543566942214966, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2667
goal_identified
=== ep: 2668, time 25.881659269332886, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2668
goal_identified
=== ep: 2669, time 30.2069833278656, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2669
goal_identified
goal_identified
goal_identified
=== ep: 2670, time 25.482802629470825, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2670
goal_identified
=== ep: 2671, time 25.372777938842773, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2671
goal_identified
=== ep: 2672, time 25.46804714202881, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2672
goal_identified
goal_identified
goal_identified
=== ep: 2673, time 25.335439920425415, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2673
goal_identified
goal_identified
=== ep: 2674, time 25.302138566970825, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2674
goal_identified
goal_identified
=== ep: 2675, time 25.87459897994995, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2675
goal_identified
=== ep: 2676, time 24.8685359954834, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2676
goal_identified
goal_identified
goal_identified
=== ep: 2677, time 25.30037760734558, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2677
goal_identified
=== ep: 2678, time 25.67884087562561, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2678
=== ep: 2679, time 33.91203260421753, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2679
=== ep: 2680, time 25.412357807159424, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2680
goal_identified
goal_identified
=== ep: 2681, time 25.462515830993652, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2681
=== ep: 2682, time 25.29450750350952, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2682
goal_identified
=== ep: 2683, time 25.49980354309082, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2683
goal_identified
=== ep: 2684, time 25.219311475753784, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 45/45)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2684
=== ep: 2685, time 25.33820652961731, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2685
goal_identified
goal_identified
=== ep: 2686, time 25.18401312828064, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2686
goal_identified
goal_identified
=== ep: 2687, time 25.378947496414185, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2687
goal_identified
goal_identified
=== ep: 2688, time 25.33954405784607, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2688
=== ep: 2689, time 30.748655080795288, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2689
goal_identified
=== ep: 2690, time 25.38490104675293, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2690
goal_identified
=== ep: 2691, time 26.022082567214966, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2691
=== ep: 2692, time 25.55298924446106, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2692
=== ep: 2693, time 25.571033000946045, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2693
goal_identified
=== ep: 2694, time 25.623948097229004, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2694
goal_identified
=== ep: 2695, time 25.6136577129364, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2695
goal_identified
=== ep: 2696, time 25.396812200546265, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2696
=== ep: 2697, time 25.584601879119873, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2697
goal_identified
goal_identified
=== ep: 2698, time 25.5448579788208, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2698
goal_identified
goal_identified
goal_identified
=== ep: 2699, time 30.23656415939331, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2699
goal_identified
goal_identified
=== ep: 2700, time 25.53975796699524, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2700
goal_identified
=== ep: 2701, time 25.596272706985474, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2701
=== ep: 2702, time 25.63162899017334, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2702
=== ep: 2703, time 25.593777179718018, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2703
goal_identified
goal_identified
goal_identified
=== ep: 2704, time 25.289919137954712, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2704
goal_identified
goal_identified
=== ep: 2705, time 25.086873769760132, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2705
goal_identified
goal_identified
goal_identified
=== ep: 2706, time 25.24410343170166, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2706
=== ep: 2707, time 25.731038093566895, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2707
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2708, time 25.365950107574463, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1861
goal_identified
=== ep: 2709, time 30.186620235443115, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2709
=== ep: 2710, time 25.43757462501526, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2710
goal_identified
goal_identified
goal_identified
=== ep: 2711, time 25.48581624031067, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2711
goal_identified
=== ep: 2712, time 25.528090715408325, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2712
goal_identified
=== ep: 2713, time 25.518447637557983, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2713
goal_identified
goal_identified
=== ep: 2714, time 25.507389783859253, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2714
goal_identified
=== ep: 2715, time 25.821181535720825, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2715
goal_identified
=== ep: 2716, time 25.506640911102295, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2716
goal_identified
=== ep: 2717, time 25.638792514801025, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2717
goal_identified
=== ep: 2718, time 25.178629636764526, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2718
goal_identified
goal_identified
=== ep: 2719, time 30.452573776245117, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2719
=== ep: 2720, time 26.702022075653076, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2720
goal_identified
goal_identified
=== ep: 2721, time 25.565423727035522, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2721
=== ep: 2722, time 25.436619520187378, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2722
goal_identified
goal_identified
goal_identified
=== ep: 2723, time 25.684041261672974, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2723
goal_identified
=== ep: 2724, time 25.48575448989868, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2724
=== ep: 2725, time 25.575657606124878, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2725
goal_identified
goal_identified
goal_identified
=== ep: 2726, time 25.72810935974121, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2726
goal_identified
=== ep: 2727, time 25.488729000091553, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2727
goal_identified
goal_identified
=== ep: 2728, time 25.748552560806274, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2728
goal_identified
goal_identified
=== ep: 2729, time 30.007350206375122, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2729
=== ep: 2730, time 25.910193920135498, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2730
goal_identified
=== ep: 2731, time 25.688432216644287, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2731
goal_identified
=== ep: 2732, time 25.82254147529602, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2732
=== ep: 2733, time 25.73232626914978, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2733
=== ep: 2734, time 25.260505437850952, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2734
goal_identified
=== ep: 2735, time 25.589223861694336, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2735
goal_identified
goal_identified
=== ep: 2736, time 25.575399160385132, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2736
goal_identified
goal_identified
goal_identified
=== ep: 2737, time 25.627907276153564, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2737
goal_identified
=== ep: 2738, time 25.821219205856323, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 114/114)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2738
goal_identified
goal_identified
=== ep: 2739, time 30.24328875541687, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2739
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2740, time 25.81117868423462, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2740
goal_identified
goal_identified
=== ep: 2741, time 25.854081392288208, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2741
goal_identified
goal_identified
goal_identified
=== ep: 2742, time 25.53148341178894, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2742
goal_identified
=== ep: 2743, time 25.57046341896057, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2743
goal_identified
goal_identified
=== ep: 2744, time 25.641974210739136, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2744
goal_identified
goal_identified
goal_identified
=== ep: 2745, time 25.61947727203369, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2745
goal_identified
goal_identified
=== ep: 2746, time 26.154526948928833, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2746
=== ep: 2747, time 25.966415643692017, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2747
goal_identified
=== ep: 2748, time 25.67671799659729, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2748
=== ep: 2749, time 30.695565223693848, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2749
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2750, time 25.570286989212036, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2750
goal_identified
=== ep: 2751, time 25.876941204071045, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2751
goal_identified
=== ep: 2752, time 25.767354249954224, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2752
goal_identified
=== ep: 2753, time 26.265432834625244, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2753
=== ep: 2754, time 25.682311296463013, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2754
goal_identified
goal_identified
=== ep: 2755, time 25.90760612487793, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2755
=== ep: 2756, time 25.723756790161133, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2756
goal_identified
=== ep: 2757, time 25.623838186264038, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2757
goal_identified
goal_identified
=== ep: 2758, time 25.500094652175903, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2758
goal_identified
=== ep: 2759, time 30.793893814086914, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2759
goal_identified
goal_identified
=== ep: 2760, time 25.992870807647705, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2760
=== ep: 2761, time 25.894779682159424, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2761
=== ep: 2762, time 25.813057899475098, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2762
goal_identified
=== ep: 2763, time 25.690828561782837, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2763
goal_identified
=== ep: 2764, time 25.3935444355011, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2764
goal_identified
=== ep: 2765, time 25.522416830062866, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2765
goal_identified
=== ep: 2766, time 25.871294021606445, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2766
goal_identified
goal_identified
=== ep: 2767, time 25.384355545043945, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2767
goal_identified
=== ep: 2768, time 25.447037935256958, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2768
goal_identified
goal_identified
goal_identified
=== ep: 2769, time 30.55123281478882, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2769
goal_identified
=== ep: 2770, time 25.689966201782227, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2770
goal_identified
=== ep: 2771, time 25.797072410583496, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2771
goal_identified
=== ep: 2772, time 25.531081914901733, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2772
=== ep: 2773, time 25.51036834716797, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2773
goal_identified
goal_identified
=== ep: 2774, time 25.55648136138916, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2774
goal_identified
goal_identified
=== ep: 2775, time 25.590595245361328, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2775
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2776, time 25.74621343612671, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2776
goal_identified
=== ep: 2777, time 25.635125637054443, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2777
goal_identified
goal_identified
=== ep: 2778, time 25.753085613250732, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2778
goal_identified
goal_identified
=== ep: 2779, time 30.444241523742676, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2779
goal_identified
goal_identified
=== ep: 2780, time 25.877610445022583, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2780
=== ep: 2781, time 25.550281763076782, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2781
goal_identified
goal_identified
=== ep: 2782, time 26.087080717086792, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2782
=== ep: 2783, time 25.740277528762817, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2783
goal_identified
=== ep: 2784, time 25.8183650970459, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2784
goal_identified
goal_identified
=== ep: 2785, time 25.594550371170044, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2785
=== ep: 2786, time 25.889292001724243, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2786
goal_identified
=== ep: 2787, time 26.28550887107849, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2787
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2788, time 25.616113424301147, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2788
goal_identified
goal_identified
=== ep: 2789, time 30.61205244064331, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2789
goal_identified
goal_identified
=== ep: 2790, time 25.34613013267517, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2790
goal_identified
goal_identified
=== ep: 2791, time 26.001499891281128, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2791
=== ep: 2792, time 25.97330665588379, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2792
goal_identified
=== ep: 2793, time 26.09358501434326, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2793
goal_identified
goal_identified
=== ep: 2794, time 26.126240730285645, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2794
=== ep: 2795, time 25.78786325454712, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2795
=== ep: 2796, time 25.746705293655396, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2796
=== ep: 2797, time 25.842209577560425, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2797
goal_identified
=== ep: 2798, time 26.02775812149048, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2798
goal_identified
=== ep: 2799, time 30.575889110565186, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2799
goal_identified
goal_identified
=== ep: 2800, time 25.866368293762207, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2800
goal_identified
=== ep: 2801, time 25.77982187271118, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2801
goal_identified
=== ep: 2802, time 25.90086841583252, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2802
goal_identified
=== ep: 2803, time 25.945032358169556, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2803
goal_identified
goal_identified
=== ep: 2804, time 25.823709964752197, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2804
goal_identified
goal_identified
=== ep: 2805, time 25.70002579689026, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2805
goal_identified
=== ep: 2806, time 25.88181757926941, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2806
goal_identified
goal_identified
=== ep: 2807, time 25.82195281982422, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2807
=== ep: 2808, time 26.398439407348633, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2808
goal_identified
goal_identified
goal_identified
=== ep: 2809, time 30.768060207366943, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2809
goal_identified
goal_identified
goal_identified
=== ep: 2810, time 26.077146291732788, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2810
goal_identified
goal_identified
=== ep: 2811, time 25.70521306991577, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2811
goal_identified
goal_identified
=== ep: 2812, time 26.138585805892944, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2812
goal_identified
goal_identified
=== ep: 2813, time 25.87492084503174, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2813
goal_identified
=== ep: 2814, time 26.3031268119812, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2814
=== ep: 2815, time 25.67717981338501, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2815
=== ep: 2816, time 26.343560457229614, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2816
goal_identified
goal_identified
=== ep: 2817, time 25.704923391342163, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2817
goal_identified
goal_identified
=== ep: 2818, time 25.56612467765808, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2818
=== ep: 2819, time 30.368810892105103, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2819
goal_identified
goal_identified
=== ep: 2820, time 25.802394151687622, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2820
goal_identified
goal_identified
=== ep: 2821, time 25.87702965736389, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2821
goal_identified
=== ep: 2822, time 25.657800912857056, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2822
=== ep: 2823, time 26.057564973831177, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2823
goal_identified
=== ep: 2824, time 25.817800045013428, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2824
goal_identified
=== ep: 2825, time 25.8245587348938, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2825
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2826, time 26.02214026451111, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2826
=== ep: 2827, time 25.807610034942627, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2827
goal_identified
goal_identified
goal_identified
=== ep: 2828, time 25.806493520736694, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2828
goal_identified
goal_identified
=== ep: 2829, time 30.54158329963684, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2829
goal_identified
=== ep: 2830, time 26.000443935394287, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2830
goal_identified
goal_identified
goal_identified
=== ep: 2831, time 25.43259572982788, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2831
=== ep: 2832, time 26.194422960281372, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2832
goal_identified
goal_identified
goal_identified
=== ep: 2833, time 25.76339817047119, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2833
=== ep: 2834, time 25.821423768997192, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2834
=== ep: 2835, time 25.87082862854004, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2835
goal_identified
=== ep: 2836, time 25.845841646194458, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2836
=== ep: 2837, time 26.056121110916138, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2837
goal_identified
=== ep: 2838, time 25.816834449768066, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2838
goal_identified
goal_identified
=== ep: 2839, time 30.657176971435547, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2839
goal_identified
goal_identified
goal_identified
=== ep: 2840, time 25.763694763183594, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2840
goal_identified
=== ep: 2841, time 25.890944957733154, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2841
goal_identified
goal_identified
=== ep: 2842, time 25.897199153900146, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2842
goal_identified
goal_identified
goal_identified
=== ep: 2843, time 25.74020481109619, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2843
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2844, time 25.76242709159851, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2844
=== ep: 2845, time 25.690812587738037, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2845
goal_identified
=== ep: 2846, time 25.872249603271484, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2846
goal_identified
=== ep: 2847, time 25.670344352722168, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 52/52)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2847
goal_identified
goal_identified
=== ep: 2848, time 25.752412796020508, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2848
goal_identified
goal_identified
=== ep: 2849, time 30.390950202941895, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2849
goal_identified
goal_identified
=== ep: 2850, time 26.033479690551758, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2850
goal_identified
=== ep: 2851, time 25.836678743362427, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2851
goal_identified
=== ep: 2852, time 25.829087257385254, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2852
goal_identified
goal_identified
=== ep: 2853, time 25.715829133987427, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2853
goal_identified
=== ep: 2854, time 25.939157247543335, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 54/54)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2854
goal_identified
goal_identified
=== ep: 2855, time 25.71081852912903, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2855
goal_identified
=== ep: 2856, time 25.885990381240845, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2856
=== ep: 2857, time 25.87766671180725, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2857
=== ep: 2858, time 26.240752935409546, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2858
goal_identified
goal_identified
=== ep: 2859, time 30.612876176834106, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2859
goal_identified
=== ep: 2860, time 25.96091651916504, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2860
goal_identified
goal_identified
goal_identified
=== ep: 2861, time 25.906818866729736, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2861
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2862, time 26.140592336654663, eps 0.001, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1870
goal_identified
goal_identified
=== ep: 2863, time 26.050442695617676, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2863
=== ep: 2864, time 26.231191158294678, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2864
goal_identified
=== ep: 2865, time 25.918410539627075, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2865
=== ep: 2866, time 26.167787313461304, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2866
goal_identified
=== ep: 2867, time 26.04234290122986, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2867
goal_identified
goal_identified
goal_identified
=== ep: 2868, time 26.082698583602905, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2868
goal_identified
=== ep: 2869, time 30.87392520904541, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2869
goal_identified
=== ep: 2870, time 26.244160413742065, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2870
goal_identified
=== ep: 2871, time 25.791380405426025, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2871
=== ep: 2872, time 25.98489737510681, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2872
goal_identified
=== ep: 2873, time 26.322351455688477, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2873
goal_identified
=== ep: 2874, time 26.07973551750183, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2874
=== ep: 2875, time 26.140969276428223, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2875
goal_identified
goal_identified
goal_identified
=== ep: 2876, time 25.784151792526245, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2876
goal_identified
=== ep: 2877, time 26.270930528640747, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2877
=== ep: 2878, time 25.56799054145813, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 56/56)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2878
goal_identified
=== ep: 2879, time 30.498278617858887, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2879
goal_identified
=== ep: 2880, time 25.775662183761597, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2880
goal_identified
goal_identified
=== ep: 2881, time 26.02828335762024, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2881
goal_identified
goal_identified
=== ep: 2882, time 25.87298035621643, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2882
goal_identified
goal_identified
=== ep: 2883, time 25.794390439987183, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2883
goal_identified
goal_identified
goal_identified
=== ep: 2884, time 25.841059684753418, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2884
=== ep: 2885, time 25.977546215057373, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2885
goal_identified
=== ep: 2886, time 26.139694690704346, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2886
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2887, time 25.954000234603882, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2887
goal_identified
goal_identified
=== ep: 2888, time 26.075327396392822, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2888
goal_identified
goal_identified
goal_identified
=== ep: 2889, time 31.30109477043152, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2889
goal_identified
goal_identified
goal_identified
=== ep: 2890, time 26.324458360671997, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2890
goal_identified
=== ep: 2891, time 25.7814621925354, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2891
goal_identified
=== ep: 2892, time 25.88494038581848, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2892
goal_identified
goal_identified
=== ep: 2893, time 26.334514141082764, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2893
goal_identified
=== ep: 2894, time 25.85793113708496, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2894
goal_identified
=== ep: 2895, time 25.968114376068115, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2895
goal_identified
goal_identified
=== ep: 2896, time 25.70575499534607, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2896
goal_identified
=== ep: 2897, time 26.070404052734375, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2897
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2898, time 26.031456232070923, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2898
goal_identified
=== ep: 2899, time 30.886715173721313, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2899
=== ep: 2900, time 26.25361442565918, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2900
goal_identified
goal_identified
=== ep: 2901, time 25.943554162979126, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2901
=== ep: 2902, time 25.922340631484985, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2902
goal_identified
goal_identified
=== ep: 2903, time 26.048678398132324, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2903
goal_identified
=== ep: 2904, time 26.09236788749695, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2904
=== ep: 2905, time 26.092607975006104, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2905
goal_identified
goal_identified
=== ep: 2906, time 26.378021955490112, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2906
goal_identified
=== ep: 2907, time 25.999483823776245, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2907
goal_identified
goal_identified
=== ep: 2908, time 26.13249897956848, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2908
=== ep: 2909, time 30.546679735183716, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2909
goal_identified
goal_identified
=== ep: 2910, time 26.24369502067566, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2910
goal_identified
=== ep: 2911, time 25.999457836151123, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2911
goal_identified
goal_identified
=== ep: 2912, time 26.066502571105957, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2912
goal_identified
=== ep: 2913, time 25.76179528236389, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2913
goal_identified
goal_identified
goal_identified
=== ep: 2914, time 25.90054154396057, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2914
=== ep: 2915, time 25.810078144073486, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2915
goal_identified
=== ep: 2916, time 26.22151780128479, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2916
goal_identified
goal_identified
goal_identified
=== ep: 2917, time 26.195448875427246, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2917
goal_identified
goal_identified
goal_identified
=== ep: 2918, time 26.035547733306885, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2918
goal_identified
goal_identified
goal_identified
=== ep: 2919, time 30.814377546310425, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2919
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2920, time 26.012771606445312, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2920
goal_identified
goal_identified
=== ep: 2921, time 25.817606210708618, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2921
goal_identified
goal_identified
=== ep: 2922, time 25.798914909362793, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2922
goal_identified
=== ep: 2923, time 26.060194492340088, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2923
goal_identified
goal_identified
=== ep: 2924, time 25.896939754486084, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2924
goal_identified
goal_identified
=== ep: 2925, time 26.443988800048828, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2925
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2926, time 26.506162405014038, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2926
goal_identified
goal_identified
=== ep: 2927, time 25.936131954193115, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2927
goal_identified
goal_identified
goal_identified
=== ep: 2928, time 25.894526720046997, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2928
goal_identified
=== ep: 2929, time 30.530284881591797, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2929
goal_identified
=== ep: 2930, time 26.320632457733154, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2930
goal_identified
goal_identified
goal_identified
=== ep: 2931, time 25.953174114227295, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2931
goal_identified
=== ep: 2932, time 26.322705268859863, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2932
goal_identified
=== ep: 2933, time 25.808954000473022, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2933
goal_identified
goal_identified
=== ep: 2934, time 26.015687942504883, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2934
goal_identified
=== ep: 2935, time 26.08286738395691, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2935
goal_identified
goal_identified
goal_identified
=== ep: 2936, time 26.062352657318115, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2936
goal_identified
=== ep: 2937, time 26.05117964744568, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2937
=== ep: 2938, time 25.863317489624023, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2938
goal_identified
goal_identified
=== ep: 2939, time 31.064327716827393, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2939
goal_identified
=== ep: 2940, time 25.957090139389038, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2940
=== ep: 2941, time 26.160234212875366, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2941
goal_identified
goal_identified
goal_identified
=== ep: 2942, time 26.292247772216797, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2942
goal_identified
goal_identified
=== ep: 2943, time 26.0658860206604, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2943
=== ep: 2944, time 25.884203672409058, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2944
=== ep: 2945, time 25.64867115020752, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2945
goal_identified
=== ep: 2946, time 26.04971408843994, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2946
goal_identified
=== ep: 2947, time 26.612510681152344, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2947
goal_identified
=== ep: 2948, time 26.2322895526886, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2948
goal_identified
=== ep: 2949, time 30.679033279418945, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2949
goal_identified
=== ep: 2950, time 26.509952783584595, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2950
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2951, time 25.80923867225647, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2951
goal_identified
goal_identified
=== ep: 2952, time 26.016178607940674, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2952
=== ep: 2953, time 26.247666120529175, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2953
goal_identified
goal_identified
=== ep: 2954, time 26.2388334274292, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2954
goal_identified
=== ep: 2955, time 25.757855653762817, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2955
goal_identified
=== ep: 2956, time 25.900171279907227, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2956
goal_identified
=== ep: 2957, time 25.959468603134155, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2957
goal_identified
goal_identified
=== ep: 2958, time 26.01662564277649, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2958
=== ep: 2959, time 31.125160217285156, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2959
goal_identified
=== ep: 2960, time 26.17274236679077, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2960
=== ep: 2961, time 25.96906328201294, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2961
=== ep: 2962, time 26.542145013809204, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2962
goal_identified
=== ep: 2963, time 26.53380298614502, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2963
goal_identified
goal_identified
=== ep: 2964, time 26.092037439346313, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2964
goal_identified
goal_identified
=== ep: 2965, time 26.47778630256653, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2965
goal_identified
goal_identified
=== ep: 2966, time 26.262747764587402, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2966
goal_identified
=== ep: 2967, time 26.02450704574585, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2967
goal_identified
goal_identified
=== ep: 2968, time 26.24162197113037, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2968
goal_identified
goal_identified
goal_identified
=== ep: 2969, time 30.428633213043213, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2969
goal_identified
=== ep: 2970, time 26.05965566635132, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2970
goal_identified
goal_identified
=== ep: 2971, time 25.871745824813843, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2971
goal_identified
=== ep: 2972, time 26.708654165267944, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2972
goal_identified
goal_identified
=== ep: 2973, time 26.157814264297485, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2973
goal_identified
goal_identified
=== ep: 2974, time 26.170308828353882, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2974
goal_identified
goal_identified
=== ep: 2975, time 25.970333576202393, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2975
=== ep: 2976, time 26.296847105026245, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2976
goal_identified
goal_identified
=== ep: 2977, time 26.15263533592224, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2977
goal_identified
goal_identified
=== ep: 2978, time 26.03075647354126, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2978
goal_identified
goal_identified
=== ep: 2979, time 30.73224377632141, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2979
=== ep: 2980, time 26.167542695999146, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2980
=== ep: 2981, time 26.12586545944214, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2981
goal_identified
goal_identified
goal_identified
=== ep: 2982, time 26.186472415924072, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2982
goal_identified
goal_identified
goal_identified
=== ep: 2983, time 26.03169822692871, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2983
goal_identified
=== ep: 2984, time 26.056551933288574, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2984
goal_identified
goal_identified
=== ep: 2985, time 26.21894121170044, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2985
goal_identified
goal_identified
=== ep: 2986, time 26.393986463546753, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2986
goal_identified
goal_identified
=== ep: 2987, time 25.997833251953125, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2987
=== ep: 2988, time 26.102013111114502, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2988
goal_identified
=== ep: 2989, time 30.969788789749146, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2989
goal_identified
goal_identified
goal_identified
=== ep: 2990, time 26.403083086013794, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2990
=== ep: 2991, time 26.172906398773193, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2991
goal_identified
=== ep: 2992, time 26.28555989265442, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2992
goal_identified
goal_identified
=== ep: 2993, time 26.169565677642822, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2993
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2994, time 26.39646625518799, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2994
=== ep: 2995, time 24.805455923080444, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 47/47)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2995
=== ep: 2996, time 25.89870572090149, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2996
=== ep: 2997, time 26.149157285690308, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2997
goal_identified
=== ep: 2998, time 26.205392360687256, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2998
goal_identified
goal_identified
=== ep: 2999, time 30.862691164016724, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2999
goal_identified
goal_identified
=== ep: 3000, time 26.081191301345825, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3000
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3001, time 26.356322050094604, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3001
goal_identified
=== ep: 3002, time 26.008099794387817, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3002
=== ep: 3003, time 26.217090845108032, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3003
goal_identified
goal_identified
=== ep: 3004, time 26.408733129501343, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3004
goal_identified
goal_identified
goal_identified
=== ep: 3005, time 26.319902896881104, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3005
goal_identified
goal_identified
=== ep: 3006, time 25.791247606277466, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3006
goal_identified
=== ep: 3007, time 26.128157377243042, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3007
goal_identified
goal_identified
=== ep: 3008, time 26.327232599258423, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3008
goal_identified
goal_identified
=== ep: 3009, time 30.781673908233643, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3009
goal_identified
=== ep: 3010, time 26.178728342056274, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3010
goal_identified
=== ep: 3011, time 26.261779308319092, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3011
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3012, time 26.36184549331665, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3012
goal_identified
goal_identified
=== ep: 3013, time 26.005913972854614, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3013
goal_identified
=== ep: 3014, time 26.452171564102173, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3014
goal_identified
=== ep: 3015, time 26.329658269882202, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3015
goal_identified
goal_identified
=== ep: 3016, time 26.172626972198486, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3016
goal_identified
goal_identified
=== ep: 3017, time 26.20126700401306, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3017
goal_identified
goal_identified
goal_identified
=== ep: 3018, time 26.078126430511475, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3018
=== ep: 3019, time 31.05247712135315, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3019
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3020, time 26.496031999588013, eps 0.001, sum reward: 4, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3020
goal_identified
=== ep: 3021, time 26.430105447769165, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3021
=== ep: 3022, time 26.436732530593872, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3022
=== ep: 3023, time 26.226624965667725, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3023
goal_identified
goal_identified
=== ep: 3024, time 26.022833585739136, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3024
goal_identified
=== ep: 3025, time 26.222251176834106, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3025
goal_identified
=== ep: 3026, time 26.04407787322998, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3026
goal_identified
=== ep: 3027, time 25.973583221435547, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3027
goal_identified
=== ep: 3028, time 26.638283729553223, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3028
goal_identified
=== ep: 3029, time 30.798062562942505, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3029
goal_identified
goal_identified
=== ep: 3030, time 26.15910530090332, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3030
goal_identified
goal_identified
goal_identified
=== ep: 3031, time 26.095712184906006, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3031
goal_identified
goal_identified
=== ep: 3032, time 26.237732887268066, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3032
goal_identified
=== ep: 3033, time 26.01157855987549, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3033
goal_identified
goal_identified
=== ep: 3034, time 26.097358465194702, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3034
goal_identified
goal_identified
=== ep: 3035, time 26.546212434768677, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3035
=== ep: 3036, time 26.25665044784546, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3036
goal_identified
=== ep: 3037, time 26.103185892105103, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3037
goal_identified
goal_identified
=== ep: 3038, time 25.92403745651245, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3038
goal_identified
=== ep: 3039, time 30.947754621505737, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3039
goal_identified
goal_identified
goal_identified
=== ep: 3040, time 26.13404393196106, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3040
goal_identified
goal_identified
=== ep: 3041, time 26.38525366783142, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3041
goal_identified
goal_identified
=== ep: 3042, time 25.91971707344055, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3042
goal_identified
goal_identified
=== ep: 3043, time 26.238754987716675, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3043
goal_identified
=== ep: 3044, time 26.0482976436615, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3044
goal_identified
goal_identified
=== ep: 3045, time 26.56930661201477, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3045
=== ep: 3046, time 26.064905166625977, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3046
goal_identified
goal_identified
=== ep: 3047, time 26.29153609275818, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3047
goal_identified
=== ep: 3048, time 26.23350477218628, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3048
=== ep: 3049, time 31.092267513275146, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3049
=== ep: 3050, time 26.388423681259155, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3050
=== ep: 3051, time 25.999656438827515, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3051
=== ep: 3052, time 26.1939115524292, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3052
goal_identified
goal_identified
goal_identified
=== ep: 3053, time 26.27041506767273, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3053
=== ep: 3054, time 26.366917848587036, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3054
=== ep: 3055, time 26.16862154006958, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3055
goal_identified
=== ep: 3056, time 26.44070792198181, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3056
goal_identified
goal_identified
=== ep: 3057, time 26.03678321838379, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3057
goal_identified
goal_identified
=== ep: 3058, time 26.406241178512573, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3058
=== ep: 3059, time 30.930432081222534, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3059
goal_identified
goal_identified
=== ep: 3060, time 26.12673330307007, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3060
goal_identified
=== ep: 3061, time 25.811774253845215, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3061
goal_identified
=== ep: 3062, time 26.327880859375, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3062
=== ep: 3063, time 26.58479642868042, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3063
goal_identified
=== ep: 3064, time 25.958925247192383, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3064
goal_identified
goal_identified
goal_identified
=== ep: 3065, time 26.34593176841736, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3065
goal_identified
=== ep: 3066, time 26.005106687545776, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3066
=== ep: 3067, time 26.48568296432495, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3067
goal_identified
goal_identified
=== ep: 3068, time 26.62399196624756, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3068
=== ep: 3069, time 31.067713975906372, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3069
goal_identified
=== ep: 3070, time 26.336211681365967, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3070
goal_identified
goal_identified
goal_identified
=== ep: 3071, time 26.1889066696167, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3071
goal_identified
goal_identified
goal_identified
=== ep: 3072, time 26.37183928489685, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3072
goal_identified
goal_identified
=== ep: 3073, time 26.68857765197754, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3073
goal_identified
goal_identified
=== ep: 3074, time 26.195067882537842, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3074
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3075, time 26.321083545684814, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3075
goal_identified
goal_identified
=== ep: 3076, time 26.673877954483032, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3076
goal_identified
=== ep: 3077, time 26.212948322296143, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3077
goal_identified
=== ep: 3078, time 26.03229594230652, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3078
goal_identified
=== ep: 3079, time 30.524361848831177, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3079
goal_identified
goal_identified
=== ep: 3080, time 26.13378119468689, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3080
goal_identified
goal_identified
=== ep: 3081, time 26.63952612876892, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3081
goal_identified
=== ep: 3082, time 26.071512699127197, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3082
goal_identified
goal_identified
=== ep: 3083, time 26.47463083267212, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3083
=== ep: 3084, time 26.66116166114807, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3084
goal_identified
goal_identified
=== ep: 3085, time 26.248892784118652, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3085
goal_identified
=== ep: 3086, time 25.870161056518555, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3086
=== ep: 3087, time 26.101864337921143, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3087
=== ep: 3088, time 26.08902621269226, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3088
goal_identified
=== ep: 3089, time 30.667036771774292, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3089
goal_identified
=== ep: 3090, time 26.051860809326172, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3090
goal_identified
goal_identified
goal_identified
=== ep: 3091, time 26.127279043197632, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3091
goal_identified
=== ep: 3092, time 26.18657374382019, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3092
goal_identified
goal_identified
=== ep: 3093, time 26.034162759780884, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3093
goal_identified
goal_identified
goal_identified
=== ep: 3094, time 26.161133527755737, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3094
goal_identified
=== ep: 3095, time 26.45317053794861, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3095
goal_identified
goal_identified
goal_identified
=== ep: 3096, time 26.450924396514893, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3096
goal_identified
goal_identified
=== ep: 3097, time 26.54425573348999, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3097
goal_identified
=== ep: 3098, time 25.956064462661743, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3098
goal_identified
=== ep: 3099, time 30.988080501556396, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3099
goal_identified
goal_identified
=== ep: 3100, time 26.186357736587524, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3100
=== ep: 3101, time 26.524584770202637, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3101
goal_identified
goal_identified
=== ep: 3102, time 26.296266794204712, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3102
goal_identified
=== ep: 3103, time 26.182745695114136, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 55/55)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3103
=== ep: 3104, time 26.548861742019653, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3104
goal_identified
=== ep: 3105, time 26.164554119110107, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3105
=== ep: 3106, time 26.159748077392578, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3106
goal_identified
goal_identified
goal_identified
=== ep: 3107, time 26.25232195854187, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3107
goal_identified
goal_identified
=== ep: 3108, time 26.16910481452942, eps 0.001, sum reward: 2, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3108
goal_identified
goal_identified
=== ep: 3109, time 30.88640332221985, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3109
goal_identified
goal_identified
=== ep: 3110, time 26.16476035118103, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3110
goal_identified
=== ep: 3111, time 26.60073208808899, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3111
goal_identified
goal_identified
=== ep: 3112, time 26.247568130493164, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 123/123)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3112
=== ep: 3113, time 26.243266344070435, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3113
goal_identified
=== ep: 3114, time 26.327609539031982, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3114
goal_identified
=== ep: 3115, time 26.16559362411499, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3115
goal_identified
goal_identified
goal_identified
=== ep: 3116, time 26.46467900276184, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3116
goal_identified
=== ep: 3117, time 26.39903712272644, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3117
=== ep: 3118, time 26.526036024093628, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3118
goal_identified
=== ep: 3119, time 30.60995078086853, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3119
goal_identified
goal_identified
=== ep: 3120, time 26.08051586151123, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3120
goal_identified
=== ep: 3121, time 26.39134979248047, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3121
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3122, time 26.044487237930298, eps 0.001, sum reward: 5, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3122
goal_identified
goal_identified
=== ep: 3123, time 26.222779273986816, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3123
goal_identified
goal_identified
=== ep: 3124, time 26.386595726013184, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3124
goal_identified
goal_identified
goal_identified
=== ep: 3125, time 26.069000959396362, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3125
goal_identified
=== ep: 3126, time 26.168774127960205, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3126
goal_identified
goal_identified
goal_identified
=== ep: 3127, time 26.281099796295166, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3127
=== ep: 3128, time 26.399091005325317, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3128
goal_identified
goal_identified
=== ep: 3129, time 31.02318286895752, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3129
goal_identified
=== ep: 3130, time 26.138200283050537, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3130
goal_identified
=== ep: 3131, time 26.244060277938843, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3131
goal_identified
goal_identified
goal_identified
=== ep: 3132, time 26.58730912208557, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3132
=== ep: 3133, time 26.62972378730774, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3133
goal_identified
=== ep: 3134, time 26.166746616363525, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3134
goal_identified
goal_identified
goal_identified
=== ep: 3135, time 26.2433762550354, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3135
goal_identified
=== ep: 3136, time 26.61116123199463, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3136
goal_identified
=== ep: 3137, time 26.75082492828369, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3137
=== ep: 3138, time 26.492713451385498, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3138
goal_identified
goal_identified
goal_identified
=== ep: 3139, time 30.951821088790894, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3139
goal_identified
goal_identified
goal_identified
=== ep: 3140, time 26.124165773391724, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3140
=== ep: 3141, time 26.528525352478027, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3141
goal_identified
goal_identified
=== ep: 3142, time 26.1589138507843, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3142
goal_identified
=== ep: 3143, time 26.644524097442627, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3143
goal_identified
goal_identified
=== ep: 3144, time 26.409438133239746, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3144
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3145, time 26.20699906349182, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3145
=== ep: 3146, time 26.344292163848877, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3146
goal_identified
=== ep: 3147, time 26.44342350959778, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3147
goal_identified
=== ep: 3148, time 26.796013593673706, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3148
goal_identified
goal_identified
goal_identified
=== ep: 3149, time 31.32772970199585, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3149
goal_identified
=== ep: 3150, time 26.21672296524048, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3150
=== ep: 3151, time 26.48208236694336, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3151
=== ep: 3152, time 25.993043661117554, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3152
goal_identified
goal_identified
=== ep: 3153, time 26.67185091972351, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3153
=== ep: 3154, time 26.27034878730774, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3154
goal_identified
=== ep: 3155, time 26.213513135910034, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3155
goal_identified
goal_identified
=== ep: 3156, time 26.226362466812134, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3156
goal_identified
=== ep: 3157, time 26.510957717895508, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3157
goal_identified
=== ep: 3158, time 26.437028169631958, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3158
goal_identified
=== ep: 3159, time 30.651833534240723, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3159
goal_identified
=== ep: 3160, time 26.393228769302368, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3160
goal_identified
goal_identified
=== ep: 3161, time 26.26025629043579, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3161
goal_identified
goal_identified
=== ep: 3162, time 26.112603664398193, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3162
=== ep: 3163, time 31.196918725967407, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3163
goal_identified
goal_identified
goal_identified
=== ep: 3164, time 26.508365392684937, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3164
goal_identified
=== ep: 3165, time 25.727275609970093, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3165
=== ep: 3166, time 26.570727825164795, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3166
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3167, time 26.103400230407715, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3167
goal_identified
=== ep: 3168, time 26.748646020889282, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3168
goal_identified
goal_identified
=== ep: 3169, time 30.900209426879883, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3169
goal_identified
=== ep: 3170, time 25.894201517105103, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3170
=== ep: 3171, time 26.15419888496399, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3171
goal_identified
goal_identified
goal_identified
=== ep: 3172, time 26.271865367889404, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3172
goal_identified
goal_identified
=== ep: 3173, time 26.30874013900757, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 124/124)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3173
goal_identified
=== ep: 3174, time 26.114105463027954, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3174
=== ep: 3175, time 26.22322154045105, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3175
goal_identified
goal_identified
goal_identified
=== ep: 3176, time 26.399952173233032, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3176
goal_identified
goal_identified
goal_identified
=== ep: 3177, time 26.321391105651855, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3177
=== ep: 3178, time 26.31484627723694, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3178
=== ep: 3179, time 30.607192754745483, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3179
goal_identified
goal_identified
goal_identified
=== ep: 3180, time 26.68725347518921, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3180
goal_identified
goal_identified
=== ep: 3181, time 26.142313957214355, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3181
goal_identified
goal_identified
=== ep: 3182, time 26.4366455078125, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3182
goal_identified
goal_identified
=== ep: 3183, time 26.449086666107178, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3183
goal_identified
=== ep: 3184, time 26.226659297943115, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3184
=== ep: 3185, time 26.707890272140503, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3185
=== ep: 3186, time 26.411661386489868, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3186
=== ep: 3187, time 25.96641778945923, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3187
goal_identified
goal_identified
=== ep: 3188, time 26.419352531433105, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3188
goal_identified
goal_identified
goal_identified
=== ep: 3189, time 30.945333003997803, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3189
goal_identified
goal_identified
=== ep: 3190, time 26.320353507995605, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3190
goal_identified
goal_identified
=== ep: 3191, time 26.356341123580933, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3191
goal_identified
=== ep: 3192, time 25.907655000686646, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3192
goal_identified
=== ep: 3193, time 27.095642805099487, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3193
goal_identified
goal_identified
=== ep: 3194, time 26.067198276519775, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3194
goal_identified
goal_identified
=== ep: 3195, time 26.11962580680847, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3195
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3196, time 25.99367046356201, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3196
=== ep: 3197, time 26.34968852996826, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3197
=== ep: 3198, time 26.45004367828369, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3198
goal_identified
goal_identified
=== ep: 3199, time 30.84465503692627, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3199
=== ep: 3200, time 26.752268075942993, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3200
=== ep: 3201, time 26.165308237075806, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3201
=== ep: 3202, time 26.322261333465576, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3202
=== ep: 3203, time 26.293267250061035, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3203
goal_identified
=== ep: 3204, time 26.498677730560303, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3204
goal_identified
=== ep: 3205, time 26.07399010658264, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3205
goal_identified
goal_identified
goal_identified
=== ep: 3206, time 26.293230295181274, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3206
goal_identified
=== ep: 3207, time 26.27630043029785, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3207
goal_identified
=== ep: 3208, time 26.517319202423096, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3208
goal_identified
goal_identified
=== ep: 3209, time 31.0464985370636, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3209
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3210, time 26.418395280838013, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3210
goal_identified
=== ep: 3211, time 26.569122076034546, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3211
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3212, time 26.15583038330078, eps 0.001, sum reward: 5, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3212
goal_identified
=== ep: 3213, time 26.21413826942444, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3213
goal_identified
goal_identified
=== ep: 3214, time 26.17778778076172, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3214
goal_identified
=== ep: 3215, time 26.47070860862732, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3215
=== ep: 3216, time 26.663326740264893, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3216
goal_identified
goal_identified
goal_identified
=== ep: 3217, time 26.374067068099976, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3217
goal_identified
=== ep: 3218, time 26.40991735458374, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3218
goal_identified
goal_identified
=== ep: 3219, time 31.026418209075928, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3219
=== ep: 3220, time 26.24574589729309, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3220
goal_identified
goal_identified
=== ep: 3221, time 26.182130813598633, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3221
goal_identified
=== ep: 3222, time 26.408501148223877, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3222
goal_identified
=== ep: 3223, time 26.331629037857056, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3223
goal_identified
=== ep: 3224, time 26.537302017211914, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3224
=== ep: 3225, time 26.12124800682068, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3225
goal_identified
goal_identified
goal_identified
=== ep: 3226, time 26.116976499557495, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3226
goal_identified
goal_identified
=== ep: 3227, time 26.43391752243042, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3227
=== ep: 3228, time 26.040297746658325, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3228
=== ep: 3229, time 31.31452488899231, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3229
goal_identified
=== ep: 3230, time 26.381843090057373, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3230
goal_identified
=== ep: 3231, time 26.382056713104248, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3231
goal_identified
=== ep: 3232, time 26.587764739990234, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3232
goal_identified
goal_identified
=== ep: 3233, time 26.514659643173218, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3233
goal_identified
=== ep: 3234, time 26.72607731819153, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3234
goal_identified
=== ep: 3235, time 26.502652645111084, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3235
=== ep: 3236, time 26.18281316757202, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3236
goal_identified
=== ep: 3237, time 26.34325408935547, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3237
goal_identified
=== ep: 3238, time 26.226399183273315, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3238
goal_identified
=== ep: 3239, time 31.206950426101685, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3239
goal_identified
goal_identified
=== ep: 3240, time 26.267133712768555, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3240
goal_identified
=== ep: 3241, time 26.63713264465332, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3241
goal_identified
=== ep: 3242, time 26.06398344039917, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3242
goal_identified
=== ep: 3243, time 26.332715034484863, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3243
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3244, time 26.50266170501709, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3244
goal_identified
=== ep: 3245, time 26.414607763290405, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3245
=== ep: 3246, time 26.32690405845642, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3246
goal_identified
goal_identified
=== ep: 3247, time 26.37350630760193, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3247
goal_identified
=== ep: 3248, time 26.17071557044983, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3248
=== ep: 3249, time 30.849613428115845, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3249
goal_identified
=== ep: 3250, time 26.348535776138306, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3250
goal_identified
=== ep: 3251, time 26.18060040473938, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3251
=== ep: 3252, time 26.210237741470337, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3252
goal_identified
goal_identified
goal_identified
=== ep: 3253, time 26.78928852081299, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3253
goal_identified
=== ep: 3254, time 26.374778270721436, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3254
goal_identified
goal_identified
goal_identified
=== ep: 3255, time 26.50406050682068, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3255
=== ep: 3256, time 26.333486795425415, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3256
goal_identified
=== ep: 3257, time 26.414140462875366, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3257
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3258, time 26.383389949798584, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3258
goal_identified
goal_identified
goal_identified
=== ep: 3259, time 30.668047189712524, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3259
goal_identified
goal_identified
=== ep: 3260, time 26.399624586105347, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3260
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3261, time 26.194605827331543, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2142
goal_identified
=== ep: 3262, time 26.339951753616333, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3262
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3263, time 26.43878483772278, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3263
goal_identified
=== ep: 3264, time 26.162463426589966, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3264
goal_identified
goal_identified
=== ep: 3265, time 26.385590314865112, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3265
goal_identified
goal_identified
=== ep: 3266, time 26.09067964553833, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3266
goal_identified
=== ep: 3267, time 26.39377450942993, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3267
goal_identified
=== ep: 3268, time 26.49091386795044, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3268
=== ep: 3269, time 31.192681074142456, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3269
=== ep: 3270, time 26.364035606384277, eps 0.001, sum reward: 0, score_diff -5, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3270
goal_identified
=== ep: 3271, time 26.204553365707397, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3271
=== ep: 3272, time 26.504640817642212, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3272
goal_identified
goal_identified
goal_identified
=== ep: 3273, time 26.160311698913574, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3273
=== ep: 3274, time 26.409981966018677, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3274
goal_identified
goal_identified
=== ep: 3275, time 26.149574756622314, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3275
=== ep: 3276, time 26.510427236557007, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3276
goal_identified
goal_identified
=== ep: 3277, time 26.446865558624268, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3277
goal_identified
goal_identified
=== ep: 3278, time 26.320396423339844, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3278
=== ep: 3279, time 30.728617429733276, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3279
goal_identified
goal_identified
goal_identified
=== ep: 3280, time 26.011392831802368, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3280
goal_identified
goal_identified
=== ep: 3281, time 26.55534815788269, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3281
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3282, time 26.347341299057007, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3282
=== ep: 3283, time 26.31035852432251, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3283
=== ep: 3284, time 26.342713594436646, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3284
goal_identified
=== ep: 3285, time 26.345898866653442, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3285
goal_identified
=== ep: 3286, time 26.437085390090942, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3286
goal_identified
goal_identified
=== ep: 3287, time 26.312788724899292, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3287
goal_identified
goal_identified
goal_identified
=== ep: 3288, time 26.147236108779907, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3288
=== ep: 3289, time 30.72528862953186, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3289
goal_identified
goal_identified
goal_identified
=== ep: 3290, time 26.40541386604309, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3290
goal_identified
goal_identified
=== ep: 3291, time 26.234842777252197, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3291
goal_identified
goal_identified
=== ep: 3292, time 26.89133381843567, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3292
goal_identified
goal_identified
=== ep: 3293, time 26.016796827316284, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3293
goal_identified
=== ep: 3294, time 26.539582014083862, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3294
goal_identified
=== ep: 3295, time 26.266857147216797, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3295
goal_identified
goal_identified
goal_identified
=== ep: 3296, time 25.7167706489563, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3296
=== ep: 3297, time 26.312811613082886, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3297
=== ep: 3298, time 26.435548782348633, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3298
=== ep: 3299, time 31.05786681175232, eps 0.001, sum reward: 0, score_diff -5, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3299
=== ep: 3300, time 26.354063987731934, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3300
goal_identified
=== ep: 3301, time 26.666746854782104, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3301
goal_identified
goal_identified
=== ep: 3302, time 26.348867416381836, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3302
goal_identified
goal_identified
=== ep: 3303, time 26.334914922714233, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3303
goal_identified
goal_identified
goal_identified
=== ep: 3304, time 26.104255199432373, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3304
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3305, time 26.273419618606567, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3305
=== ep: 3306, time 26.670255184173584, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3306
goal_identified
goal_identified
goal_identified
=== ep: 3307, time 26.344281435012817, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3307
=== ep: 3308, time 26.874633073806763, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3308
goal_identified
goal_identified
=== ep: 3309, time 30.699838638305664, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3309
goal_identified
=== ep: 3310, time 25.99126148223877, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3310
=== ep: 3311, time 26.564865589141846, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3311
goal_identified
=== ep: 3312, time 26.293070316314697, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3312
goal_identified
goal_identified
=== ep: 3313, time 26.353999614715576, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3313
goal_identified
=== ep: 3314, time 26.685914754867554, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3314
=== ep: 3315, time 25.86870288848877, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3315
goal_identified
goal_identified
=== ep: 3316, time 26.475176095962524, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3316
goal_identified
=== ep: 3317, time 26.005163431167603, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3317
goal_identified
goal_identified
goal_identified
=== ep: 3318, time 26.488699197769165, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3318
goal_identified
goal_identified
=== ep: 3319, time 30.676625728607178, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3319
goal_identified
goal_identified
goal_identified
=== ep: 3320, time 26.09282374382019, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3320
goal_identified
=== ep: 3321, time 26.563299655914307, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3321
=== ep: 3322, time 26.73782515525818, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3322
goal_identified
=== ep: 3323, time 26.425527095794678, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3323
goal_identified
=== ep: 3324, time 26.542242765426636, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3324
goal_identified
goal_identified
goal_identified
=== ep: 3325, time 25.67015242576599, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3325
goal_identified
goal_identified
goal_identified
=== ep: 3326, time 26.491665601730347, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3326
goal_identified
=== ep: 3327, time 26.07623815536499, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3327
goal_identified
goal_identified
=== ep: 3328, time 26.2492196559906, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3328
goal_identified
=== ep: 3329, time 30.956230640411377, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3329
goal_identified
=== ep: 3330, time 26.561483144760132, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3330
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3331, time 26.468108892440796, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3331
goal_identified
=== ep: 3332, time 26.422398567199707, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3332
goal_identified
=== ep: 3333, time 26.05571699142456, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3333
goal_identified
goal_identified
=== ep: 3334, time 26.52114748954773, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3334
goal_identified
=== ep: 3335, time 26.107255697250366, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3335
=== ep: 3336, time 26.65238046646118, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3336
=== ep: 3337, time 26.43224596977234, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3337
goal_identified
=== ep: 3338, time 26.225539684295654, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3338
goal_identified
=== ep: 3339, time 31.207612991333008, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3339
=== ep: 3340, time 26.706831693649292, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3340
goal_identified
=== ep: 3341, time 26.209096670150757, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3341
=== ep: 3342, time 26.819900035858154, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3342
=== ep: 3343, time 26.70537495613098, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3343
goal_identified
goal_identified
=== ep: 3344, time 26.26603102684021, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3344
goal_identified
=== ep: 3345, time 26.196595668792725, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3345
goal_identified
goal_identified
goal_identified
=== ep: 3346, time 26.32271122932434, eps 0.001, sum reward: 3, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3346
goal_identified
=== ep: 3347, time 26.370965242385864, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3347
goal_identified
goal_identified
=== ep: 3348, time 26.308602571487427, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3348
goal_identified
=== ep: 3349, time 31.035844087600708, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3349
goal_identified
=== ep: 3350, time 26.389789581298828, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3350
goal_identified
=== ep: 3351, time 26.32776403427124, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3351
goal_identified
goal_identified
=== ep: 3352, time 26.127612590789795, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3352
goal_identified
goal_identified
=== ep: 3353, time 26.414823055267334, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3353
goal_identified
=== ep: 3354, time 26.0418963432312, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 48/48)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3354
goal_identified
goal_identified
=== ep: 3355, time 26.19281506538391, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3355
=== ep: 3356, time 26.310161113739014, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3356
goal_identified
goal_identified
goal_identified
=== ep: 3357, time 26.39113759994507, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3357
=== ep: 3358, time 26.519951581954956, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3358
goal_identified
goal_identified
goal_identified
=== ep: 3359, time 30.855452299118042, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3359
goal_identified
goal_identified
goal_identified
=== ep: 3360, time 26.54179334640503, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3360
=== ep: 3361, time 26.453096866607666, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3361
=== ep: 3362, time 26.42685341835022, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3362
goal_identified
goal_identified
=== ep: 3363, time 26.58713412284851, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3363
goal_identified
=== ep: 3364, time 26.33476686477661, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3364
=== ep: 3365, time 26.867172718048096, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3365
goal_identified
goal_identified
goal_identified
=== ep: 3366, time 26.511959314346313, eps 0.001, sum reward: 3, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3366
goal_identified
goal_identified
goal_identified
=== ep: 3367, time 26.31208825111389, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3367
goal_identified
goal_identified
=== ep: 3368, time 25.905174016952515, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3368
goal_identified
goal_identified
goal_identified
=== ep: 3369, time 30.92162013053894, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3369
goal_identified
goal_identified
=== ep: 3370, time 26.018012762069702, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3370
=== ep: 3371, time 26.006821393966675, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3371
goal_identified
goal_identified
=== ep: 3372, time 26.323006629943848, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3372
goal_identified
goal_identified
=== ep: 3373, time 26.21449637413025, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3373
=== ep: 3374, time 26.47631001472473, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3374
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3375, time 26.33359694480896, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3375
goal_identified
goal_identified
=== ep: 3376, time 26.3058180809021, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3376
goal_identified
goal_identified
=== ep: 3377, time 26.45912790298462, eps 0.001, sum reward: 2, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3377
goal_identified
=== ep: 3378, time 26.315228700637817, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 104/104)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3378
=== ep: 3379, time 30.817452669143677, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3379
goal_identified
=== ep: 3380, time 25.98321223258972, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3380
goal_identified
goal_identified
=== ep: 3381, time 26.154205560684204, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3381
goal_identified
=== ep: 3382, time 26.575082302093506, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3382
goal_identified
goal_identified
goal_identified
=== ep: 3383, time 26.103434801101685, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3383
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3384, time 25.98295569419861, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3384
goal_identified
=== ep: 3385, time 26.523011207580566, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3385
goal_identified
goal_identified
=== ep: 3386, time 26.20196795463562, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3386
goal_identified
goal_identified
=== ep: 3387, time 26.389354705810547, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 113/113)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3387
goal_identified
=== ep: 3388, time 26.575193881988525, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3388
goal_identified
goal_identified
=== ep: 3389, time 30.969412088394165, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3389
=== ep: 3390, time 26.621928691864014, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3390
goal_identified
goal_identified
=== ep: 3391, time 26.482446670532227, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3391
goal_identified
=== ep: 3392, time 26.57572031021118, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3392
goal_identified
=== ep: 3393, time 26.1940279006958, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3393
goal_identified
goal_identified
=== ep: 3394, time 26.27086567878723, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3394
goal_identified
=== ep: 3395, time 25.948559045791626, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3395
goal_identified
goal_identified
=== ep: 3396, time 26.17260432243347, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3396
=== ep: 3397, time 26.034571886062622, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3397
=== ep: 3398, time 26.415261030197144, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 117/117)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3398
=== ep: 3399, time 30.565302848815918, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3399
goal_identified
goal_identified
goal_identified
=== ep: 3400, time 26.16407036781311, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3400
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3401, time 26.22632074356079, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3401
goal_identified
=== ep: 3402, time 25.815127849578857, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3402
goal_identified
=== ep: 3403, time 26.263917922973633, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3403
=== ep: 3404, time 26.55540919303894, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3404
goal_identified
=== ep: 3405, time 26.4824960231781, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3405
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3406, time 26.32865571975708, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3406
goal_identified
=== ep: 3407, time 26.3004412651062, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3407
goal_identified
goal_identified
=== ep: 3408, time 26.21147084236145, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3408
goal_identified
=== ep: 3409, time 30.93450355529785, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3409
goal_identified
=== ep: 3410, time 26.808265686035156, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3410
=== ep: 3411, time 26.63536262512207, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3411
goal_identified
goal_identified
=== ep: 3412, time 26.52124834060669, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3412
=== ep: 3413, time 26.6357364654541, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3413
=== ep: 3414, time 26.42082691192627, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3414
goal_identified
=== ep: 3415, time 26.43638324737549, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3415
goal_identified
=== ep: 3416, time 26.226709127426147, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3416
goal_identified
=== ep: 3417, time 26.200938940048218, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3417
=== ep: 3418, time 26.770410299301147, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3418
goal_identified
=== ep: 3419, time 30.849266290664673, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3419
goal_identified
goal_identified
goal_identified
=== ep: 3420, time 26.252641439437866, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3420
goal_identified
=== ep: 3421, time 26.06118631362915, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3421
goal_identified
goal_identified
=== ep: 3422, time 26.177783966064453, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3422
goal_identified
=== ep: 3423, time 26.544272422790527, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3423
=== ep: 3424, time 26.109728574752808, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3424
=== ep: 3425, time 26.316153287887573, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3425
goal_identified
=== ep: 3426, time 25.875231981277466, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3426
goal_identified
goal_identified
=== ep: 3427, time 26.307876110076904, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3427
goal_identified
=== ep: 3428, time 26.180701971054077, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3428
goal_identified
=== ep: 3429, time 31.282468557357788, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3429
goal_identified
goal_identified
goal_identified
=== ep: 3430, time 26.483335494995117, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3430
goal_identified
goal_identified
=== ep: 3431, time 26.371440410614014, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3431
goal_identified
goal_identified
=== ep: 3432, time 26.410812616348267, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3432
goal_identified
goal_identified
goal_identified
=== ep: 3433, time 25.97638702392578, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3433
=== ep: 3434, time 26.0898597240448, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3434
goal_identified
goal_identified
goal_identified
=== ep: 3435, time 26.455185413360596, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3435
goal_identified
goal_identified
=== ep: 3436, time 26.383768558502197, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3436
goal_identified
goal_identified
=== ep: 3437, time 26.124969244003296, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3437
goal_identified
goal_identified
=== ep: 3438, time 26.428935050964355, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3438
=== ep: 3439, time 31.02629518508911, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3439
goal_identified
goal_identified
goal_identified
=== ep: 3440, time 26.109612703323364, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3440
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3441, time 25.94234275817871, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3441
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3442, time 26.245219469070435, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3442
=== ep: 3443, time 26.167952060699463, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3443
=== ep: 3444, time 26.529179334640503, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3444
=== ep: 3445, time 26.44208288192749, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3445
goal_identified
=== ep: 3446, time 26.239248037338257, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3446
=== ep: 3447, time 26.631612300872803, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3447
goal_identified
=== ep: 3448, time 26.181166648864746, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3448
goal_identified
goal_identified
=== ep: 3449, time 31.07157611846924, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3449
goal_identified
=== ep: 3450, time 26.548555612564087, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3450
=== ep: 3451, time 26.591654062271118, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3451
goal_identified
goal_identified
goal_identified
=== ep: 3452, time 26.19230365753174, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3452
=== ep: 3453, time 26.33880376815796, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3453
goal_identified
goal_identified
goal_identified
=== ep: 3454, time 26.04883360862732, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3454
goal_identified
goal_identified
=== ep: 3455, time 26.218801736831665, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3455
goal_identified
=== ep: 3456, time 26.20217490196228, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3456
goal_identified
goal_identified
goal_identified
=== ep: 3457, time 26.231898546218872, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3457
=== ep: 3458, time 26.41379404067993, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3458
goal_identified
=== ep: 3459, time 31.18639612197876, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3459
goal_identified
goal_identified
goal_identified
=== ep: 3460, time 26.24996590614319, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3460
=== ep: 3461, time 26.400604248046875, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3461
goal_identified
=== ep: 3462, time 26.1216139793396, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3462
goal_identified
goal_identified
=== ep: 3463, time 26.36630082130432, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3463
=== ep: 3464, time 25.902000904083252, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3464
goal_identified
=== ep: 3465, time 26.37807011604309, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3465
goal_identified
goal_identified
=== ep: 3466, time 26.24828791618347, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3466
goal_identified
=== ep: 3467, time 26.51783013343811, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3467
=== ep: 3468, time 26.622178554534912, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3468
goal_identified
goal_identified
=== ep: 3469, time 31.08116316795349, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3469
goal_identified
=== ep: 3470, time 26.288584232330322, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3470
goal_identified
goal_identified
=== ep: 3471, time 26.31772470474243, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3471
goal_identified
goal_identified
=== ep: 3472, time 26.26052689552307, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3472
goal_identified
=== ep: 3473, time 26.488510847091675, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3473
goal_identified
=== ep: 3474, time 26.434269428253174, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3474
goal_identified
goal_identified
=== ep: 3475, time 26.05535340309143, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3475
goal_identified
=== ep: 3476, time 26.595810413360596, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3476
=== ep: 3477, time 26.127246141433716, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3477
goal_identified
=== ep: 3478, time 26.489322185516357, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3478
goal_identified
goal_identified
goal_identified
=== ep: 3479, time 30.849360466003418, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3479
goal_identified
goal_identified
=== ep: 3480, time 26.581533432006836, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3480
=== ep: 3481, time 26.70098352432251, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3481
=== ep: 3482, time 26.446720600128174, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3482
goal_identified
=== ep: 3483, time 26.53947424888611, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3483
goal_identified
=== ep: 3484, time 26.29390025138855, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3484
=== ep: 3485, time 26.334015369415283, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3485
goal_identified
=== ep: 3486, time 26.26846933364868, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3486
goal_identified
goal_identified
=== ep: 3487, time 26.23760747909546, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3487
=== ep: 3488, time 26.63831400871277, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3488
goal_identified
goal_identified
goal_identified
=== ep: 3489, time 30.797300100326538, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3489
goal_identified
=== ep: 3490, time 26.55756378173828, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3490
goal_identified
goal_identified
goal_identified
=== ep: 3491, time 26.581705570220947, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3491
goal_identified
=== ep: 3492, time 26.461328983306885, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3492
goal_identified
=== ep: 3493, time 25.899653911590576, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 53/53)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3493
goal_identified
=== ep: 3494, time 26.055522680282593, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3494
goal_identified
goal_identified
goal_identified
=== ep: 3495, time 26.41742205619812, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3495
goal_identified
=== ep: 3496, time 26.356759548187256, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3496
goal_identified
goal_identified
goal_identified
=== ep: 3497, time 26.277156591415405, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3497
goal_identified
goal_identified
=== ep: 3498, time 26.4610493183136, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3498
goal_identified
=== ep: 3499, time 30.538246154785156, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3499
goal_identified
goal_identified
goal_identified
=== ep: 3500, time 26.0058491230011, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3500
=== ep: 3501, time 26.51987338066101, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3501
goal_identified
goal_identified
goal_identified
=== ep: 3502, time 26.135129690170288, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3502
=== ep: 3503, time 26.545950651168823, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3503
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3504, time 26.181969165802002, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3504
goal_identified
goal_identified
=== ep: 3505, time 27.053834676742554, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3505
goal_identified
goal_identified
=== ep: 3506, time 26.036717653274536, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3506
=== ep: 3507, time 26.315256118774414, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3507
goal_identified
goal_identified
=== ep: 3508, time 26.385252475738525, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3508
goal_identified
=== ep: 3509, time 31.01906728744507, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3509
goal_identified
goal_identified
=== ep: 3510, time 26.47260284423828, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3510
goal_identified
=== ep: 3511, time 26.53674817085266, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3511
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3512, time 26.284419298171997, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3512
goal_identified
=== ep: 3513, time 26.372050523757935, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3513
goal_identified
=== ep: 3514, time 26.368818521499634, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3514
goal_identified
=== ep: 3515, time 26.65484595298767, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 110/110)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3515
=== ep: 3516, time 26.45706558227539, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3516
goal_identified
goal_identified
=== ep: 3517, time 26.22369623184204, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3517
=== ep: 3518, time 26.35632610321045, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3518
=== ep: 3519, time 30.90153431892395, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3519
=== ep: 3520, time 26.066237926483154, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3520
goal_identified
goal_identified
=== ep: 3521, time 26.51528239250183, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3521
goal_identified
goal_identified
goal_identified
=== ep: 3522, time 26.508190870285034, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3522
goal_identified
goal_identified
=== ep: 3523, time 26.323713779449463, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3523
=== ep: 3524, time 26.406534671783447, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3524
=== ep: 3525, time 26.223594665527344, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3525
=== ep: 3526, time 26.5780029296875, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3526
goal_identified
=== ep: 3527, time 26.38694715499878, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3527
=== ep: 3528, time 26.697518348693848, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3528
=== ep: 3529, time 30.718345165252686, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3529
goal_identified
goal_identified
goal_identified
=== ep: 3530, time 26.15470576286316, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3530
goal_identified
=== ep: 3531, time 26.160517692565918, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3531
goal_identified
goal_identified
=== ep: 3532, time 26.22996950149536, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3532
goal_identified
goal_identified
=== ep: 3533, time 26.180973052978516, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3533
goal_identified
=== ep: 3534, time 26.84041452407837, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3534
goal_identified
=== ep: 3535, time 26.402796506881714, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3535
goal_identified
goal_identified
goal_identified
=== ep: 3536, time 26.24684739112854, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3536
goal_identified
=== ep: 3537, time 26.37388801574707, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3537
goal_identified
goal_identified
=== ep: 3538, time 26.409261465072632, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3538
goal_identified
=== ep: 3539, time 31.243951320648193, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3539
goal_identified
goal_identified
=== ep: 3540, time 26.462278127670288, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3540
goal_identified
goal_identified
goal_identified
=== ep: 3541, time 26.37315559387207, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3541
goal_identified
goal_identified
=== ep: 3542, time 26.194222688674927, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3542
goal_identified
=== ep: 3543, time 26.008769273757935, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3543
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3544, time 26.55899715423584, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3544
goal_identified
=== ep: 3545, time 26.100350379943848, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3545
goal_identified
goal_identified
goal_identified
=== ep: 3546, time 26.102163076400757, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3546
goal_identified
=== ep: 3547, time 26.510960817337036, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3547
=== ep: 3548, time 26.40925145149231, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3548
goal_identified
goal_identified
=== ep: 3549, time 31.03929328918457, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3549
goal_identified
=== ep: 3550, time 26.52439308166504, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3550
=== ep: 3551, time 26.64711332321167, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3551
goal_identified
goal_identified
=== ep: 3552, time 26.457435369491577, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3552
goal_identified
goal_identified
=== ep: 3553, time 26.581655263900757, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3553
goal_identified
=== ep: 3554, time 26.513048887252808, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3554
goal_identified
=== ep: 3555, time 26.20665168762207, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3555
goal_identified
=== ep: 3556, time 26.311012744903564, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3556
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3557, time 26.231488704681396, eps 0.001, sum reward: 4, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3557
goal_identified
goal_identified
goal_identified
=== ep: 3558, time 26.07107949256897, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3558
=== ep: 3559, time 30.638730764389038, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3559
goal_identified
=== ep: 3560, time 26.52811312675476, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3560
=== ep: 3561, time 26.296070098876953, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3561
goal_identified
=== ep: 3562, time 26.440489292144775, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3562
=== ep: 3563, time 26.436335563659668, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 60/60)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3563
goal_identified
=== ep: 3564, time 26.339254140853882, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3564
goal_identified
goal_identified
=== ep: 3565, time 26.497450590133667, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3565
goal_identified
goal_identified
=== ep: 3566, time 26.255284309387207, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3566
goal_identified
goal_identified
goal_identified
=== ep: 3567, time 26.508862257003784, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3567
=== ep: 3568, time 26.249728202819824, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3568
goal_identified
goal_identified
=== ep: 3569, time 30.85999584197998, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3569
goal_identified
=== ep: 3570, time 26.039543628692627, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3570
goal_identified
goal_identified
=== ep: 3571, time 26.317917108535767, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3571
goal_identified
goal_identified
goal_identified
=== ep: 3572, time 26.232568502426147, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3572
goal_identified
=== ep: 3573, time 26.33855128288269, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3573
goal_identified
goal_identified
=== ep: 3574, time 26.092117071151733, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3574
goal_identified
goal_identified
goal_identified
=== ep: 3575, time 26.161649703979492, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3575
=== ep: 3576, time 26.429101705551147, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3576
goal_identified
=== ep: 3577, time 26.637953996658325, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3577
=== ep: 3578, time 26.57042121887207, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3578
goal_identified
=== ep: 3579, time 30.892199277877808, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3579
goal_identified
goal_identified
=== ep: 3580, time 26.40396785736084, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3580
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3581, time 26.23934841156006, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3581
=== ep: 3582, time 26.408286333084106, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3582
goal_identified
=== ep: 3583, time 26.841532468795776, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3583
goal_identified
goal_identified
goal_identified
=== ep: 3584, time 26.281720638275146, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3584
=== ep: 3585, time 26.616310596466064, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3585
goal_identified
=== ep: 3586, time 26.047791719436646, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3586
goal_identified
=== ep: 3587, time 25.93070387840271, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3587
goal_identified
goal_identified
goal_identified
=== ep: 3588, time 26.458752632141113, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3588
goal_identified
goal_identified
goal_identified
=== ep: 3589, time 31.100424766540527, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3589
goal_identified
=== ep: 3590, time 26.467007875442505, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3590
=== ep: 3591, time 26.497217416763306, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3591
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3592, time 26.727744579315186, eps 0.001, sum reward: 5, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3592
goal_identified
goal_identified
goal_identified
=== ep: 3593, time 26.248502254486084, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3593
=== ep: 3594, time 26.258535146713257, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3594
=== ep: 3595, time 26.594808101654053, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3595
goal_identified
=== ep: 3596, time 26.28302311897278, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3596
goal_identified
=== ep: 3597, time 26.235599994659424, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3597
goal_identified
goal_identified
goal_identified
=== ep: 3598, time 26.377532243728638, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3598
=== ep: 3599, time 30.76711106300354, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3599
=== ep: 3600, time 26.659039735794067, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3600
goal_identified
goal_identified
=== ep: 3601, time 26.428351163864136, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3601
goal_identified
=== ep: 3602, time 26.448084831237793, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3602
=== ep: 3603, time 26.52008557319641, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3603
goal_identified
=== ep: 3604, time 26.341650247573853, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3604
goal_identified
=== ep: 3605, time 26.603330612182617, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3605
goal_identified
goal_identified
=== ep: 3606, time 26.163806915283203, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3606
goal_identified
=== ep: 3607, time 26.34054160118103, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3607
goal_identified
goal_identified
goal_identified
=== ep: 3608, time 26.4458589553833, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3608
goal_identified
=== ep: 3609, time 30.797333240509033, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3609
=== ep: 3610, time 26.34586262702942, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3610
goal_identified
goal_identified
=== ep: 3611, time 26.400251150131226, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3611
goal_identified
=== ep: 3612, time 26.560230016708374, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3612
goal_identified
goal_identified
=== ep: 3613, time 26.481906175613403, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3613
goal_identified
goal_identified
=== ep: 3614, time 26.26654076576233, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3614
goal_identified
=== ep: 3615, time 26.54998517036438, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3615
=== ep: 3616, time 26.398710012435913, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3616
goal_identified
goal_identified
=== ep: 3617, time 26.321141481399536, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3617
goal_identified
=== ep: 3618, time 26.50491166114807, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3618
=== ep: 3619, time 30.742274522781372, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3619
goal_identified
=== ep: 3620, time 26.096840381622314, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3620
goal_identified
goal_identified
goal_identified
=== ep: 3621, time 26.109797716140747, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3621
goal_identified
goal_identified
goal_identified
=== ep: 3622, time 26.07206916809082, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3622
goal_identified
goal_identified
goal_identified
=== ep: 3623, time 26.279693603515625, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3623
goal_identified
goal_identified
goal_identified
=== ep: 3624, time 26.471519708633423, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3624
goal_identified
goal_identified
=== ep: 3625, time 25.94022011756897, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3625
=== ep: 3626, time 26.22865581512451, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3626
=== ep: 3627, time 26.280039072036743, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3627
=== ep: 3628, time 26.444018363952637, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3628
=== ep: 3629, time 31.372692584991455, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3629
=== ep: 3630, time 26.22517704963684, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3630
goal_identified
=== ep: 3631, time 26.67022442817688, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 118/118)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3631
goal_identified
goal_identified
=== ep: 3632, time 26.2883141040802, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3632
=== ep: 3633, time 26.398783206939697, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3633
goal_identified
goal_identified
=== ep: 3634, time 26.200264930725098, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3634
goal_identified
goal_identified
=== ep: 3635, time 26.27892541885376, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3635
goal_identified
goal_identified
=== ep: 3636, time 26.448144912719727, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3636
goal_identified
=== ep: 3637, time 26.133641958236694, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3637
goal_identified
=== ep: 3638, time 25.882129192352295, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3638
goal_identified
goal_identified
=== ep: 3639, time 31.505406379699707, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3639
=== ep: 3640, time 26.622793674468994, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3640
goal_identified
goal_identified
=== ep: 3641, time 26.593639612197876, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3641
goal_identified
goal_identified
=== ep: 3642, time 26.31775450706482, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3642
goal_identified
=== ep: 3643, time 26.47122550010681, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3643
=== ep: 3644, time 26.355947256088257, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3644
goal_identified
=== ep: 3645, time 26.149792432785034, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3645
goal_identified
=== ep: 3646, time 26.571785926818848, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3646
goal_identified
goal_identified
=== ep: 3647, time 26.453336477279663, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3647
goal_identified
=== ep: 3648, time 26.161279678344727, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3648
goal_identified
goal_identified
goal_identified
=== ep: 3649, time 31.238176345825195, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3649
goal_identified
goal_identified
=== ep: 3650, time 26.583575963974, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3650
goal_identified
=== ep: 3651, time 26.290093183517456, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3651
goal_identified
goal_identified
goal_identified
=== ep: 3652, time 26.2366464138031, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3652
goal_identified
goal_identified
goal_identified
=== ep: 3653, time 26.216286182403564, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3653
goal_identified
=== ep: 3654, time 26.40864372253418, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3654
=== ep: 3655, time 26.73530888557434, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3655
goal_identified
=== ep: 3656, time 26.24559736251831, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3656
=== ep: 3657, time 26.532476902008057, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3657
goal_identified
=== ep: 3658, time 26.134167194366455, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3658
=== ep: 3659, time 31.391345977783203, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3659
goal_identified
=== ep: 3660, time 26.411708116531372, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3660
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3661, time 26.17285966873169, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3661
goal_identified
=== ep: 3662, time 26.57986617088318, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3662
goal_identified
=== ep: 3663, time 25.83057951927185, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3663
goal_identified
goal_identified
goal_identified
=== ep: 3664, time 26.62817883491516, eps 0.001, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3664
goal_identified
=== ep: 3665, time 26.508673906326294, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3665
goal_identified
goal_identified
=== ep: 3666, time 26.33122158050537, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3666
=== ep: 3667, time 26.369523525238037, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3667
goal_identified
=== ep: 3668, time 26.339932441711426, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3668
goal_identified
goal_identified
goal_identified
=== ep: 3669, time 30.494415283203125, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3669
goal_identified
=== ep: 3670, time 26.47820281982422, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3670
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3671, time 26.475766897201538, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2373
goal_identified
=== ep: 3672, time 26.546494722366333, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3672
goal_identified
=== ep: 3673, time 26.447252988815308, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3673
goal_identified
=== ep: 3674, time 26.25255012512207, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3674
goal_identified
=== ep: 3675, time 26.32022714614868, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3675
goal_identified
=== ep: 3676, time 26.45425319671631, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3676
goal_identified
=== ep: 3677, time 26.66104793548584, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3677
goal_identified
goal_identified
=== ep: 3678, time 26.327291011810303, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3678
=== ep: 3679, time 31.919578552246094, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3679
goal_identified
goal_identified
goal_identified
=== ep: 3680, time 26.70994210243225, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3680
=== ep: 3681, time 26.557475328445435, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3681
=== ep: 3682, time 26.5771267414093, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3682
goal_identified
goal_identified
=== ep: 3683, time 26.08311104774475, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3683
goal_identified
goal_identified
=== ep: 3684, time 26.663025617599487, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3684
goal_identified
goal_identified
=== ep: 3685, time 25.899299144744873, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3685
=== ep: 3686, time 26.722996950149536, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3686
goal_identified
=== ep: 3687, time 26.758107900619507, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3687
goal_identified
=== ep: 3688, time 26.206207752227783, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3688
=== ep: 3689, time 31.43763041496277, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3689
goal_identified
goal_identified
=== ep: 3690, time 26.512250661849976, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3690
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3691, time 26.49861979484558, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3691
=== ep: 3692, time 26.180951356887817, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3692
goal_identified
=== ep: 3693, time 26.140594959259033, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3693
goal_identified
goal_identified
goal_identified
=== ep: 3694, time 26.155287265777588, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3694
goal_identified
=== ep: 3695, time 26.152708053588867, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3695
goal_identified
goal_identified
=== ep: 3696, time 26.22111940383911, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3696
goal_identified
goal_identified
goal_identified
=== ep: 3697, time 26.094806909561157, eps 0.001, sum reward: 3, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3697
goal_identified
goal_identified
=== ep: 3698, time 26.431634187698364, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3698
goal_identified
=== ep: 3699, time 31.226203203201294, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3699
goal_identified
=== ep: 3700, time 26.367039442062378, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3700
goal_identified
goal_identified
=== ep: 3701, time 26.46285128593445, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3701
goal_identified
=== ep: 3702, time 26.165417432785034, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3702
goal_identified
=== ep: 3703, time 26.219653367996216, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3703
goal_identified
goal_identified
=== ep: 3704, time 26.353957891464233, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3704
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3705, time 25.998295783996582, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3705
goal_identified
goal_identified
goal_identified
=== ep: 3706, time 26.30138373374939, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3706
goal_identified
goal_identified
=== ep: 3707, time 26.60832142829895, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3707
goal_identified
goal_identified
goal_identified
=== ep: 3708, time 26.649457216262817, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3708
goal_identified
=== ep: 3709, time 30.995091199874878, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3709
goal_identified
goal_identified
=== ep: 3710, time 26.32990264892578, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3710
=== ep: 3711, time 26.08517551422119, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3711
=== ep: 3712, time 26.366297245025635, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3712
goal_identified
=== ep: 3713, time 25.979707956314087, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3713
goal_identified
goal_identified
=== ep: 3714, time 26.105474710464478, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3714
goal_identified
=== ep: 3715, time 26.00554871559143, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3715
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3716, time 26.23259663581848, eps 0.001, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2393
goal_identified
=== ep: 3717, time 25.9886417388916, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3717
goal_identified
goal_identified
goal_identified
=== ep: 3718, time 26.35336923599243, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3718
=== ep: 3719, time 36.945380210876465, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3719
goal_identified
goal_identified
=== ep: 3720, time 26.262176036834717, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3720
goal_identified
goal_identified
=== ep: 3721, time 26.627368927001953, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3721
=== ep: 3722, time 26.322921752929688, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3722
goal_identified
=== ep: 3723, time 26.242977380752563, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3723
=== ep: 3724, time 26.497552394866943, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3724
goal_identified
goal_identified
=== ep: 3725, time 26.767700910568237, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3725
goal_identified
=== ep: 3726, time 26.55055069923401, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 116/116)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3726
=== ep: 3727, time 26.526370763778687, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3727
goal_identified
goal_identified
goal_identified
=== ep: 3728, time 26.039705514907837, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3728
goal_identified
goal_identified
=== ep: 3729, time 31.116997480392456, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3729
=== ep: 3730, time 26.386565685272217, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3730
goal_identified
goal_identified
=== ep: 3731, time 25.908372163772583, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3731
goal_identified
=== ep: 3732, time 26.254987716674805, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 112/112)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3732
=== ep: 3733, time 26.457086086273193, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 62/62)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3733
goal_identified
goal_identified
=== ep: 3734, time 26.220917224884033, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3734
goal_identified
goal_identified
=== ep: 3735, time 26.0420982837677, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3735
goal_identified
=== ep: 3736, time 26.206963062286377, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3736
goal_identified
goal_identified
=== ep: 3737, time 25.973211526870728, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3737
goal_identified
=== ep: 3738, time 26.499886751174927, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3738
goal_identified
goal_identified
goal_identified
=== ep: 3739, time 31.191930055618286, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3739
goal_identified
=== ep: 3740, time 26.596171855926514, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3740
goal_identified
=== ep: 3741, time 26.39198660850525, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3741
goal_identified
goal_identified
goal_identified
=== ep: 3742, time 26.011383295059204, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3742
goal_identified
goal_identified
goal_identified
=== ep: 3743, time 26.343918085098267, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3743
goal_identified
=== ep: 3744, time 26.319692373275757, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3744
=== ep: 3745, time 26.404795169830322, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3745
goal_identified
=== ep: 3746, time 26.364936351776123, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3746
goal_identified
goal_identified
=== ep: 3747, time 26.108702659606934, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3747
goal_identified
=== ep: 3748, time 26.303656101226807, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 122/122)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3748
goal_identified
goal_identified
goal_identified
=== ep: 3749, time 31.459596872329712, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3749
goal_identified
goal_identified
=== ep: 3750, time 26.193448066711426, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3750
goal_identified
=== ep: 3751, time 26.339263200759888, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3751
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3752, time 26.388113498687744, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3752
=== ep: 3753, time 26.276628971099854, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3753
=== ep: 3754, time 26.728533267974854, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3754
goal_identified
=== ep: 3755, time 26.33138656616211, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3755
goal_identified
=== ep: 3756, time 26.311338424682617, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3756
goal_identified
goal_identified
goal_identified
=== ep: 3757, time 26.27001690864563, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3757
=== ep: 3758, time 26.153904676437378, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3758
goal_identified
=== ep: 3759, time 31.5958833694458, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3759
goal_identified
goal_identified
=== ep: 3760, time 26.386473894119263, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3760
=== ep: 3761, time 26.178088188171387, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3761
=== ep: 3762, time 26.29871678352356, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3762
goal_identified
=== ep: 3763, time 26.533239603042603, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3763
=== ep: 3764, time 26.266824960708618, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 78/78)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3764
=== ep: 3765, time 26.23471236228943, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3765
=== ep: 3766, time 26.306987047195435, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3766
goal_identified
goal_identified
goal_identified
=== ep: 3767, time 26.173553705215454, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3767
goal_identified
goal_identified
=== ep: 3768, time 26.310298442840576, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3768
goal_identified
goal_identified
=== ep: 3769, time 31.166865348815918, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3769
goal_identified
=== ep: 3770, time 26.841206312179565, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3770
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3771, time 26.30715537071228, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3771
=== ep: 3772, time 26.24377942085266, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3772
goal_identified
=== ep: 3773, time 26.956294059753418, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 119/119)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3773
goal_identified
=== ep: 3774, time 26.32054829597473, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3774
goal_identified
=== ep: 3775, time 26.315677642822266, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3775
goal_identified
=== ep: 3776, time 26.49147057533264, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3776
=== ep: 3777, time 26.621949672698975, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3777
goal_identified
=== ep: 3778, time 26.33596181869507, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3778
goal_identified
=== ep: 3779, time 31.378578901290894, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3779
goal_identified
=== ep: 3780, time 26.07857632637024, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3780
=== ep: 3781, time 26.362932682037354, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3781
=== ep: 3782, time 26.2922306060791, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3782
=== ep: 3783, time 26.52874231338501, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3783
=== ep: 3784, time 26.590656042099, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3784
=== ep: 3785, time 26.439027309417725, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3785
goal_identified
=== ep: 3786, time 26.173972368240356, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3786
goal_identified
=== ep: 3787, time 26.341840744018555, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3787
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3788, time 26.263415575027466, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3788
goal_identified
goal_identified
=== ep: 3789, time 31.434696674346924, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3789
goal_identified
=== ep: 3790, time 26.595815658569336, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3790
goal_identified
goal_identified
goal_identified
=== ep: 3791, time 26.87414789199829, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3791
goal_identified
=== ep: 3792, time 26.512627601623535, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3792
goal_identified
=== ep: 3793, time 26.199376344680786, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3793
goal_identified
goal_identified
=== ep: 3794, time 26.310446739196777, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3794
goal_identified
=== ep: 3795, time 26.236231803894043, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3795
goal_identified
=== ep: 3796, time 26.780550479888916, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3796
goal_identified
goal_identified
=== ep: 3797, time 26.082088232040405, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3797
=== ep: 3798, time 26.600106477737427, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3798
goal_identified
=== ep: 3799, time 31.429511070251465, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3799
=== ep: 3800, time 26.26060652732849, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3800
goal_identified
=== ep: 3801, time 26.49949073791504, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3801
goal_identified
=== ep: 3802, time 26.033666610717773, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3802
goal_identified
=== ep: 3803, time 26.197795391082764, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3803
goal_identified
goal_identified
=== ep: 3804, time 26.65703535079956, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3804
goal_identified
=== ep: 3805, time 26.43671154975891, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3805
goal_identified
=== ep: 3806, time 26.30721950531006, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3806
goal_identified
goal_identified
goal_identified
=== ep: 3807, time 26.369144439697266, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3807
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3808, time 26.53651714324951, eps 0.001, sum reward: 5, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3808
=== ep: 3809, time 30.979209184646606, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3809
goal_identified
goal_identified
goal_identified
=== ep: 3810, time 26.320266008377075, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3810
goal_identified
goal_identified
=== ep: 3811, time 26.39630651473999, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3811
goal_identified
=== ep: 3812, time 26.568334102630615, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3812
goal_identified
=== ep: 3813, time 26.221043348312378, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3813
=== ep: 3814, time 26.721386194229126, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3814
goal_identified
=== ep: 3815, time 26.609918355941772, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3815
=== ep: 3816, time 26.4745934009552, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3816
goal_identified
=== ep: 3817, time 26.37318778038025, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3817
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3818, time 26.295058727264404, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3818
goal_identified
=== ep: 3819, time 31.65226936340332, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3819
goal_identified
=== ep: 3820, time 26.69954252243042, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3820
goal_identified
goal_identified
goal_identified
=== ep: 3821, time 26.5404052734375, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3821
goal_identified
goal_identified
goal_identified
=== ep: 3822, time 26.797590970993042, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3822
goal_identified
=== ep: 3823, time 26.138136386871338, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3823
goal_identified
goal_identified
=== ep: 3824, time 25.984851121902466, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 63/63)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3824
goal_identified
goal_identified
=== ep: 3825, time 26.29607915878296, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3825
goal_identified
=== ep: 3826, time 26.23636245727539, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 88/88)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3826
goal_identified
goal_identified
=== ep: 3827, time 26.342983961105347, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3827
goal_identified
=== ep: 3828, time 26.331565141677856, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3828
=== ep: 3829, time 31.501846313476562, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3829
goal_identified
=== ep: 3830, time 26.145690441131592, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3830
goal_identified
=== ep: 3831, time 26.311983823776245, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3831
goal_identified
goal_identified
=== ep: 3832, time 26.173166036605835, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3832
goal_identified
=== ep: 3833, time 26.743809461593628, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3833
goal_identified
=== ep: 3834, time 26.66981053352356, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3834
goal_identified
=== ep: 3835, time 26.46386456489563, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3835
goal_identified
=== ep: 3836, time 26.224034786224365, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3836
goal_identified
=== ep: 3837, time 26.707019329071045, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3837
goal_identified
=== ep: 3838, time 26.38984727859497, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3838
goal_identified
goal_identified
=== ep: 3839, time 31.238163232803345, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3839
goal_identified
goal_identified
goal_identified
=== ep: 3840, time 25.94631004333496, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3840
goal_identified
goal_identified
=== ep: 3841, time 26.501643896102905, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3841
goal_identified
=== ep: 3842, time 26.505522966384888, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3842
goal_identified
goal_identified
=== ep: 3843, time 26.20156955718994, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3843
goal_identified
goal_identified
=== ep: 3844, time 26.535603761672974, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3844
goal_identified
goal_identified
goal_identified
=== ep: 3845, time 26.551105499267578, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3845
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3846, time 26.519951581954956, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3846
=== ep: 3847, time 26.517369270324707, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3847
=== ep: 3848, time 26.64091420173645, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3848
=== ep: 3849, time 31.227797031402588, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3849
=== ep: 3850, time 26.37975311279297, eps 0.001, sum reward: 0, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3850
goal_identified
=== ep: 3851, time 26.54606008529663, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3851
goal_identified
=== ep: 3852, time 26.207037925720215, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3852
goal_identified
=== ep: 3853, time 26.544699668884277, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3853
goal_identified
=== ep: 3854, time 26.626882314682007, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3854
goal_identified
goal_identified
=== ep: 3855, time 26.213397979736328, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3855
=== ep: 3856, time 26.037124156951904, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3856
goal_identified
=== ep: 3857, time 26.467800617218018, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3857
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3858, time 26.58969807624817, eps 0.001, sum reward: 4, score_diff 5, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2708
goal_identified
=== ep: 3859, time 31.208104372024536, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3859
=== ep: 3860, time 26.670446395874023, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3860
goal_identified
goal_identified
=== ep: 3861, time 26.080077648162842, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3861
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3862, time 26.342954635620117, eps 0.001, sum reward: 5, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3862
goal_identified
goal_identified
goal_identified
=== ep: 3863, time 26.44611644744873, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3863
goal_identified
goal_identified
=== ep: 3864, time 26.675760507583618, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3864
goal_identified
=== ep: 3865, time 26.228655099868774, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3865
=== ep: 3866, time 26.95834994316101, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3866
goal_identified
=== ep: 3867, time 26.59708023071289, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3867
=== ep: 3868, time 26.142975091934204, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3868
goal_identified
=== ep: 3869, time 31.12335181236267, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3869
goal_identified
=== ep: 3870, time 26.148589611053467, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3870
goal_identified
goal_identified
goal_identified
=== ep: 3871, time 26.74419331550598, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3871
goal_identified
goal_identified
=== ep: 3872, time 26.221322774887085, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3872
=== ep: 3873, time 26.589979887008667, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3873
goal_identified
=== ep: 3874, time 26.61092233657837, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3874
goal_identified
goal_identified
=== ep: 3875, time 26.05900263786316, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3875
goal_identified
goal_identified
=== ep: 3876, time 26.54921531677246, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3876
=== ep: 3877, time 26.591951847076416, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3877
goal_identified
goal_identified
=== ep: 3878, time 26.252037048339844, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3878
goal_identified
=== ep: 3879, time 31.057335376739502, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3879
=== ep: 3880, time 26.20552682876587, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3880
goal_identified
goal_identified
=== ep: 3881, time 26.01052188873291, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 64/64)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3881
goal_identified
goal_identified
=== ep: 3882, time 26.121280908584595, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3882
goal_identified
=== ep: 3883, time 26.223517656326294, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3883
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3884, time 26.6004159450531, eps 0.001, sum reward: 4, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 106/106)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3884
goal_identified
goal_identified
=== ep: 3885, time 26.439817667007446, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3885
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3886, time 26.72475290298462, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3886
goal_identified
=== ep: 3887, time 26.19419240951538, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 98/98)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3887
goal_identified
=== ep: 3888, time 26.464301824569702, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3888
=== ep: 3889, time 31.407757997512817, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3889
goal_identified
goal_identified
goal_identified
=== ep: 3890, time 26.658920526504517, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3890
goal_identified
goal_identified
goal_identified
=== ep: 3891, time 26.154221296310425, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 95/95)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3891
goal_identified
goal_identified
goal_identified
=== ep: 3892, time 26.282703399658203, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 115/115)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3892
=== ep: 3893, time 26.521831274032593, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3893
=== ep: 3894, time 26.471840381622314, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3894
goal_identified
goal_identified
=== ep: 3895, time 26.678202152252197, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3895
goal_identified
goal_identified
=== ep: 3896, time 26.46314263343811, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 101/101)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3896
goal_identified
goal_identified
goal_identified
=== ep: 3897, time 26.266125440597534, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3897
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3898, time 26.224513053894043, eps 0.001, sum reward: 4, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3898
goal_identified
=== ep: 3899, time 31.12098264694214, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3899
goal_identified
goal_identified
=== ep: 3900, time 26.190821170806885, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3900
goal_identified
goal_identified
=== ep: 3901, time 26.227476835250854, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 61/61)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3901
goal_identified
=== ep: 3902, time 26.669212102890015, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 84/84)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3902
=== ep: 3903, time 26.350665807724, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 77/77)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3903
=== ep: 3904, time 26.25000023841858, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3904
goal_identified
goal_identified
=== ep: 3905, time 26.389482021331787, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3905
=== ep: 3906, time 26.40330958366394, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3906
goal_identified
goal_identified
goal_identified
=== ep: 3907, time 26.313428163528442, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3907
goal_identified
goal_identified
goal_identified
=== ep: 3908, time 26.20442843437195, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 67/67)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3908
goal_identified
goal_identified
=== ep: 3909, time 31.055795431137085, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3909
goal_identified
goal_identified
=== ep: 3910, time 26.36610221862793, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 100/100)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3910
goal_identified
=== ep: 3911, time 26.373049020767212, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3911
=== ep: 3912, time 26.316221952438354, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3912
goal_identified
goal_identified
=== ep: 3913, time 26.612727880477905, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3913
goal_identified
=== ep: 3914, time 26.30531597137451, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3914
goal_identified
goal_identified
goal_identified
=== ep: 3915, time 26.38675093650818, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 90/90)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3915
=== ep: 3916, time 26.339900732040405, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3916
goal_identified
goal_identified
goal_identified
=== ep: 3917, time 26.648693799972534, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3917
goal_identified
=== ep: 3918, time 26.58980679512024, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 107/107)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3918
goal_identified
=== ep: 3919, time 31.21077823638916, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3919
goal_identified
goal_identified
=== ep: 3920, time 26.414190530776978, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3920
goal_identified
goal_identified
=== ep: 3921, time 26.545361518859863, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3921
=== ep: 3922, time 26.022911548614502, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3922
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3923, time 26.561119318008423, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 111/111)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3923
=== ep: 3924, time 26.26499652862549, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3924
goal_identified
=== ep: 3925, time 26.360167264938354, eps 0.001, sum reward: 1, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3925
goal_identified
goal_identified
=== ep: 3926, time 26.39987063407898, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3926
goal_identified
goal_identified
=== ep: 3927, time 26.171398162841797, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 76/76)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3927
goal_identified
=== ep: 3928, time 26.627729415893555, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3928
=== ep: 3929, time 31.174951553344727, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3929
goal_identified
goal_identified
goal_identified
=== ep: 3930, time 26.758671760559082, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3930
=== ep: 3931, time 26.27428102493286, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3931
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3932, time 26.043365001678467, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3932
goal_identified
=== ep: 3933, time 26.52883291244507, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3933
goal_identified
=== ep: 3934, time 26.11077570915222, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3934
goal_identified
=== ep: 3935, time 26.195613861083984, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3935
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3936, time 26.25344204902649, eps 0.001, sum reward: 4, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3936
=== ep: 3937, time 26.555777311325073, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 94/94)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3937
=== ep: 3938, time 26.125309944152832, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3938
=== ep: 3939, time 31.313621520996094, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3939
goal_identified
=== ep: 3940, time 26.074392080307007, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3940
goal_identified
goal_identified
goal_identified
=== ep: 3941, time 26.85184645652771, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3941
goal_identified
=== ep: 3942, time 26.334689378738403, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3942
=== ep: 3943, time 26.611695051193237, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3943
=== ep: 3944, time 26.44636607170105, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 92/92)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3944
goal_identified
=== ep: 3945, time 26.458511114120483, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3945
goal_identified
=== ep: 3946, time 26.426862955093384, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3946
=== ep: 3947, time 26.2038676738739, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3947
goal_identified
=== ep: 3948, time 26.069426774978638, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3948
goal_identified
=== ep: 3949, time 30.860723972320557, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 57/57)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3949
goal_identified
goal_identified
goal_identified
=== ep: 3950, time 26.01767921447754, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3950
goal_identified
goal_identified
goal_identified
=== ep: 3951, time 26.325099229812622, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3951
goal_identified
goal_identified
=== ep: 3952, time 26.200348377227783, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3952
goal_identified
goal_identified
goal_identified
=== ep: 3953, time 26.160232067108154, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3953
goal_identified
goal_identified
=== ep: 3954, time 26.573910236358643, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3954
goal_identified
goal_identified
=== ep: 3955, time 26.587192058563232, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 109/109)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3955
goal_identified
=== ep: 3956, time 26.074100494384766, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 68/68)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3956
goal_identified
=== ep: 3957, time 26.37769365310669, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 103/103)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3957
goal_identified
=== ep: 3958, time 26.053273677825928, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3958
goal_identified
goal_identified
goal_identified
=== ep: 3959, time 31.3913152217865, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3959
goal_identified
goal_identified
goal_identified
=== ep: 3960, time 26.373251914978027, eps 0.001, sum reward: 3, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3960
=== ep: 3961, time 26.46616792678833, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3961
goal_identified
=== ep: 3962, time 26.224626064300537, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3962
=== ep: 3963, time 26.3610782623291, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3963
goal_identified
=== ep: 3964, time 26.33824062347412, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 97/97)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3964
goal_identified
goal_identified
=== ep: 3965, time 26.360606908798218, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 105/105)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3965
=== ep: 3966, time 26.54155421257019, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3966
goal_identified
goal_identified
=== ep: 3967, time 26.442262649536133, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3967
goal_identified
goal_identified
=== ep: 3968, time 26.29463529586792, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3968
=== ep: 3969, time 31.06965923309326, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001) (win_game called correctly 74/74)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3969
=== ep: 3970, time 26.49226474761963, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 108/108)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3970
=== ep: 3971, time 26.287509441375732, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 73/73)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3971
goal_identified
=== ep: 3972, time 26.256561279296875, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3972
goal_identified
=== ep: 3973, time 26.433789491653442, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 70/70)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3973
goal_identified
=== ep: 3974, time 26.389580965042114, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 91/91)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3974
=== ep: 3975, time 26.510262966156006, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3975
goal_identified
goal_identified
=== ep: 3976, time 26.335013151168823, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 65/65)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3976
=== ep: 3977, time 26.187691926956177, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 59/59)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3977
goal_identified
=== ep: 3978, time 26.44268488883972, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 87/87)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3978
goal_identified
goal_identified
=== ep: 3979, time 31.076796054840088, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001) (win_game called correctly 80/80)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3979
=== ep: 3980, time 26.37765383720398, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 83/83)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3980
goal_identified
goal_identified
=== ep: 3981, time 27.794323682785034, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 93/93)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3981
=== ep: 3982, time 26.45670175552368, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 79/79)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3982
goal_identified
=== ep: 3983, time 26.512688636779785, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 96/96)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3983
goal_identified
goal_identified
=== ep: 3984, time 26.255343675613403, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 86/86)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3984
goal_identified
=== ep: 3985, time 26.4651939868927, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 82/82)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3985
=== ep: 3986, time 26.475686073303223, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 102/102)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3986
goal_identified
goal_identified
=== ep: 3987, time 25.775385856628418, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 66/66)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3987
=== ep: 3988, time 26.219728469848633, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3988
=== ep: 3989, time 31.063538789749146, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001) (win_game called correctly 72/72)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3989
=== ep: 3990, time 26.603965997695923, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001) (win_game called correctly 99/99)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3990
goal_identified
goal_identified
=== ep: 3991, time 26.345472812652588, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 89/89)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3991
goal_identified
=== ep: 3992, time 26.85671329498291, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 81/81)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3992
goal_identified
goal_identified
=== ep: 3993, time 26.416229963302612, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3993
goal_identified
=== ep: 3994, time 26.259451150894165, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 85/85)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3994
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 3995, time 26.31672739982605, eps 0.001, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001) (win_game called correctly 75/75)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3995
=== ep: 3996, time 26.42043709754944, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001) (win_game called correctly 69/69)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3996
goal_identified
goal_identified
=== ep: 3997, time 26.156028509140015, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001) (win_game called correctly 71/71)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3997
goal_identified
=== ep: 3998, time 26.108968019485474, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001) (win_game called correctly 51/51)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3998
goal_identified
=== ep: 3999, time 31.308300971984863, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001) (win_game called correctly 81/81)
