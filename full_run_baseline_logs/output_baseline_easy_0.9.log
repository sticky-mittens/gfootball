==> Playing in 11_vs_11_easy_stochastic.
==>Level 1
==>OTs in this level are dict_keys(['win_game'])
==>Currently learning win_game
==>using device cuda
==>critic has 5 layers and 500 hidden units.
goal_identified
=== ep: 0, time 26.27503228187561, eps 0.9, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 1, time 27.301205158233643, eps 0.8561552526261419, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 2, time 27.467154264450073, eps 0.8144488388143276, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 3, time 28.92302703857422, eps 0.774776470806127, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 4, time 29.82137131690979, eps 0.7370389470171057, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 5, time 30.116133451461792, eps 0.701141903981193, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 6, time 31.532229900360107, eps 0.6669955803928644, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 7, time 32.088797092437744, eps 0.6345145926571234, sum reward: 5, score_diff 5, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 8, time 35.97423052787781, eps 0.6036177213860398, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 9, time 32.230061054229736, eps 0.5742277083079742, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 10, time 33.31004524230957, eps 0.5462710630816575, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 11, time 34.48829221725464, eps 0.5196778795320575, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 12, time 35.21343493461609, eps 0.49438166084852986, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 13, time 36.74118995666504, eps 0.47031915330815344, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 14, time 37.422521114349365, eps 0.4474301881084772, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
=== ep: 15, time 39.22645449638367, eps 0.42565753091417224, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
=== ep: 16, time 41.385465145111084, eps 0.4049467387413822, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 17, time 40.50688600540161, eps 0.3852460238219053, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
=== ep: 18, time 41.91777014732361, eps 0.3665061241067986, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 19, time 42.69106984138489, eps 0.3486801800855966, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 1
goal_identified
=== ep: 20, time 43.377336263656616, eps 0.3317236176131267, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
/home/ksridhar/GRF/scripts/policies_baseline.py:437: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 21 > 20 and we are deleting ep 15
goal_identified
=== ep: 21, time 43.44004440307617, eps 0.31559403645092865, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 16
goal_identified
=== ep: 22, time 52.84888935089111, eps 0.3002511042445735, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 18
=== ep: 23, time 42.974223613739014, eps 0.2856564556717689, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 20
goal_identified
goal_identified
=== ep: 24, time 42.4466917514801, eps 0.27177359650906974, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 23
goal_identified
goal_identified
=== ep: 25, time 43.957722425460815, eps 0.2585678123773109, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 0
=== ep: 26, time 45.33407974243164, eps 0.24600608193757734, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 26
goal_identified
=== ep: 27, time 45.785266637802124, eps 0.23405699432065646, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 4
goal_identified
goal_identified
=== ep: 28, time 48.133829832077026, eps 0.22269067058350425, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 5
goal_identified
goal_identified
=== ep: 29, time 51.0347740650177, eps 0.2118786889963241, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 8
goal_identified
=== ep: 30, time 47.93760848045349, eps 0.2015940139734384, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 9
goal_identified
goal_identified
=== ep: 31, time 49.492804527282715, eps 0.191810928470242, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 11
goal_identified
=== ep: 32, time 49.487713098526, eps 0.1825049696771952, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 19
goal_identified
=== ep: 33, time 52.269272804260254, eps 0.17365286785005798, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 21
goal_identified
=== ep: 34, time 49.769381284713745, eps 0.16523248812340846, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 22
goal_identified
goal_identified
=== ep: 35, time 53.01833724975586, eps 0.15722277516195018, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 35
goal_identified
goal_identified
=== ep: 36, time 50.175719261169434, eps 0.1496037005112063, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 27
goal_identified
=== ep: 37, time 49.688239097595215, eps 0.14235621251595124, sum reward: 1, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 37
goal_identified
=== ep: 38, time 49.45262432098389, eps 0.13546218868114893, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 30
goal_identified
goal_identified
goal_identified
=== ep: 39, time 56.69716739654541, eps 0.1289043903562757, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 31
goal_identified
goal_identified
goal_identified
=== ep: 40, time 55.85948157310486, eps 0.12266641962971482, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 32
=== ep: 41, time 50.87909936904907, eps 0.116732678325436, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 41
goal_identified
goal_identified
goal_identified
=== ep: 42, time 52.53900384902954, eps 0.11108832899943073, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 33
goal_identified
goal_identified
goal_identified
=== ep: 43, time 54.103423833847046, eps 0.10571925783837377, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 34
goal_identified
=== ep: 44, time 61.93208456039429, eps 0.10061203936773815, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 38
=== ep: 45, time 55.28849911689758, eps 0.09575390288111604, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 45
goal_identified
=== ep: 46, time 68.14844298362732, eps 0.09113270050680057, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 44
=== ep: 47, time 56.67783188819885, eps 0.08673687683177911, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 47
=== ep: 48, time 56.24316668510437, eps 0.08255544000718185, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 48
=== ep: 49, time 60.43398189544678, eps 0.07857793426293408, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 49
goal_identified
goal_identified
=== ep: 50, time 58.100866079330444, eps 0.07479441376288502, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 46
goal_identified
=== ep: 51, time 71.72968077659607, eps 0.0711954177350367, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 50
goal_identified
goal_identified
goal_identified
=== ep: 52, time 56.78372621536255, eps 0.06777194681468615, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 51
goal_identified
=== ep: 53, time 56.38820767402649, eps 0.06451544054132621, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 53
goal_identified
=== ep: 54, time 66.53229665756226, eps 0.06141775595303503, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 54
=== ep: 55, time 57.847564935684204, eps 0.05847114722483011, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 55
goal_identified
=== ep: 56, time 64.83361220359802, eps 0.05566824630007096, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 56
=== ep: 57, time 56.72250723838806, eps 0.05300204446647978, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 57
goal_identified
=== ep: 58, time 56.491360902786255, eps 0.050465874830710106, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 58
goal_identified
goal_identified
=== ep: 59, time 66.82812190055847, eps 0.04805339564764071, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 59
=== ep: 60, time 59.54010009765625, eps 0.045758574462709686, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 60
goal_identified
goal_identified
goal_identified
=== ep: 61, time 59.04774713516235, eps 0.043575673027635695, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 3
=== ep: 62, time 69.15969800949097, eps 0.04149923295180846, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 62
goal_identified
goal_identified
goal_identified
=== ep: 63, time 61.60359811782837, eps 0.03952406205346913, sum reward: 3, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 63
goal_identified
=== ep: 64, time 73.3271369934082, eps 0.03764522137655123, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 64
=== ep: 65, time 61.83375859260559, eps 0.03585801284071809, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 65
goal_identified
=== ep: 66, time 69.44196963310242, eps 0.034157967493714775, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 66
goal_identified
=== ep: 67, time 66.30148100852966, eps 0.03254083433665968, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 67
=== ep: 68, time 64.79158163070679, eps 0.031002569694333147, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 68
=== ep: 69, time 63.10607361793518, eps 0.02953932710388308, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 69
goal_identified
=== ep: 70, time 63.33600115776062, eps 0.028147447696664333, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 70
goal_identified
=== ep: 71, time 63.87267088890076, eps 0.026823451049161253, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 71
goal_identified
=== ep: 72, time 72.79357504844666, eps 0.025564026480116013, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 72
=== ep: 73, time 68.02199506759644, eps 0.02436602477210106, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 73
=== ep: 74, time 66.69338703155518, eps 0.02322645029683511, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 74
goal_identified
=== ep: 75, time 62.20103478431702, eps 0.02214245352455219, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 75
=== ep: 76, time 70.20747065544128, eps 0.02111132389869288, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 76
=== ep: 77, time 70.38356685638428, eps 0.020130483058101077, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 77
=== ep: 78, time 69.06992053985596, eps 0.019197478389778148, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 78
=== ep: 79, time 64.70227432250977, eps 0.018309976896072843, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 79
goal_identified
goal_identified
=== ep: 80, time 65.57602143287659, eps 0.017465759360972027, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 12
goal_identified
goal_identified
goal_identified
=== ep: 81, time 83.34541773796082, eps 0.01666271480090467, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 13
=== ep: 82, time 89.82421731948853, eps 0.015898835186183367, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 82
goal_identified
=== ep: 83, time 77.90190863609314, eps 0.015172210419884185, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 83
goal_identified
=== ep: 84, time 76.35141921043396, eps 0.014481023561609456, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 84
=== ep: 85, time 91.17733359336853, eps 0.01382354628419033, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 85
goal_identified
goal_identified
=== ep: 86, time 88.17372488975525, eps 0.013198134551968641, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 86
=== ep: 87, time 78.22292995452881, eps 0.012603224509851407, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 87
goal_identified
=== ep: 88, time 78.23958921432495, eps 0.012037328572858524, sum reward: 1, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 88
goal_identified
=== ep: 89, time 86.63225793838501, eps 0.011499031706385502, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 89
goal_identified
goal_identified
=== ep: 90, time 91.91641855239868, eps 0.010986987887879832, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 17
goal_identified
=== ep: 91, time 84.26327109336853, eps 0.010499916741083536, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 91
=== ep: 92, time 82.77590250968933, eps 0.010036600334425595, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 92
goal_identified
=== ep: 93, time 102.00846219062805, eps 0.00959588013555861, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 93
goal_identified
=== ep: 94, time 80.69906258583069, eps 0.009176654114424539, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 94
goal_identified
goal_identified
goal_identified
=== ep: 95, time 81.74119400978088, eps 0.00877787398760545, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 24
goal_identified
=== ep: 96, time 86.69496989250183, eps 0.008398542597069007, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 96
=== ep: 97, time 120.71648049354553, eps 0.008037711416753971, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 97
=== ep: 98, time 87.70009112358093, eps 0.00769447818076098, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 98
goal_identified
=== ep: 99, time 87.76817607879639, eps 0.007367984627217855, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 99
=== ep: 100, time 95.39172410964966, eps 0.007057414352177835, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 100
goal_identified
goal_identified
=== ep: 101, time 99.51924920082092, eps 0.006761990768184489, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 25
goal_identified
=== ep: 102, time 92.84939312934875, eps 0.006480975162398559, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 102
goal_identified
=== ep: 103, time 92.34080624580383, eps 0.006213664849431085, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 103
goal_identified
=== ep: 104, time 124.96423625946045, eps 0.005959391414263934, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 104
goal_identified
=== ep: 105, time 91.06297636032104, eps 0.005717519040864065, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 105
=== ep: 106, time 89.88384294509888, eps 0.005487442922312285, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 106
=== ep: 107, time 113.6220326423645, eps 0.005268587748470919, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 107
goal_identified
=== ep: 108, time 93.16764521598816, eps 0.005060406267408787, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 108
goal_identified
=== ep: 109, time 90.16129469871521, eps 0.004862377916986354, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 109
=== ep: 110, time 105.19491124153137, eps 0.004674007523179196, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 110
goal_identified
=== ep: 111, time 102.49881887435913, eps 0.004494824061885041, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 111
goal_identified
=== ep: 112, time 88.25643420219421, eps 0.0043243794811181555, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 112
goal_identified
=== ep: 113, time 102.05223035812378, eps 0.0041622475806460035, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 113
=== ep: 114, time 116.19056963920593, eps 0.0040080229462666735, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 114
=== ep: 115, time 84.92498278617859, eps 0.0038613199360621906, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 115
=== ep: 116, time 88.37226486206055, eps 0.003721771716092858, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 116
=== ep: 117, time 102.82622337341309, eps 0.0035890293431213305, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 117
=== ep: 118, time 97.87607169151306, eps 0.0034627608920727634, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 118
goal_identified
=== ep: 119, time 92.28669333457947, eps 0.00334265062604924, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 119
goal_identified
=== ep: 120, time 99.75149822235107, eps 0.0032283982068230565, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 120
goal_identified
goal_identified
=== ep: 121, time 111.23396492004395, eps 0.0031197179438347193, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 28
=== ep: 122, time 98.28244733810425, eps 0.0030163380798177374, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 122
=== ep: 123, time 109.7853753566742, eps 0.0029180001112638996, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 123
=== ep: 124, time 118.26240706443787, eps 0.002824458142029865, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 124
goal_identified
goal_identified
=== ep: 125, time 94.56158375740051, eps 0.0027354782684687108, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 125
=== ep: 126, time 106.4246928691864, eps 0.0026508379945489875, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 126
goal_identified
=== ep: 127, time 113.22717380523682, eps 0.0025703256754987464, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 127
=== ep: 128, time 100.18052005767822, eps 0.0024937399885833667, sum reward: 0, score_diff -4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 128
=== ep: 129, time 101.51111507415771, eps 0.0024208894296938593, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 129
goal_identified
=== ep: 130, time 109.92821550369263, eps 0.0023515918344868374, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 130
=== ep: 131, time 107.14293169975281, eps 0.002285673922878779, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 131
goal_identified
goal_identified
goal_identified
=== ep: 132, time 99.53867554664612, eps 0.0022229708657555565, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 29
=== ep: 133, time 117.8353865146637, eps 0.0021633258728137976, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 133
goal_identified
goal_identified
=== ep: 134, time 117.96743106842041, eps 0.0021065898005034594, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 36
=== ep: 135, time 112.53742957115173, eps 0.002052620779091266, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 135
=== ep: 136, time 141.21618175506592, eps 0.0020012838579124784, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 136
=== ep: 137, time 116.9658272266388, eps 0.0019524506679239415, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 137
=== ep: 138, time 117.02973484992981, eps 0.001905999100714611, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 138
=== ep: 139, time 143.24225234985352, eps 0.001861813003170924, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 139
goal_identified
=== ep: 140, time 117.53048157691956, eps 0.0018197818870335101, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 140
=== ep: 141, time 117.11348676681519, eps 0.0017798006526189953, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 141
=== ep: 142, time 131.2365140914917, eps 0.0017417693260160481, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 142
=== ep: 143, time 115.29526734352112, eps 0.0017055928090985275, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 143
=== ep: 144, time 140.33841514587402, eps 0.0016711806417306348, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 144
=== ep: 145, time 108.36881446838379, eps 0.0016384467755694515, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 145
=== ep: 146, time 118.35567307472229, eps 0.0016073093588992661, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 146
=== ep: 147, time 137.69112372398376, eps 0.0015776905319596466, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 147
goal_identified
goal_identified
=== ep: 148, time 113.27295279502869, eps 0.0015495162322554856, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 148
goal_identified
=== ep: 149, time 127.98896622657776, eps 0.0015227160093621863, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 149
=== ep: 150, time 125.87577962875366, eps 0.0014972228487629025, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 150
=== ep: 151, time 114.85775971412659, eps 0.0014729730042773413, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 151
=== ep: 152, time 144.12502121925354, eps 0.001449905838663109, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 152
=== ep: 153, time 120.31957221031189, eps 0.00142796367199102, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 153
=== ep: 154, time 121.84888887405396, eps 0.0014070916374152305, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 154
=== ep: 155, time 133.63523769378662, eps 0.001387237543977543, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 155
=== ep: 156, time 117.74600601196289, eps 0.0013683517461028282, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 156
=== ep: 157, time 120.64066362380981, eps 0.0013503870194592265, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 157
goal_identified
=== ep: 158, time 119.83762526512146, eps 0.0013332984428727204, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 158
=== ep: 159, time 127.9479398727417, eps 0.001317043286000802, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 159
goal_identified
goal_identified
=== ep: 160, time 130.3343756198883, eps 0.0013015809024843582, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 40
=== ep: 161, time 118.59534168243408, eps 0.0012868726283106018, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 161
=== ep: 162, time 118.86566638946533, eps 0.0012728816851329014, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 162
goal_identified
=== ep: 163, time 134.34620141983032, eps 0.0012595730883057546, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 163
=== ep: 164, time 129.03354048728943, eps 0.001246913559404956, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 164
=== ep: 165, time 130.59771037101746, eps 0.0012348714430141991, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 165
goal_identified
=== ep: 166, time 134.64416551589966, eps 0.0012234166275700486, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 166
goal_identified
=== ep: 167, time 119.19178509712219, eps 0.001212520470067348, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 167
goal_identified
goal_identified
=== ep: 168, time 134.75061106681824, eps 0.0012021557244367845, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 43
goal_identified
=== ep: 169, time 126.48954653739929, eps 0.0011922964734155277, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 169
=== ep: 170, time 118.64816975593567, eps 0.001182918063740569, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 170
goal_identified
=== ep: 171, time 151.66503620147705, eps 0.0011739970445027263, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 171
=== ep: 172, time 117.20669770240784, eps 0.0011655111085071537, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 172
=== ep: 173, time 131.83589577674866, eps 0.001157439036493735, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 173
goal_identified
=== ep: 174, time 129.62626123428345, eps 0.0011497606440778825, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 174
=== ep: 175, time 115.91191625595093, eps 0.0011424567312790603, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 175
goal_identified
=== ep: 176, time 149.58554458618164, eps 0.0011355090345108335, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 176
=== ep: 177, time 115.58865284919739, eps 0.0011289001809123877, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 177
=== ep: 178, time 117.02654838562012, eps 0.0011226136449073282, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 178
goal_identified
=== ep: 179, time 139.6817364692688, eps 0.001116633706881133, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 179
=== ep: 180, time 112.24344205856323, eps 0.001110945413873925, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 180
goal_identified
=== ep: 181, time 126.3814046382904, eps 0.001105534542190287, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 181
goal_identified
=== ep: 182, time 130.0036644935608, eps 0.0011003875618326132, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 182
goal_identified
=== ep: 183, time 116.21180510520935, eps 0.0010954916026690664, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 183
goal_identified
=== ep: 184, time 139.62812304496765, eps 0.001090834422251547, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 184
=== ep: 185, time 117.52931427955627, eps 0.0010864043752031938, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 185
goal_identified
=== ep: 186, time 116.18371176719666, eps 0.0010821903840988777, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 186
=== ep: 187, time 153.5892457962036, eps 0.0010781819117658682, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 187
=== ep: 188, time 115.51332569122314, eps 0.0010743689349354123, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 188
=== ep: 189, time 117.30378866195679, eps 0.0010707419191793434, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 189
goal_identified
=== ep: 190, time 145.62348771095276, eps 0.0010672917950690429, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 190
goal_identified
goal_identified
=== ep: 191, time 115.23369717597961, eps 0.0010640099354971456, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 52
goal_identified
goal_identified
=== ep: 192, time 135.9188311100006, eps 0.0010608881341052777, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 192
goal_identified
=== ep: 193, time 136.31370210647583, eps 0.0010579185847638855, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 193
=== ep: 194, time 111.64639949798584, eps 0.0010550938620528466, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 194
goal_identified
=== ep: 195, time 134.39610362052917, eps 0.001052406902694051, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 195
=== ep: 196, time 126.88764214515686, eps 0.001049850987889527, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 196
=== ep: 197, time 132.26288151741028, eps 0.0010474197265209469, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 197
goal_identified
=== ep: 198, time 150.0340507030487, eps 0.0010451070391685015, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 198
=== ep: 199, time 134.19634103775024, eps 0.001042907142909185, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 199
goal_identified
=== ep: 200, time 130.54629468917847, eps 0.001040814536856474, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 200
=== ep: 201, time 134.61963534355164, eps 0.0010388239884052469, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 201
=== ep: 202, time 135.73017239570618, eps 0.0010369305201475454, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 202
=== ep: 203, time 152.27412128448486, eps 0.0010351293974264616, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 203
goal_identified
=== ep: 204, time 136.31118035316467, eps 0.00103341611649703, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 204
goal_identified
=== ep: 205, time 126.58358907699585, eps 0.0010317863932645186, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 205
=== ep: 206, time 146.9958052635193, eps 0.0010302361525719613, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 206
=== ep: 207, time 127.31848764419556, eps 0.0010287615180101426, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 207
goal_identified
=== ep: 208, time 146.7552638053894, eps 0.001027358802224555, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 208
goal_identified
=== ep: 209, time 141.80881023406982, eps 0.0010260244976950921, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 209
goal_identified
goal_identified
=== ep: 210, time 122.66809630393982, eps 0.0010247552679654227, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 210
=== ep: 211, time 151.53728890419006, eps 0.00102354793930011, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 211
=== ep: 212, time 125.84377217292786, eps 0.0010223994927486214, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 212
=== ep: 213, time 140.44816303253174, eps 0.001021307056596379, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 213
=== ep: 214, time 134.46987199783325, eps 0.0010202678991839778, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 214
=== ep: 215, time 122.17462849617004, eps 0.0010192794220766138, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 215
goal_identified
=== ep: 216, time 162.14948654174805, eps 0.0010183391535666436, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 216
goal_identified
=== ep: 217, time 121.1484477519989, eps 0.0010174447424930286, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 217
=== ep: 218, time 125.13680768013, eps 0.0010165939523622068, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 218
goal_identified
=== ep: 219, time 147.07100558280945, eps 0.0010157846557556941, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 219
goal_identified
=== ep: 220, time 125.93255996704102, eps 0.001015014829010431, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 220
=== ep: 221, time 151.20002126693726, eps 0.0010142825471585687, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 221
goal_identified
=== ep: 222, time 126.28302073478699, eps 0.0010135859791140496, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 222
=== ep: 223, time 124.58013200759888, eps 0.0010129233830939361, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 223
=== ep: 224, time 157.3020260334015, eps 0.0010122931022630473, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 224
=== ep: 225, time 124.5447826385498, eps 0.001011693560591007, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 225
=== ep: 226, time 140.65479254722595, eps 0.0010111232589113477, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 226
=== ep: 227, time 136.38673377037048, eps 0.0010105807711728136, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 227
goal_identified
=== ep: 228, time 128.3218355178833, eps 0.0010100647408734893, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 228
=== ep: 229, time 153.59455108642578, eps 0.001009573877668838, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 229
goal_identified
=== ep: 230, time 128.85069274902344, eps 0.001009106954145169, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 230
=== ep: 231, time 135.31475043296814, eps 0.0010086628027504636, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 231
=== ep: 232, time 148.51480531692505, eps 0.0010082403128748867, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 232
goal_identified
=== ep: 233, time 135.47895646095276, eps 0.0010078384280736842, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 233
=== ep: 234, time 143.11439394950867, eps 0.001007456143425521, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 234
=== ep: 235, time 132.46794629096985, eps 0.001007092503019653, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 235
goal_identified
=== ep: 236, time 128.77653551101685, eps 0.001006746597565654, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 236
=== ep: 237, time 146.57602620124817, eps 0.001006417562119715, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 237
=== ep: 238, time 135.1482720375061, eps 0.0010061045739218342, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 238
=== ep: 239, time 125.51300024986267, eps 0.0010058068503384884, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 239
goal_identified
goal_identified
=== ep: 240, time 145.8104817867279, eps 0.001005523646905642, sum reward: 2, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 240
goal_identified
goal_identified
=== ep: 241, time 122.76971697807312, eps 0.001005254255467199, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 61
goal_identified
=== ep: 242, time 150.27907371520996, eps 0.0010049980024042435, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 242
goal_identified
=== ep: 243, time 127.67409873008728, eps 0.0010047542469506416, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 243
goal_identified
=== ep: 244, time 126.94278526306152, eps 0.0010045223795907931, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 244
goal_identified
=== ep: 245, time 155.79480838775635, eps 0.001004301820535524, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 245
=== ep: 246, time 126.60982084274292, eps 0.0010040920182723119, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 246
=== ep: 247, time 156.95336961746216, eps 0.0010038924481862177, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 247
goal_identified
goal_identified
=== ep: 248, time 126.52183961868286, eps 0.0010037026112480747, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 80
=== ep: 249, time 124.56058168411255, eps 0.0010035220327666559, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 249
goal_identified
=== ep: 250, time 159.52909445762634, eps 0.0010033502612016988, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 250
=== ep: 251, time 133.91601371765137, eps 0.001003186867034819, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 251
=== ep: 252, time 141.98049783706665, eps 0.001003031441695491, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 252
=== ep: 253, time 134.82114171981812, eps 0.0010028835965394094, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 253
=== ep: 254, time 129.80238723754883, eps 0.0010027429618766747, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 254
=== ep: 255, time 161.8617115020752, eps 0.0010026091860473767, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 255
=== ep: 256, time 127.15142703056335, eps 0.0010024819345422614, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 256
=== ep: 257, time 151.74906182289124, eps 0.0010023608891662839, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 257
goal_identified
=== ep: 258, time 127.26952195167542, eps 0.001002245747242954, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 258
goal_identified
=== ep: 259, time 134.9572880268097, eps 0.0010021362208574892, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 259
goal_identified
goal_identified
=== ep: 260, time 151.68531489372253, eps 0.001002032036136876, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 260
goal_identified
goal_identified
=== ep: 261, time 123.61879396438599, eps 0.0010019329325650452, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 90
=== ep: 262, time 152.5229525566101, eps 0.0010018386623314465, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 262
=== ep: 263, time 135.18956184387207, eps 0.0010017489897113931, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 263
goal_identified
=== ep: 264, time 137.85186672210693, eps 0.0010016636904766263, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 264
goal_identified
=== ep: 265, time 156.21471977233887, eps 0.0010015825513346283, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 265
goal_identified
=== ep: 266, time 137.9526343345642, eps 0.0010015053693952815, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 266
=== ep: 267, time 144.77867794036865, eps 0.0010014319516635345, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 267
goal_identified
=== ep: 268, time 127.67272162437439, eps 0.0010013621145568167, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 268
goal_identified
=== ep: 269, time 132.45360255241394, eps 0.0010012956834459848, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 269
goal_identified
=== ep: 270, time 168.35704350471497, eps 0.0010012324922186594, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 270
goal_identified
=== ep: 271, time 167.92890429496765, eps 0.001001172382863857, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 271
=== ep: 272, time 206.5437388420105, eps 0.0010011152050768812, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 272
=== ep: 273, time 216.24767518043518, eps 0.0010010608158834819, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 273
=== ep: 274, time 248.0492126941681, eps 0.0010010090792823456, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 274
=== ep: 275, time 203.50893807411194, eps 0.0010009598659050213, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 275
=== ep: 276, time 250.82101917266846, eps 0.0010009130526924313, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 276
=== ep: 277, time 195.2277388572693, eps 0.0010008685225871602, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 277
goal_identified
=== ep: 278, time 243.21732449531555, eps 0.0010008261642407504, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 278
=== ep: 279, time 208.50174069404602, eps 0.001000785871735272, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 279
=== ep: 280, time 224.84337639808655, eps 0.0010007475443184742, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 280
=== ep: 281, time 213.72588443756104, eps 0.001000711086151851, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 281
=== ep: 282, time 228.20091795921326, eps 0.0010006764060709957, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 282
goal_identified
=== ep: 283, time 210.08658719062805, eps 0.001000643417357642, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 283
=== ep: 284, time 210.75871443748474, eps 0.0010006120375228235, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 284
=== ep: 285, time 193.60706210136414, eps 0.0010005821881006083, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 285
=== ep: 286, time 217.12337684631348, eps 0.0010005537944518927, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 286
goal_identified
=== ep: 287, time 199.3858184814453, eps 0.0010005267855777657, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 287
goal_identified
goal_identified
=== ep: 288, time 212.92843008041382, eps 0.0010005010939419733, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 101
goal_identified
=== ep: 289, time 217.84785413742065, eps 0.001000476655302044, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 289
=== ep: 290, time 218.97992038726807, eps 0.0010004534085486486, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 290
goal_identified
=== ep: 291, time 220.17359805107117, eps 0.0010004312955527947, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 291
=== ep: 292, time 205.95483255386353, eps 0.0010004102610204745, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 292
=== ep: 293, time 218.1920783519745, eps 0.0010003902523544011, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 293
=== ep: 294, time 210.44995498657227, eps 0.0010003712195224871, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 294
goal_identified
=== ep: 295, time 206.83468747138977, eps 0.0010003531149327387, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 295
=== ep: 296, time 199.71431589126587, eps 0.0010003358933142518, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 296
goal_identified
=== ep: 297, time 217.27772307395935, eps 0.0010003195116040093, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 297
=== ep: 298, time 185.14614915847778, eps 0.0010003039288392032, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 298
=== ep: 299, time 207.85163283348083, eps 0.0010002891060548044, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 299
goal_identified
=== ep: 300, time 178.08489990234375, eps 0.0010002750061861312, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 300
goal_identified
=== ep: 301, time 182.29394578933716, eps 0.0010002615939761676, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 301
=== ep: 302, time 175.22892379760742, eps 0.001000248835887403, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 302
goal_identified
goal_identified
=== ep: 303, time 186.9794590473175, eps 0.0010002367000179694, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 121
=== ep: 304, time 173.55867552757263, eps 0.0010002251560218723, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 304
=== ep: 305, time 203.99515962600708, eps 0.0010002141750331084, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 305
goal_identified
=== ep: 306, time 161.35895800590515, eps 0.0010002037295934862, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 306
=== ep: 307, time 178.40800619125366, eps 0.0010001937935839656, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 307
goal_identified
goal_identified
goal_identified
=== ep: 308, time 163.55121970176697, eps 0.0010001843421593476, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 134
=== ep: 309, time 152.8019642829895, eps 0.0010001753516861473, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 309
goal_identified
=== ep: 310, time 172.96446800231934, eps 0.0010001667996834991, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 310
goal_identified
=== ep: 311, time 151.79880833625793, eps 0.001000158664766942, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 311
goal_identified
=== ep: 312, time 185.03247332572937, eps 0.0010001509265949466, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 312
=== ep: 313, time 147.3920030593872, eps 0.001000143565818053, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 313
=== ep: 314, time 185.56367874145508, eps 0.0010001365640304844, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 314
=== ep: 315, time 145.53557562828064, eps 0.0010001299037241253, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 315
=== ep: 316, time 169.63070344924927, eps 0.0010001235682447402, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 316
=== ep: 317, time 160.5772442817688, eps 0.0010001175417503308, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 317
=== ep: 318, time 160.18387246131897, eps 0.0010001118091715218, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 318
=== ep: 319, time 170.7814381122589, eps 0.0010001063561738807, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 319
goal_identified
=== ep: 320, time 152.14877772331238, eps 0.0010001011691220727, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 320
goal_identified
=== ep: 321, time 178.4066240787506, eps 0.0010000962350457665, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 321
goal_identified
=== ep: 322, time 148.27710270881653, eps 0.0010000915416072012, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 322
=== ep: 323, time 171.39799976348877, eps 0.0010000870770703358, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 323
goal_identified
=== ep: 324, time 150.86416053771973, eps 0.0010000828302715028, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 324
=== ep: 325, time 158.26319289207458, eps 0.0010000787905914928, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 325
=== ep: 326, time 170.93566989898682, eps 0.0010000749479290019, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 326
=== ep: 327, time 159.29835844039917, eps 0.001000071292675372, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 327
goal_identified
=== ep: 328, time 183.50899958610535, eps 0.001000067815690565, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 328
=== ep: 329, time 159.6360960006714, eps 0.0010000645082803084, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 329
goal_identified
=== ep: 330, time 178.31305861473083, eps 0.0010000613621743532, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 330
=== ep: 331, time 169.23320293426514, eps 0.0010000583695057963, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 331
goal_identified
=== ep: 332, time 156.43109846115112, eps 0.0010000555227914069, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 332
=== ep: 333, time 180.17323923110962, eps 0.0010000528149129166, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 333
=== ep: 334, time 150.4394085407257, eps 0.0010000502390992187, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 334
goal_identified
=== ep: 335, time 188.22624802589417, eps 0.0010000477889094373, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 335
goal_identified
goal_identified
=== ep: 336, time 147.76820540428162, eps 0.0010000454582168217, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 160
=== ep: 337, time 190.3511438369751, eps 0.001000043241193426, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 337
=== ep: 338, time 159.53903770446777, eps 0.0010000411322955373, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 338
=== ep: 339, time 164.9258258342743, eps 0.0010000391262498123, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 339
goal_identified
=== ep: 340, time 167.70919466018677, eps 0.001000037218040092, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 340
=== ep: 341, time 159.1631202697754, eps 0.0010000354028948577, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 341
goal_identified
goal_identified
=== ep: 342, time 158.50190687179565, eps 0.0010000336762753012, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 168
goal_identified
=== ep: 343, time 145.28031396865845, eps 0.001000032033863974, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 343
=== ep: 344, time 147.1575050354004, eps 0.0010000304715539925, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 344
goal_identified
=== ep: 345, time 150.06043910980225, eps 0.001000028985438768, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 345
=== ep: 346, time 136.85640025138855, eps 0.001000027571802238, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 346
=== ep: 347, time 172.72392797470093, eps 0.0010000262271095755, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 347
goal_identified
=== ep: 348, time 138.94593024253845, eps 0.0010000249479983478, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 348
goal_identified
goal_identified
=== ep: 349, time 157.84174919128418, eps 0.0010000237312701107, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 191
goal_identified
=== ep: 350, time 135.89127445220947, eps 0.00100002257388241, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 350
=== ep: 351, time 137.42026662826538, eps 0.0010000214729411737, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 351
goal_identified
goal_identified
=== ep: 352, time 167.88288116455078, eps 0.0010000204256934752, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 241
=== ep: 353, time 136.70271015167236, eps 0.0010000194295206493, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 353
=== ep: 354, time 169.71534609794617, eps 0.0010000184819317455, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 354
=== ep: 355, time 135.66576313972473, eps 0.001000017580557298, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 355
=== ep: 356, time 150.21747589111328, eps 0.001000016723143401, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 356
goal_identified
goal_identified
=== ep: 357, time 163.7160303592682, eps 0.0010000159075460732, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 248
=== ep: 358, time 142.9059329032898, eps 0.0010000151317258964, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 358
=== ep: 359, time 151.37525844573975, eps 0.0010000143937429161, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 359
=== ep: 360, time 149.68885326385498, eps 0.0010000136917517905, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 360
=== ep: 361, time 144.9363396167755, eps 0.001000013023997176, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 361
goal_identified
goal_identified
=== ep: 362, time 171.49916887283325, eps 0.0010000123888093385, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 261
goal_identified
goal_identified
=== ep: 363, time 152.26459527015686, eps 0.0010000117845999773, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 363
=== ep: 364, time 164.5766041278839, eps 0.0010000112098582543, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 364
=== ep: 365, time 161.4156813621521, eps 0.001000010663147016, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 365
goal_identified
goal_identified
=== ep: 366, time 130.5852084159851, eps 0.0010000101430991996, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 366
=== ep: 367, time 183.6154749393463, eps 0.0010000096484144142, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 367
goal_identified
=== ep: 368, time 133.8583242893219, eps 0.0010000091778556905, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 368
goal_identified
goal_identified
=== ep: 369, time 176.67309594154358, eps 0.0010000087302463867, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 369
goal_identified
=== ep: 370, time 143.15798139572144, eps 0.001000008304467246, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 370
=== ep: 371, time 155.6229863166809, eps 0.0010000078994535993, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 371
=== ep: 372, time 161.44422674179077, eps 0.0010000075141927012, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 372
goal_identified
=== ep: 373, time 148.35197567939758, eps 0.0010000071477211988, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 373
goal_identified
=== ep: 374, time 174.55574011802673, eps 0.0010000067991227223, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 374
goal_identified
goal_identified
=== ep: 375, time 151.7340874671936, eps 0.0010000064675255943, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 288
goal_identified
=== ep: 376, time 149.52302241325378, eps 0.001000006152100649, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 376
goal_identified
=== ep: 377, time 172.2586588859558, eps 0.0010000058520591598, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 377
goal_identified
=== ep: 378, time 145.49725914001465, eps 0.0010000055666508666, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 378
=== ep: 379, time 186.63210201263428, eps 0.0010000052951621003, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 379
goal_identified
=== ep: 380, time 146.25465655326843, eps 0.0010000050369139975, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 380
=== ep: 381, time 179.88199281692505, eps 0.001000004791260803, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 381
=== ep: 382, time 152.3973286151886, eps 0.0010000045575882562, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 382
goal_identified
=== ep: 383, time 169.28372168540955, eps 0.001000004335312054, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 383
=== ep: 384, time 164.8669822216034, eps 0.0010000041238763903, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 384
=== ep: 385, time 154.66027355194092, eps 0.0010000039227525655, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 385
=== ep: 386, time 178.58950757980347, eps 0.0010000037314376652, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 386
goal_identified
goal_identified
=== ep: 387, time 160.21922326087952, eps 0.001000003549453303, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 303
=== ep: 388, time 140.64056849479675, eps 0.0010000033763444226, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 388
goal_identified
=== ep: 389, time 177.7085897922516, eps 0.001000003211678162, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 389
=== ep: 390, time 139.72860407829285, eps 0.0010000030550427698, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 390
=== ep: 391, time 190.04704642295837, eps 0.0010000029060465757, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 391
=== ep: 392, time 134.77293920516968, eps 0.0010000027643170119, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 392
goal_identified
=== ep: 393, time 154.23197841644287, eps 0.0010000026294996803, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 393
=== ep: 394, time 159.75738978385925, eps 0.0010000025012574677, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 394
goal_identified
goal_identified
=== ep: 395, time 138.16710591316223, eps 0.0010000023792697014, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 336
=== ep: 396, time 183.68917059898376, eps 0.0010000022632313489, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 396
=== ep: 397, time 140.8308126926422, eps 0.0010000021528522535, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 397
goal_identified
=== ep: 398, time 166.69373989105225, eps 0.00100000204785641, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 398
=== ep: 399, time 145.37114715576172, eps 0.0010000019479812744, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 399
=== ep: 400, time 160.88558888435364, eps 0.0010000018529771066, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 400
=== ep: 401, time 168.66217422485352, eps 0.0010000017626063467, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 401
goal_identified
=== ep: 402, time 155.51976370811462, eps 0.0010000016766430208, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 402
goal_identified
goal_identified
=== ep: 403, time 166.0101854801178, eps 0.0010000015948721758, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 403
=== ep: 404, time 151.6882152557373, eps 0.001000001517089342, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 404
=== ep: 405, time 152.3090214729309, eps 0.0010000014431000217, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 405
goal_identified
=== ep: 406, time 165.59532499313354, eps 0.001000001372719203, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 406
=== ep: 407, time 147.67174863815308, eps 0.0010000013057708975, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 407
goal_identified
goal_identified
=== ep: 408, time 187.05926966667175, eps 0.0010000012420876994, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 342
=== ep: 409, time 138.9187638759613, eps 0.0010000011815103674, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 409
=== ep: 410, time 176.99305200576782, eps 0.001000001123887427, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 410
=== ep: 411, time 151.6806607246399, eps 0.0010000010690747903, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 411
=== ep: 412, time 168.350665807724, eps 0.0010000010169353975, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 412
=== ep: 413, time 166.15782642364502, eps 0.0010000009673388729, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 413
=== ep: 414, time 162.1214418411255, eps 0.0010000009201611994, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 414
=== ep: 415, time 170.33883714675903, eps 0.0010000008752844081, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 415
=== ep: 416, time 157.83260536193848, eps 0.0010000008325962838, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 416
=== ep: 417, time 149.45679473876953, eps 0.001000000791990084, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 417
=== ep: 418, time 163.22940492630005, eps 0.0010000007533642718, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 418
goal_identified
=== ep: 419, time 152.14104580879211, eps 0.0010000007166222626, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 419
goal_identified
goal_identified
goal_identified
=== ep: 420, time 171.08604669570923, eps 0.0010000006816721825, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 349
=== ep: 421, time 155.40926671028137, eps 0.001000000648426638, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 421
goal_identified
goal_identified
=== ep: 422, time 154.77647137641907, eps 0.0010000006168024976, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 352
goal_identified
=== ep: 423, time 177.5764617919922, eps 0.0010000005867206849, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 423
goal_identified
=== ep: 424, time 151.17428755760193, eps 0.0010000005581059794, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 424
=== ep: 425, time 173.9905195236206, eps 0.0010000005308868295, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 425
goal_identified
=== ep: 426, time 144.32215642929077, eps 0.0010000005049951733, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 426
=== ep: 427, time 201.33882188796997, eps 0.001000000480366268, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 427
goal_identified
=== ep: 428, time 143.4511959552765, eps 0.0010000004569385287, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 428
=== ep: 429, time 187.4128897190094, eps 0.0010000004346533736, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 429
goal_identified
goal_identified
goal_identified
=== ep: 430, time 146.39330911636353, eps 0.0010000004134550786, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 357
=== ep: 431, time 162.59667348861694, eps 0.0010000003932906364, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 431
goal_identified
=== ep: 432, time 166.55459237098694, eps 0.0010000003741096257, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 432
=== ep: 433, time 154.7726399898529, eps 0.001000000355864084, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 433
=== ep: 434, time 186.10684323310852, eps 0.0010000003385083878, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 434
goal_identified
goal_identified
=== ep: 435, time 160.86779642105103, eps 0.001000000321999139, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 362
goal_identified
=== ep: 436, time 175.96234607696533, eps 0.0010000003062950555, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 436
goal_identified
=== ep: 437, time 167.32669734954834, eps 0.0010000002913568694, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 437
goal_identified
=== ep: 438, time 151.89350128173828, eps 0.0010000002771472273, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 438
goal_identified
=== ep: 439, time 189.29670906066895, eps 0.0010000002636305976, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 439
goal_identified
=== ep: 440, time 146.98536777496338, eps 0.0010000002507731815, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 440
goal_identified
goal_identified
=== ep: 441, time 195.30626368522644, eps 0.0010000002385428292, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 375
=== ep: 442, time 151.66896510124207, eps 0.0010000002269089582, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 442
=== ep: 443, time 187.48703241348267, eps 0.0010000002158424776, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 443
=== ep: 444, time 154.98711323738098, eps 0.0010000002053157158, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 444
goal_identified
goal_identified
goal_identified
=== ep: 445, time 169.38335847854614, eps 0.0010000001953023503, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 387
goal_identified
=== ep: 446, time 167.6563675403595, eps 0.001000000185777342, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 446
goal_identified
=== ep: 447, time 155.54014825820923, eps 0.0010000001767168742, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 447
=== ep: 448, time 177.84273171424866, eps 0.0010000001680982905, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 448
goal_identified
goal_identified
goal_identified
=== ep: 449, time 161.4312870502472, eps 0.0010000001599000403, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 395
=== ep: 450, time 176.42391777038574, eps 0.0010000001521016232, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 450
goal_identified
=== ep: 451, time 165.04819202423096, eps 0.0010000001446835395, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 451
=== ep: 452, time 165.55840420722961, eps 0.0010000001376272401, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 452
goal_identified
=== ep: 453, time 167.92398929595947, eps 0.0010000001309150804, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 453
=== ep: 454, time 146.49975895881653, eps 0.0010000001245302765, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 454
=== ep: 455, time 182.11431336402893, eps 0.0010000001184568633, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 455
goal_identified
=== ep: 456, time 159.35660815238953, eps 0.0010000001126796538, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 456
=== ep: 457, time 191.8042562007904, eps 0.0010000001071842023, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 457
goal_identified
goal_identified
=== ep: 458, time 150.15341973304749, eps 0.001000000101956767, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 408
goal_identified
=== ep: 459, time 159.71557307243347, eps 0.001000000096984277, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 459
=== ep: 460, time 164.8345844745636, eps 0.001000000092254298, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 460
goal_identified
goal_identified
=== ep: 461, time 151.85247683525085, eps 0.0010000000877550027, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 422
goal_identified
=== ep: 462, time 178.42141246795654, eps 0.0010000000834751407, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 462
=== ep: 463, time 148.66942024230957, eps 0.00100000007940401, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 463
goal_identified
goal_identified
goal_identified
=== ep: 464, time 175.10193920135498, eps 0.0010000000755314307, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 435
goal_identified
=== ep: 465, time 145.54874229431152, eps 0.0010000000718477194, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 465
goal_identified
=== ep: 466, time 149.32546019554138, eps 0.0010000000683436647, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 466
=== ep: 467, time 168.9083731174469, eps 0.001000000065010505, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 467
=== ep: 468, time 151.98738622665405, eps 0.0010000000618399052, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 468
=== ep: 469, time 166.38831734657288, eps 0.0010000000588239375, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 469
goal_identified
=== ep: 470, time 156.77825379371643, eps 0.0010000000559550603, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 470
=== ep: 471, time 144.65688157081604, eps 0.0010000000532260998, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 471
goal_identified
goal_identified
=== ep: 472, time 171.3262655735016, eps 0.0010000000506302322, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 472
=== ep: 473, time 149.17425918579102, eps 0.0010000000481609666, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 473
=== ep: 474, time 187.12160468101501, eps 0.0010000000458121286, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 474
=== ep: 475, time 139.08590364456177, eps 0.0010000000435778447, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 475
goal_identified
goal_identified
=== ep: 476, time 138.96963763237, eps 0.001000000041452528, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 441
=== ep: 477, time 176.3495008945465, eps 0.0010000000394308644, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 477
goal_identified
=== ep: 478, time 146.28414487838745, eps 0.0010000000375077985, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 478
goal_identified
=== ep: 479, time 179.38439178466797, eps 0.0010000000356785216, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 479
=== ep: 480, time 146.3638050556183, eps 0.0010000000339384595, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 480
=== ep: 481, time 157.8594105243683, eps 0.0010000000322832614, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 481
goal_identified
goal_identified
=== ep: 482, time 161.01399564743042, eps 0.0010000000307087882, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 482
goal_identified
=== ep: 483, time 149.10054564476013, eps 0.001000000029211103, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 483
=== ep: 484, time 176.08336210250854, eps 0.0010000000277864607, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 484
goal_identified
goal_identified
=== ep: 485, time 147.43196558952332, eps 0.0010000000264312988, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 458
=== ep: 486, time 142.35864305496216, eps 0.0010000000251422292, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 486
=== ep: 487, time 169.24046277999878, eps 0.0010000000239160282, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 487
goal_identified
=== ep: 488, time 157.39107275009155, eps 0.00100000002274963, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 488
goal_identified
=== ep: 489, time 180.30479097366333, eps 0.0010000000216401172, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 489
goal_identified
=== ep: 490, time 153.07188415527344, eps 0.0010000000205847162, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 490
=== ep: 491, time 154.60296058654785, eps 0.0010000000195807877, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 491
goal_identified
=== ep: 492, time 181.9411542415619, eps 0.0010000000186258216, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 492
goal_identified
=== ep: 493, time 146.66749787330627, eps 0.0010000000177174295, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 493
goal_identified
goal_identified
=== ep: 494, time 188.03227877616882, eps 0.0010000000168533404, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 461
=== ep: 495, time 142.04269289970398, eps 0.0010000000160313932, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 495
=== ep: 496, time 161.96968865394592, eps 0.001000000015249533, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 496
=== ep: 497, time 151.54791617393494, eps 0.0010000000145058043, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 497
=== ep: 498, time 144.23710465431213, eps 0.001000000013798348, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 498
goal_identified
=== ep: 499, time 178.606920003891, eps 0.0010000000131253947, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 499
goal_identified
=== ep: 500, time 153.5573389530182, eps 0.0010000000124852615, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 500
=== ep: 501, time 169.28851747512817, eps 0.0010000000118763482, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 501
=== ep: 502, time 156.86613583564758, eps 0.0010000000112971319, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 502
goal_identified
=== ep: 503, time 158.78433346748352, eps 0.0010000000107461642, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 503
=== ep: 504, time 176.80970311164856, eps 0.0010000000102220676, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 504
=== ep: 505, time 164.95722675323486, eps 0.0010000000097235315, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 505
=== ep: 506, time 165.1719138622284, eps 0.0010000000092493092, sum reward: 0, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 464
=== ep: 507, time 173.61302423477173, eps 0.0010000000087982152, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 507
=== ep: 508, time 138.74139595031738, eps 0.0010000000083691212, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 508
goal_identified
=== ep: 509, time 182.88829469680786, eps 0.0010000000079609542, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 509
=== ep: 510, time 142.11265444755554, eps 0.001000000007572694, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 510
=== ep: 511, time 181.19803714752197, eps 0.0010000000072033692, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 511
=== ep: 512, time 161.81452798843384, eps 0.001000000006852057, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 512
=== ep: 513, time 153.11472702026367, eps 0.001000000006517878, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 513
goal_identified
=== ep: 514, time 169.0722849369049, eps 0.0010000000061999974, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 514
=== ep: 515, time 148.36907958984375, eps 0.0010000000058976199, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 515
goal_identified
=== ep: 516, time 182.91180801391602, eps 0.0010000000056099897, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 516
=== ep: 517, time 145.2959921360016, eps 0.0010000000053363872, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 517
goal_identified
=== ep: 518, time 164.46495723724365, eps 0.0010000000050761286, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 518
=== ep: 519, time 161.3931348323822, eps 0.001000000004828563, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 519
goal_identified
goal_identified
=== ep: 520, time 143.71260356903076, eps 0.001000000004593071, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 476
goal_identified
goal_identified
=== ep: 521, time 179.73127937316895, eps 0.0010000000043690644, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 485
=== ep: 522, time 141.82939505577087, eps 0.0010000000041559827, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 522
=== ep: 523, time 153.07592725753784, eps 0.0010000000039532928, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 523
goal_identified
=== ep: 524, time 166.5172061920166, eps 0.0010000000037604885, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 524
goal_identified
goal_identified
goal_identified
=== ep: 525, time 149.36166381835938, eps 0.0010000000035770874, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 494
goal_identified
=== ep: 526, time 145.61905527114868, eps 0.0010000000034026306, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 526
goal_identified
goal_identified
=== ep: 527, time 122.46304130554199, eps 0.0010000000032366824, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 527
=== ep: 528, time 126.96698522567749, eps 0.0010000000030788276, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 528
=== ep: 529, time 124.2513108253479, eps 0.0010000000029286714, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 529
=== ep: 530, time 121.17730331420898, eps 0.0010000000027858384, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 530
=== ep: 531, time 127.55239748954773, eps 0.0010000000026499714, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 531
goal_identified
goal_identified
=== ep: 532, time 124.60710191726685, eps 0.0010000000025207308, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 532
goal_identified
goal_identified
=== ep: 533, time 124.4702537059784, eps 0.0010000000023977934, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 506
=== ep: 534, time 125.00542092323303, eps 0.0010000000022808515, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 534
goal_identified
goal_identified
=== ep: 535, time 128.4504427909851, eps 0.0010000000021696133, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 520
goal_identified
=== ep: 536, time 132.3264765739441, eps 0.0010000000020637999, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 536
goal_identified
goal_identified
=== ep: 537, time 127.77482485771179, eps 0.0010000000019631471, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 537
goal_identified
goal_identified
=== ep: 538, time 133.13061451911926, eps 0.0010000000018674034, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 521
goal_identified
goal_identified
=== ep: 539, time 133.83594465255737, eps 0.001000000001776329, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 533
=== ep: 540, time 133.0362160205841, eps 0.0010000000016896964, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 540
=== ep: 541, time 132.29727578163147, eps 0.001000000001607289, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 541
=== ep: 542, time 139.5270230770111, eps 0.0010000000015289005, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 542
goal_identified
=== ep: 543, time 130.25009560585022, eps 0.0010000000014543352, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 543
=== ep: 544, time 136.2559208869934, eps 0.0010000000013834064, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 544
goal_identified
=== ep: 545, time 133.73596572875977, eps 0.001000000001315937, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 545
goal_identified
goal_identified
=== ep: 546, time 137.87009453773499, eps 0.0010000000012517578, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 535
=== ep: 547, time 134.30388855934143, eps 0.001000000001190709, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 547
=== ep: 548, time 137.8065116405487, eps 0.0010000000011326374, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 548
=== ep: 549, time 136.1818778514862, eps 0.001000000001077398, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 549
goal_identified
=== ep: 550, time 134.24599027633667, eps 0.0010000000010248527, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 550
goal_identified
=== ep: 551, time 132.92994809150696, eps 0.00100000000097487, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 551
goal_identified
=== ep: 552, time 132.01144123077393, eps 0.001000000000927325, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 552
=== ep: 553, time 133.97975301742554, eps 0.0010000000008820989, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 553
goal_identified
=== ep: 554, time 136.15201997756958, eps 0.0010000000008390784, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 554
=== ep: 555, time 139.25804829597473, eps 0.001000000000798156, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 555
goal_identified
=== ep: 556, time 134.24664545059204, eps 0.0010000000007592295, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 556
goal_identified
goal_identified
=== ep: 557, time 134.67632150650024, eps 0.0010000000007222014, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 557
goal_identified
=== ep: 558, time 134.93004179000854, eps 0.0010000000006869794, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 558
goal_identified
=== ep: 559, time 136.09181690216064, eps 0.001000000000653475, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 559
goal_identified
goal_identified
goal_identified
=== ep: 560, time 131.26156973838806, eps 0.0010000000006216046, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 538
goal_identified
=== ep: 561, time 132.32300782203674, eps 0.0010000000005912885, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 561
=== ep: 562, time 130.1463339328766, eps 0.0010000000005624511, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 562
=== ep: 563, time 131.82213425636292, eps 0.00100000000053502, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 563
=== ep: 564, time 129.38051080703735, eps 0.001000000000508927, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 564
=== ep: 565, time 136.47173690795898, eps 0.001000000000484106, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 565
goal_identified
goal_identified
=== ep: 566, time 130.85672163963318, eps 0.001000000000460496, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 539
=== ep: 567, time 129.8736629486084, eps 0.0010000000004380374, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 567
goal_identified
goal_identified
goal_identified
=== ep: 568, time 124.77673006057739, eps 0.001000000000416674, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 546
goal_identified
=== ep: 569, time 123.56870937347412, eps 0.0010000000003963527, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 569
goal_identified
goal_identified
=== ep: 570, time 123.40436029434204, eps 0.0010000000003770222, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 566
=== ep: 571, time 121.94069337844849, eps 0.0010000000003586346, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 571
goal_identified
goal_identified
goal_identified
=== ep: 572, time 125.64821910858154, eps 0.0010000000003411438, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 570
=== ep: 573, time 124.57330989837646, eps 0.001000000000324506, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 573
=== ep: 574, time 127.16343522071838, eps 0.0010000000003086798, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 574
=== ep: 575, time 123.89746975898743, eps 0.0010000000002936252, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 575
=== ep: 576, time 125.95043659210205, eps 0.001000000000279305, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 576
goal_identified
=== ep: 577, time 122.77475380897522, eps 0.0010000000002656831, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 577
=== ep: 578, time 124.73823380470276, eps 0.0010000000002527256, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 578
goal_identified
=== ep: 579, time 124.21621918678284, eps 0.0010000000002404, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 579
=== ep: 580, time 122.43931674957275, eps 0.0010000000002286756, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 580
=== ep: 581, time 126.25892567634583, eps 0.0010000000002175229, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 581
=== ep: 582, time 123.75434231758118, eps 0.0010000000002069142, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 582
goal_identified
goal_identified
=== ep: 583, time 124.23918747901917, eps 0.0010000000001968228, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 583
=== ep: 584, time 126.42416596412659, eps 0.0010000000001872237, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 584
=== ep: 585, time 128.0421233177185, eps 0.0010000000001780928, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 585
=== ep: 586, time 125.35099267959595, eps 0.001000000000169407, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 586
=== ep: 587, time 121.8697190284729, eps 0.001000000000161145, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 587
goal_identified
goal_identified
goal_identified
=== ep: 588, time 123.74382972717285, eps 0.0010000000001532858, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 6
=== ep: 589, time 131.89548468589783, eps 0.00100000000014581, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 589
goal_identified
=== ep: 590, time 128.5765905380249, eps 0.0010000000001386988, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 590
goal_identified
=== ep: 591, time 132.91114497184753, eps 0.0010000000001319344, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 591
=== ep: 592, time 134.30356907844543, eps 0.0010000000001255, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 592
=== ep: 593, time 129.13971519470215, eps 0.0010000000001193791, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 593
=== ep: 594, time 133.56827402114868, eps 0.001000000000113557, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 594
goal_identified
=== ep: 595, time 130.2968623638153, eps 0.0010000000001080186, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 595
=== ep: 596, time 131.33901643753052, eps 0.0010000000001027505, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 596
goal_identified
=== ep: 597, time 128.8676040172577, eps 0.0010000000000977393, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 597
goal_identified
goal_identified
=== ep: 598, time 130.66641879081726, eps 0.0010000000000929725, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 598
=== ep: 599, time 128.92201328277588, eps 0.0010000000000884382, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 599
=== ep: 600, time 133.47513055801392, eps 0.001000000000084125, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 600
=== ep: 601, time 129.24307703971863, eps 0.0010000000000800222, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 601
goal_identified
=== ep: 602, time 133.27808666229248, eps 0.0010000000000761195, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 602
goal_identified
=== ep: 603, time 125.2473042011261, eps 0.0010000000000724072, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 603
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 604, time 129.92664742469788, eps 0.0010000000000688757, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 10
goal_identified
goal_identified
goal_identified
=== ep: 605, time 132.67797136306763, eps 0.0010000000000655166, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 605
=== ep: 606, time 136.09699630737305, eps 0.0010000000000623215, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 606
goal_identified
goal_identified
=== ep: 607, time 135.10640931129456, eps 0.001000000000059282, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 607
goal_identified
=== ep: 608, time 138.24207258224487, eps 0.0010000000000563907, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 608
=== ep: 609, time 141.19344091415405, eps 0.0010000000000536405, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 609
goal_identified
=== ep: 610, time 138.57276725769043, eps 0.0010000000000510245, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 610
=== ep: 611, time 137.75146913528442, eps 0.0010000000000485358, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 611
goal_identified
goal_identified
=== ep: 612, time 135.87595558166504, eps 0.0010000000000461688, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 612
goal_identified
=== ep: 613, time 132.22676610946655, eps 0.0010000000000439171, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 613
goal_identified
=== ep: 614, time 141.75969862937927, eps 0.0010000000000417752, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 614
=== ep: 615, time 136.93196415901184, eps 0.0010000000000397378, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 615
goal_identified
=== ep: 616, time 138.37167143821716, eps 0.0010000000000377999, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 616
goal_identified
=== ep: 617, time 133.6362886428833, eps 0.0010000000000359563, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 617
goal_identified
=== ep: 618, time 137.82247638702393, eps 0.0010000000000342027, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 618
goal_identified
goal_identified
=== ep: 619, time 137.57154417037964, eps 0.0010000000000325345, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 619
goal_identified
=== ep: 620, time 138.4265260696411, eps 0.001000000000030948, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 620
goal_identified
goal_identified
=== ep: 621, time 134.8550124168396, eps 0.0010000000000294385, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 621
=== ep: 622, time 140.66183137893677, eps 0.0010000000000280028, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20 and we are deleting ep 622
goal_identified
