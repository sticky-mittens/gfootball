==> Playing in 11_vs_11_stochastic.
==>Level 1
==>OTs in this level are dict_keys(['win_game'])
==>Currently learning win_game
==>using device cuda
==>critic has 6 layers and 500 hidden units.
goal_identified
goal_identified
=== ep: 0, time 25.5162353515625, eps 0.9, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
=== ep: 1, time 24.76398801803589, eps 0.8561552526261419, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
=== ep: 2, time 24.70441436767578, eps 0.8144488388143276, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 3, time 24.776809215545654, eps 0.774776470806127, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
goal_identified
goal_identified
=== ep: 4, time 25.127979278564453, eps 0.7370389470171057, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
=== ep: 5, time 25.223846197128296, eps 0.701141903981193, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 6, time 25.47892165184021, eps 0.6669955803928644, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
=== ep: 7, time 25.244757413864136, eps 0.6345145926571234, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
=== ep: 8, time 25.090155363082886, eps 0.6036177213860398, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 9, time 39.73615550994873, eps 0.5742277083079742, sum reward: 1, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 8
goal_identified
goal_identified
=== ep: 10, time 25.07416033744812, eps 0.5462710630816575, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1
goal_identified
goal_identified
goal_identified
=== ep: 11, time 25.091770887374878, eps 0.5196778795320575, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3
goal_identified
goal_identified
=== ep: 12, time 25.631847381591797, eps 0.49438166084852986, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 7
goal_identified
=== ep: 13, time 25.377935886383057, eps 0.47031915330815344, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 13
goal_identified
goal_identified
=== ep: 14, time 24.99928593635559, eps 0.4474301881084772, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2
goal_identified
=== ep: 15, time 25.856247186660767, eps 0.42565753091417224, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 5
goal_identified
=== ep: 16, time 25.909963846206665, eps 0.4049467387413822, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 16
=== ep: 17, time 25.760538339614868, eps 0.3852460238219053, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 17
=== ep: 18, time 25.005224466323853, eps 0.3665061241067986, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 18
goal_identified
=== ep: 19, time 41.288023948669434, eps 0.3486801800855966, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 19
goal_identified
=== ep: 20, time 24.1990065574646, eps 0.3317236176131267, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
/home/ksridhar/GRF/scripts/policies_baseline.py:437: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 11 > 10.0 and we are deleting ep 4
=== ep: 21, time 25.33840775489807, eps 0.31559403645092865, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 21
=== ep: 22, time 25.898252964019775, eps 0.3002511042445735, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 22
=== ep: 23, time 25.330971479415894, eps 0.2856564556717689, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 23
goal_identified
goal_identified
=== ep: 24, time 26.03272294998169, eps 0.27177359650906974, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 6
goal_identified
goal_identified
=== ep: 25, time 25.683980226516724, eps 0.2585678123773109, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 25
goal_identified
=== ep: 26, time 25.273593187332153, eps 0.24600608193757734, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 12
=== ep: 27, time 25.584887981414795, eps 0.23405699432065646, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 27
goal_identified
=== ep: 28, time 25.502962589263916, eps 0.22269067058350425, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 28
goal_identified
=== ep: 29, time 38.63976216316223, eps 0.2118786889963241, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 29
=== ep: 30, time 25.914583921432495, eps 0.2015940139734384, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 30
goal_identified
=== ep: 31, time 25.49276304244995, eps 0.191810928470242, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 31
goal_identified
goal_identified
=== ep: 32, time 25.466954469680786, eps 0.1825049696771952, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 14
goal_identified
=== ep: 33, time 25.65865969657898, eps 0.17365286785005798, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 33
=== ep: 34, time 25.32898736000061, eps 0.16523248812340846, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 34
goal_identified
goal_identified
=== ep: 35, time 25.737991333007812, eps 0.15722277516195018, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 15
=== ep: 36, time 25.553492784500122, eps 0.1496037005112063, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 36
=== ep: 37, time 25.714106559753418, eps 0.14235621251595124, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 37
=== ep: 38, time 25.92564296722412, eps 0.13546218868114893, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 38
goal_identified
goal_identified
=== ep: 39, time 39.371456146240234, eps 0.1289043903562757, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 20
goal_identified
=== ep: 40, time 25.664770364761353, eps 0.12266641962971482, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 40
=== ep: 41, time 25.93865990638733, eps 0.116732678325436, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 41
=== ep: 42, time 25.838488578796387, eps 0.11108832899943073, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 42
=== ep: 43, time 25.557470321655273, eps 0.10571925783837377, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 43
=== ep: 44, time 26.02185893058777, eps 0.10061203936773815, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 44
=== ep: 45, time 25.705645084381104, eps 0.09575390288111604, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 45
=== ep: 46, time 25.80739164352417, eps 0.09113270050680057, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 46
=== ep: 47, time 25.67105221748352, eps 0.08673687683177911, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 47
=== ep: 48, time 26.051549434661865, eps 0.08255544000718185, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 48
=== ep: 49, time 41.71072053909302, eps 0.07857793426293408, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 49
=== ep: 50, time 25.85138201713562, eps 0.07479441376288502, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 50
goal_identified
=== ep: 51, time 26.81055188179016, eps 0.0711954177350367, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 24
goal_identified
=== ep: 52, time 26.43286371231079, eps 0.06777194681468615, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 52
goal_identified
=== ep: 53, time 26.116228818893433, eps 0.06451544054132621, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 26
=== ep: 54, time 26.63501501083374, eps 0.06141775595303503, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 54
goal_identified
=== ep: 55, time 26.619967937469482, eps 0.05847114722483011, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 55
goal_identified
=== ep: 56, time 26.0190589427948, eps 0.05566824630007096, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 56
=== ep: 57, time 26.17263913154602, eps 0.05300204446647978, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 57
goal_identified
=== ep: 58, time 26.0983989238739, eps 0.050465874830710106, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 58
goal_identified
=== ep: 59, time 50.689438819885254, eps 0.04805339564764071, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 59
goal_identified
=== ep: 60, time 26.416426420211792, eps 0.045758574462709686, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 60
=== ep: 61, time 26.51578116416931, eps 0.043575673027635695, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 61
=== ep: 62, time 27.014144897460938, eps 0.04149923295180846, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 62
=== ep: 63, time 27.097679138183594, eps 0.03952406205346913, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 63
=== ep: 64, time 26.570520162582397, eps 0.03764522137655123, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 64
goal_identified
=== ep: 65, time 27.05431604385376, eps 0.03585801284071809, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 65
=== ep: 66, time 26.02530860900879, eps 0.034157967493714775, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 66
=== ep: 67, time 25.94227433204651, eps 0.03254083433665968, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 67
=== ep: 68, time 26.412394046783447, eps 0.031002569694333147, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 68
=== ep: 69, time 51.531678438186646, eps 0.02953932710388308, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 69
=== ep: 70, time 25.90569019317627, eps 0.028147447696664333, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 70
=== ep: 71, time 26.52582359313965, eps 0.026823451049161253, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 71
=== ep: 72, time 26.241718769073486, eps 0.025564026480116013, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 72
=== ep: 73, time 26.97924304008484, eps 0.02436602477210106, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 73
goal_identified
=== ep: 74, time 27.250173807144165, eps 0.02322645029683511, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 32
=== ep: 75, time 26.120779752731323, eps 0.02214245352455219, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 75
=== ep: 76, time 26.911632299423218, eps 0.02111132389869288, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 76
=== ep: 77, time 27.061934232711792, eps 0.020130483058101077, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 77
=== ep: 78, time 26.634665489196777, eps 0.019197478389778148, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 78
=== ep: 79, time 57.29209899902344, eps 0.018309976896072843, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 79
goal_identified
=== ep: 80, time 26.172638177871704, eps 0.017465759360972027, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 80
=== ep: 81, time 27.412489891052246, eps 0.01666271480090467, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 81
=== ep: 82, time 26.460081577301025, eps 0.015898835186183367, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 82
=== ep: 83, time 27.625664710998535, eps 0.015172210419884185, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 83
goal_identified
=== ep: 84, time 27.02094078063965, eps 0.014481023561609456, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 84
goal_identified
goal_identified
goal_identified
=== ep: 85, time 25.7246036529541, eps 0.01382354628419033, sum reward: 3, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 85
goal_identified
=== ep: 86, time 27.2620530128479, eps 0.013198134551968641, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 86
=== ep: 87, time 25.993279457092285, eps 0.012603224509851407, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 87
goal_identified
=== ep: 88, time 26.808971643447876, eps 0.012037328572858524, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 88
goal_identified
=== ep: 89, time 56.32560324668884, eps 0.011499031706385502, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 89
=== ep: 90, time 26.88882851600647, eps 0.010986987887879832, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 90
=== ep: 91, time 26.630985736846924, eps 0.010499916741083536, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 91
=== ep: 92, time 26.5514657497406, eps 0.010036600334425595, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 92
=== ep: 93, time 26.99402165412903, eps 0.00959588013555861, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 93
=== ep: 94, time 35.490981578826904, eps 0.009176654114424539, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 94
=== ep: 95, time 26.489877939224243, eps 0.00877787398760545, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 95
=== ep: 96, time 27.315979480743408, eps 0.008398542597069007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 96
goal_identified
=== ep: 97, time 26.914443254470825, eps 0.008037711416753971, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 97
=== ep: 98, time 26.812096118927002, eps 0.00769447818076098, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 98
=== ep: 99, time 62.583757400512695, eps 0.007367984627217855, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 99
=== ep: 100, time 25.891003847122192, eps 0.007057414352177835, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 100
=== ep: 101, time 27.307986974716187, eps 0.006761990768184489, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 101
=== ep: 102, time 25.959892988204956, eps 0.006480975162398559, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 102
=== ep: 103, time 28.214253902435303, eps 0.006213664849431085, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 103
=== ep: 104, time 27.084457635879517, eps 0.005959391414263934, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 104
=== ep: 105, time 28.087355852127075, eps 0.005717519040864065, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 105
=== ep: 106, time 27.153124809265137, eps 0.005487442922312285, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 106
=== ep: 107, time 27.09741473197937, eps 0.005268587748470919, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 107
goal_identified
=== ep: 108, time 26.593513011932373, eps 0.005060406267408787, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 108
goal_identified
=== ep: 109, time 57.57402205467224, eps 0.004862377916986354, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 51
=== ep: 110, time 27.49200749397278, eps 0.004674007523179196, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 110
=== ep: 111, time 26.56299591064453, eps 0.004494824061885041, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 111
=== ep: 112, time 27.880434036254883, eps 0.0043243794811181555, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 112
=== ep: 113, time 26.56080961227417, eps 0.0041622475806460035, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 113
=== ep: 114, time 26.639387369155884, eps 0.0040080229462666735, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 114
=== ep: 115, time 27.575034856796265, eps 0.0038613199360621906, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 115
=== ep: 116, time 27.589887619018555, eps 0.003721771716092858, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 116
=== ep: 117, time 27.25437879562378, eps 0.0035890293431213305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 117
=== ep: 118, time 26.42895770072937, eps 0.0034627608920727634, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 118
goal_identified
=== ep: 119, time 57.10746645927429, eps 0.00334265062604924, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 119
=== ep: 120, time 27.55345368385315, eps 0.0032283982068230565, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 120
=== ep: 121, time 26.56968116760254, eps 0.0031197179438347193, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 121
=== ep: 122, time 27.261181592941284, eps 0.0030163380798177374, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 122
=== ep: 123, time 34.281211376190186, eps 0.0029180001112638996, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 123
goal_identified
goal_identified
=== ep: 124, time 26.877942323684692, eps 0.002824458142029865, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 53
=== ep: 125, time 35.00249624252319, eps 0.0027354782684687108, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 125
=== ep: 126, time 27.877950191497803, eps 0.0026508379945489875, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 126
=== ep: 127, time 29.457111358642578, eps 0.0025703256754987464, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 127
=== ep: 128, time 26.4710533618927, eps 0.0024937399885833667, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 128
=== ep: 129, time 58.80505180358887, eps 0.0024208894296938593, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 129
=== ep: 130, time 26.49310803413391, eps 0.0023515918344868374, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 130
=== ep: 131, time 25.873619079589844, eps 0.002285673922878779, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 131
=== ep: 132, time 26.2195143699646, eps 0.0022229708657555565, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 132
=== ep: 133, time 27.08134698867798, eps 0.0021633258728137976, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 133
=== ep: 134, time 27.382996320724487, eps 0.0021065898005034594, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 134
=== ep: 135, time 27.3139328956604, eps 0.002052620779091266, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 135
=== ep: 136, time 28.544681787490845, eps 0.0020012838579124784, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 136
=== ep: 137, time 29.03225588798523, eps 0.0019524506679239415, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 137
=== ep: 138, time 27.591144323349, eps 0.001905999100714611, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 138
=== ep: 139, time 59.51834273338318, eps 0.001861813003170924, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 139
goal_identified
=== ep: 140, time 26.756078004837036, eps 0.0018197818870335101, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 140
=== ep: 141, time 26.8775851726532, eps 0.0017798006526189953, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 141
=== ep: 142, time 27.00895404815674, eps 0.0017417693260160481, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 142
=== ep: 143, time 26.689096450805664, eps 0.0017055928090985275, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 143
=== ep: 144, time 26.979745149612427, eps 0.0016711806417306348, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 144
=== ep: 145, time 33.82604360580444, eps 0.0016384467755694515, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 145
=== ep: 146, time 26.746826887130737, eps 0.0016073093588992661, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 146
=== ep: 147, time 28.12372922897339, eps 0.0015776905319596466, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 147
=== ep: 148, time 28.129322290420532, eps 0.0015495162322554856, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 148
=== ep: 149, time 61.97802400588989, eps 0.0015227160093621863, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 149
=== ep: 150, time 27.063924312591553, eps 0.0014972228487629025, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 150
=== ep: 151, time 27.092262268066406, eps 0.0014729730042773413, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 151
=== ep: 152, time 26.525193214416504, eps 0.001449905838663109, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 152
=== ep: 153, time 37.10735201835632, eps 0.00142796367199102, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 153
=== ep: 154, time 26.973926305770874, eps 0.0014070916374152305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 154
=== ep: 155, time 36.327609062194824, eps 0.001387237543977543, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 155
=== ep: 156, time 27.128647327423096, eps 0.0013683517461028282, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 156
goal_identified
=== ep: 157, time 28.586613655090332, eps 0.0013503870194592265, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 74
=== ep: 158, time 27.655843257904053, eps 0.0013332984428727204, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 158
=== ep: 159, time 73.95513653755188, eps 0.001317043286000802, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 159
goal_identified
=== ep: 160, time 26.401344060897827, eps 0.0013015809024843582, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 160
=== ep: 161, time 26.95830273628235, eps 0.0012868726283106018, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 161
=== ep: 162, time 27.269501447677612, eps 0.0012728816851329014, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 162
=== ep: 163, time 26.74087691307068, eps 0.0012595730883057546, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 163
=== ep: 164, time 27.31796646118164, eps 0.001246913559404956, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 164
=== ep: 165, time 27.107742309570312, eps 0.0012348714430141991, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 165
=== ep: 166, time 28.973283529281616, eps 0.0012234166275700486, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 166
goal_identified
=== ep: 167, time 27.50164818763733, eps 0.001212520470067348, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 167
=== ep: 168, time 26.344146490097046, eps 0.0012021557244367845, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 168
=== ep: 169, time 71.30602383613586, eps 0.0011922964734155277, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 169
=== ep: 170, time 35.28929376602173, eps 0.001182918063740569, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 170
=== ep: 171, time 24.818375825881958, eps 0.0011739970445027263, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 171
=== ep: 172, time 35.2928466796875, eps 0.0011655111085071537, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 172
=== ep: 173, time 25.213003873825073, eps 0.001157439036493735, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 173
=== ep: 174, time 26.810449600219727, eps 0.0011497606440778825, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 174
=== ep: 175, time 26.62519097328186, eps 0.0011424567312790603, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 175
=== ep: 176, time 30.70785403251648, eps 0.0011355090345108335, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 176
goal_identified
=== ep: 177, time 25.705824375152588, eps 0.0011289001809123877, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 109
=== ep: 178, time 28.17806911468506, eps 0.0011226136449073282, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 178
=== ep: 179, time 58.88025879859924, eps 0.001116633706881133, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 179
=== ep: 180, time 26.196594715118408, eps 0.001110945413873925, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 180
=== ep: 181, time 26.11198401451111, eps 0.001105534542190287, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 181
=== ep: 182, time 25.548030376434326, eps 0.0011003875618326132, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 182
=== ep: 183, time 25.858802556991577, eps 0.0010954916026690664, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 183
=== ep: 184, time 25.76948571205139, eps 0.001090834422251547, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 184
=== ep: 185, time 25.351438522338867, eps 0.0010864043752031938, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 185
=== ep: 186, time 25.795887231826782, eps 0.0010821903840988777, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 186
=== ep: 187, time 26.03536105155945, eps 0.0010781819117658682, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 187
=== ep: 188, time 25.599637269973755, eps 0.0010743689349354123, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 188
=== ep: 189, time 75.37605786323547, eps 0.0010707419191793434, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 189
=== ep: 190, time 38.27969479560852, eps 0.0010672917950690429, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 190
=== ep: 191, time 25.353283405303955, eps 0.0010640099354971456, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 191
=== ep: 192, time 25.81042456626892, eps 0.0010608881341052777, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 192
goal_identified
=== ep: 193, time 26.036598205566406, eps 0.0010579185847638855, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 157
=== ep: 194, time 27.915944576263428, eps 0.0010550938620528466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 194
=== ep: 195, time 33.5385468006134, eps 0.001052406902694051, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 195
=== ep: 196, time 26.225224018096924, eps 0.001049850987889527, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 196
=== ep: 197, time 25.027201414108276, eps 0.0010474197265209469, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 197
=== ep: 198, time 26.807613849639893, eps 0.0010451070391685015, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 198
=== ep: 199, time 63.098209857940674, eps 0.001042907142909185, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 199
=== ep: 200, time 28.249799966812134, eps 0.001040814536856474, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 200
=== ep: 201, time 28.73491597175598, eps 0.0010388239884052469, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 201
=== ep: 202, time 26.48894715309143, eps 0.0010369305201475454, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 202
=== ep: 203, time 24.67954969406128, eps 0.0010351293974264616, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 203
=== ep: 204, time 26.513545989990234, eps 0.00103341611649703, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 204
=== ep: 205, time 34.81299543380737, eps 0.0010317863932645186, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 205
=== ep: 206, time 25.982256174087524, eps 0.0010302361525719613, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 206
=== ep: 207, time 26.007566690444946, eps 0.0010287615180101426, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 207
=== ep: 208, time 26.380165338516235, eps 0.001027358802224555, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 208
=== ep: 209, time 80.43195629119873, eps 0.0010260244976950921, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 209
=== ep: 210, time 31.473464488983154, eps 0.0010247552679654227, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 210
goal_identified
=== ep: 211, time 28.67050051689148, eps 0.00102354793930011, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 177
=== ep: 212, time 25.724069118499756, eps 0.0010223994927486214, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 212
=== ep: 213, time 26.918396949768066, eps 0.001021307056596379, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 213
=== ep: 214, time 26.25838565826416, eps 0.0010202678991839778, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 214
=== ep: 215, time 27.08971905708313, eps 0.0010192794220766138, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 215
=== ep: 216, time 26.492297887802124, eps 0.0010183391535666436, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 216
=== ep: 217, time 26.567206859588623, eps 0.0010174447424930286, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 217
=== ep: 218, time 26.146353244781494, eps 0.0010165939523622068, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 218
=== ep: 219, time 66.36867070198059, eps 0.0010157846557556941, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 219
=== ep: 220, time 27.429117441177368, eps 0.001015014829010431, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 220
=== ep: 221, time 36.46111607551575, eps 0.0010142825471585687, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 221
=== ep: 222, time 26.15818691253662, eps 0.0010135859791140496, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 222
=== ep: 223, time 28.189449548721313, eps 0.0010129233830939361, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 223
=== ep: 224, time 33.47533464431763, eps 0.0010122931022630473, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 224
=== ep: 225, time 26.59285044670105, eps 0.001011693560591007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 225
=== ep: 226, time 26.453550577163696, eps 0.0010111232589113477, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 226
=== ep: 227, time 26.248873233795166, eps 0.0010105807711728136, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 227
=== ep: 228, time 25.993857860565186, eps 0.0010100647408734893, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 228
=== ep: 229, time 67.64556670188904, eps 0.001009573877668838, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 229
=== ep: 230, time 29.01396155357361, eps 0.001009106954145169, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 230
=== ep: 231, time 26.7989182472229, eps 0.0010086628027504636, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 231
=== ep: 232, time 26.9301016330719, eps 0.0010082403128748867, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 232
=== ep: 233, time 27.009616136550903, eps 0.0010078384280736842, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 233
=== ep: 234, time 25.758355379104614, eps 0.001007456143425521, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 234
=== ep: 235, time 26.44149088859558, eps 0.001007092503019653, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 235
=== ep: 236, time 27.101977586746216, eps 0.001006746597565654, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 236
=== ep: 237, time 25.355881214141846, eps 0.001006417562119715, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 237
=== ep: 238, time 26.08379602432251, eps 0.0010061045739218342, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 238
=== ep: 239, time 71.8596682548523, eps 0.0010058068503384884, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 239
=== ep: 240, time 27.90074372291565, eps 0.001005523646905642, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 240
=== ep: 241, time 31.55561923980713, eps 0.001005254255467199, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 241
=== ep: 242, time 31.138038158416748, eps 0.0010049980024042435, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 242
=== ep: 243, time 25.836091995239258, eps 0.0010047542469506416, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 243
goal_identified
=== ep: 244, time 25.828020811080933, eps 0.0010045223795907931, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 193
=== ep: 245, time 26.733363151550293, eps 0.001004301820535524, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 245
goal_identified
=== ep: 246, time 26.43456721305847, eps 0.0010040920182723119, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 246
=== ep: 247, time 26.314847469329834, eps 0.0010038924481862177, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 247
=== ep: 248, time 26.29839038848877, eps 0.0010037026112480747, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 248
=== ep: 249, time 60.48617887496948, eps 0.0010035220327666559, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 249
=== ep: 250, time 26.43846297264099, eps 0.0010033502612016988, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 250
=== ep: 251, time 27.552846670150757, eps 0.001003186867034819, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 251
=== ep: 252, time 27.230735540390015, eps 0.001003031441695491, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 252
=== ep: 253, time 26.764516592025757, eps 0.0010028835965394094, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 253
goal_identified
=== ep: 254, time 25.780161142349243, eps 0.0010027429618766747, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 254
=== ep: 255, time 25.98820447921753, eps 0.0010026091860473767, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 255
=== ep: 256, time 23.9381320476532, eps 0.0010024819345422614, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 256
=== ep: 257, time 26.43239140510559, eps 0.0010023608891662839, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 257
=== ep: 258, time 26.00582480430603, eps 0.001002245747242954, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 258
=== ep: 259, time 63.843974113464355, eps 0.0010021362208574892, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 259
=== ep: 260, time 26.45229935646057, eps 0.001002032036136876, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 260
=== ep: 261, time 26.56886053085327, eps 0.0010019329325650452, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 261
=== ep: 262, time 26.205431699752808, eps 0.0010018386623314465, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 262
=== ep: 263, time 27.796603679656982, eps 0.0010017489897113931, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 263
=== ep: 264, time 27.27521800994873, eps 0.0010016636904766263, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 264
=== ep: 265, time 25.871785879135132, eps 0.0010015825513346283, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 265
=== ep: 266, time 29.75313115119934, eps 0.0010015053693952815, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 266
=== ep: 267, time 28.68417191505432, eps 0.0010014319516635345, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 267
=== ep: 268, time 34.58465337753296, eps 0.0010013621145568167, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 268
=== ep: 269, time 69.0058000087738, eps 0.0010012956834459848, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 269
=== ep: 270, time 26.467658758163452, eps 0.0010012324922186594, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 270
goal_identified
=== ep: 271, time 26.85053253173828, eps 0.001001172382863857, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 211
=== ep: 272, time 26.018773794174194, eps 0.0010011152050768812, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 272
=== ep: 273, time 27.607916831970215, eps 0.0010010608158834819, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 273
=== ep: 274, time 27.56251311302185, eps 0.0010010090792823456, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 274
=== ep: 275, time 37.20103669166565, eps 0.0010009598659050213, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 275
=== ep: 276, time 25.396904230117798, eps 0.0010009130526924313, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 276
=== ep: 277, time 25.756295680999756, eps 0.0010008685225871602, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 277
=== ep: 278, time 27.30629253387451, eps 0.0010008261642407504, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 278
=== ep: 279, time 62.03658151626587, eps 0.001000785871735272, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 279
=== ep: 280, time 26.786146640777588, eps 0.0010007475443184742, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 280
=== ep: 281, time 25.674920082092285, eps 0.001000711086151851, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 281
=== ep: 282, time 25.759966373443604, eps 0.0010006764060709957, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 282
=== ep: 283, time 26.659952402114868, eps 0.001000643417357642, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 283
=== ep: 284, time 26.76320195198059, eps 0.0010006120375228235, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 284
=== ep: 285, time 29.196715354919434, eps 0.0010005821881006083, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 285
=== ep: 286, time 28.524703979492188, eps 0.0010005537944518927, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 286
=== ep: 287, time 26.24476933479309, eps 0.0010005267855777657, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 287
goal_identified
=== ep: 288, time 26.751299142837524, eps 0.0010005010939419733, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 244
=== ep: 289, time 59.39719533920288, eps 0.001000476655302044, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 289
=== ep: 290, time 25.979395627975464, eps 0.0010004534085486486, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 290
=== ep: 291, time 26.27171778678894, eps 0.0010004312955527947, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 291
=== ep: 292, time 25.6961452960968, eps 0.0010004102610204745, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 292
=== ep: 293, time 25.87762451171875, eps 0.0010003902523544011, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 293
goal_identified
=== ep: 294, time 26.59460759162903, eps 0.0010003712195224871, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 294
goal_identified
=== ep: 295, time 26.386707544326782, eps 0.0010003531149327387, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 271
goal_identified
=== ep: 296, time 26.507959604263306, eps 0.0010003358933142518, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 296
=== ep: 297, time 26.725518703460693, eps 0.0010003195116040093, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 297
=== ep: 298, time 26.561419010162354, eps 0.0010003039288392032, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 298
=== ep: 299, time 55.244306325912476, eps 0.0010002891060548044, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 299
=== ep: 300, time 25.945406198501587, eps 0.0010002750061861312, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 300
=== ep: 301, time 25.759053707122803, eps 0.0010002615939761676, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 301
=== ep: 302, time 27.120182991027832, eps 0.001000248835887403, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 302
=== ep: 303, time 31.618022680282593, eps 0.0010002367000179694, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 303
goal_identified
=== ep: 304, time 26.356261253356934, eps 0.0010002251560218723, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 288
=== ep: 305, time 25.608814001083374, eps 0.0010002141750331084, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 305
=== ep: 306, time 28.47944450378418, eps 0.0010002037295934862, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 306
goal_identified
=== ep: 307, time 26.863113164901733, eps 0.0010001937935839656, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 295
=== ep: 308, time 26.462779760360718, eps 0.0010001843421593476, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 308
=== ep: 309, time 60.391196727752686, eps 0.0010001753516861473, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 309
=== ep: 310, time 26.94773507118225, eps 0.0010001667996834991, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 310
=== ep: 311, time 25.844195127487183, eps 0.001000158664766942, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 311
=== ep: 312, time 27.070984840393066, eps 0.0010001509265949466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 312
=== ep: 313, time 26.02974843978882, eps 0.001000143565818053, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 313
=== ep: 314, time 32.93034911155701, eps 0.0010001365640304844, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 314
=== ep: 315, time 28.635084867477417, eps 0.0010001299037241253, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 315
=== ep: 316, time 30.757859706878662, eps 0.0010001235682447402, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 316
=== ep: 317, time 28.53093695640564, eps 0.0010001175417503308, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 317
=== ep: 318, time 28.43814969062805, eps 0.0010001118091715218, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 318
=== ep: 319, time 50.17510652542114, eps 0.0010001063561738807, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 319
=== ep: 320, time 26.65148425102234, eps 0.0010001011691220727, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 320
=== ep: 321, time 29.52141046524048, eps 0.0010000962350457665, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 321
=== ep: 322, time 27.200140237808228, eps 0.0010000915416072012, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 322
=== ep: 323, time 25.904789924621582, eps 0.0010000870770703358, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 323
=== ep: 324, time 26.207184553146362, eps 0.0010000828302715028, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 324
=== ep: 325, time 25.983883380889893, eps 0.0010000787905914928, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 325
goal_identified
=== ep: 326, time 26.400089263916016, eps 0.0010000749479290019, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 326
=== ep: 327, time 26.064576625823975, eps 0.001000071292675372, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 327
goal_identified
goal_identified
=== ep: 328, time 26.446978330612183, eps 0.001000067815690565, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 304
=== ep: 329, time 68.8133852481842, eps 0.0010000645082803084, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 329
=== ep: 330, time 25.566598176956177, eps 0.0010000613621743532, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 330
=== ep: 331, time 26.718889713287354, eps 0.0010000583695057963, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 331
=== ep: 332, time 25.893994092941284, eps 0.0010000555227914069, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 332
goal_identified
=== ep: 333, time 26.74099111557007, eps 0.0010000528149129166, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 333
goal_identified
=== ep: 334, time 27.309528350830078, eps 0.0010000502390992187, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 307
=== ep: 335, time 28.91893243789673, eps 0.0010000477889094373, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 335
=== ep: 336, time 25.304779052734375, eps 0.0010000454582168217, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 336
=== ep: 337, time 37.037338733673096, eps 0.001000043241193426, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 337
=== ep: 338, time 34.66036081314087, eps 0.0010000411322955373, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 338
=== ep: 339, time 71.28401923179626, eps 0.0010000391262498123, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 339
=== ep: 340, time 28.67960786819458, eps 0.001000037218040092, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 340
=== ep: 341, time 26.627487182617188, eps 0.0010000354028948577, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 341
=== ep: 342, time 27.504968643188477, eps 0.0010000336762753012, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 342
goal_identified
=== ep: 343, time 26.05685257911682, eps 0.001000032033863974, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 343
=== ep: 344, time 28.478811264038086, eps 0.0010000304715539925, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 344
=== ep: 345, time 26.34939670562744, eps 0.001000028985438768, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 345
=== ep: 346, time 32.74675703048706, eps 0.001000027571802238, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 346
=== ep: 347, time 25.898399353027344, eps 0.0010000262271095755, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 347
=== ep: 348, time 26.141087770462036, eps 0.0010000249479983478, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 348
goal_identified
=== ep: 349, time 58.915541648864746, eps 0.0010000237312701107, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 334
=== ep: 350, time 27.295660495758057, eps 0.00100002257388241, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 350
=== ep: 351, time 26.2460675239563, eps 0.0010000214729411737, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 351
=== ep: 352, time 25.03899645805359, eps 0.0010000204256934752, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 352
=== ep: 353, time 26.218810081481934, eps 0.0010000194295206493, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 353
=== ep: 354, time 25.001340627670288, eps 0.0010000184819317455, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 354
=== ep: 355, time 23.952903985977173, eps 0.001000017580557298, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 355
=== ep: 356, time 27.042086839675903, eps 0.001000016723143401, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 356
=== ep: 357, time 26.186847686767578, eps 0.0010000159075460732, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 357
=== ep: 358, time 27.082395315170288, eps 0.0010000151317258964, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 358
=== ep: 359, time 48.136428356170654, eps 0.0010000143937429161, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 359
=== ep: 360, time 27.556649446487427, eps 0.0010000136917517905, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 360
goal_identified
goal_identified
=== ep: 361, time 26.90070676803589, eps 0.001000013023997176, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 349
=== ep: 362, time 27.122507095336914, eps 0.0010000123888093385, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 362
=== ep: 363, time 26.70185923576355, eps 0.0010000117845999773, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 363
=== ep: 364, time 26.456848859786987, eps 0.0010000112098582543, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 364
=== ep: 365, time 27.303277730941772, eps 0.001000010663147016, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 365
=== ep: 366, time 36.02422475814819, eps 0.0010000101430991996, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 366
=== ep: 367, time 27.77824902534485, eps 0.0010000096484144142, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 367
=== ep: 368, time 25.789488077163696, eps 0.0010000091778556905, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 368
=== ep: 369, time 63.81126546859741, eps 0.0010000087302463867, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 369
=== ep: 370, time 27.134481191635132, eps 0.001000008304467246, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 370
=== ep: 371, time 26.766027450561523, eps 0.0010000078994535993, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 371
=== ep: 372, time 27.355259656906128, eps 0.0010000075141927012, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 372
=== ep: 373, time 27.659761428833008, eps 0.0010000071477211988, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 373
=== ep: 374, time 26.77284073829651, eps 0.0010000067991227223, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 374
=== ep: 375, time 26.98063015937805, eps 0.0010000064675255943, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 375
=== ep: 376, time 26.271562099456787, eps 0.001000006152100649, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 376
=== ep: 377, time 26.89868402481079, eps 0.0010000058520591598, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 377
=== ep: 378, time 26.789644956588745, eps 0.0010000055666508666, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 378
=== ep: 379, time 49.40717554092407, eps 0.0010000052951621003, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 379
goal_identified
=== ep: 380, time 26.546497106552124, eps 0.0010000050369139975, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 361
=== ep: 381, time 26.49899172782898, eps 0.001000004791260803, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 381
=== ep: 382, time 27.92199468612671, eps 0.0010000045575882562, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 382
goal_identified
=== ep: 383, time 29.07542657852173, eps 0.001000004335312054, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 383
goal_identified
=== ep: 384, time 26.27824878692627, eps 0.0010000041238763903, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 384
=== ep: 385, time 37.875481367111206, eps 0.0010000039227525655, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 385
=== ep: 386, time 31.668519735336304, eps 0.0010000037314376652, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 386
=== ep: 387, time 26.86473560333252, eps 0.001000003549453303, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 387
goal_identified
=== ep: 388, time 27.070818185806274, eps 0.0010000033763444226, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 380
=== ep: 389, time 60.58687710762024, eps 0.001000003211678162, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 389
=== ep: 390, time 26.747987031936646, eps 0.0010000030550427698, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 390
goal_identified
=== ep: 391, time 26.348814964294434, eps 0.0010000029060465757, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 391
=== ep: 392, time 30.120161056518555, eps 0.0010000027643170119, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 392
=== ep: 393, time 26.99296236038208, eps 0.0010000026294996803, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 393
=== ep: 394, time 27.87134861946106, eps 0.0010000025012574677, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 394
=== ep: 395, time 27.24948811531067, eps 0.0010000023792697014, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 395
=== ep: 396, time 25.920022010803223, eps 0.0010000022632313489, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 396
=== ep: 397, time 33.22584080696106, eps 0.0010000021528522535, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 397
=== ep: 398, time 27.094413995742798, eps 0.00100000204785641, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 398
goal_identified
=== ep: 399, time 48.44488024711609, eps 0.0010000019479812744, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 399
=== ep: 400, time 28.24614143371582, eps 0.0010000018529771066, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 400
=== ep: 401, time 29.091515064239502, eps 0.0010000017626063467, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 401
=== ep: 402, time 27.725495100021362, eps 0.0010000016766430208, sum reward: 0, score_diff -6, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 402
=== ep: 403, time 26.83567976951599, eps 0.0010000015948721758, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 403
=== ep: 404, time 27.92685055732727, eps 0.001000001517089342, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 404
=== ep: 405, time 29.035552501678467, eps 0.0010000014431000217, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 405
=== ep: 406, time 26.84900951385498, eps 0.001000001372719203, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 406
=== ep: 407, time 27.4555766582489, eps 0.0010000013057708975, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 407
goal_identified
=== ep: 408, time 25.965883493423462, eps 0.0010000012420876994, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 408
=== ep: 409, time 69.36975860595703, eps 0.0010000011815103674, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 409
=== ep: 410, time 26.830992221832275, eps 0.001000001123887427, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 410
=== ep: 411, time 26.866100549697876, eps 0.0010000010690747903, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 411
=== ep: 412, time 26.661237478256226, eps 0.0010000010169353975, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 412
=== ep: 413, time 39.09670162200928, eps 0.0010000009673388729, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 413
=== ep: 414, time 28.09586501121521, eps 0.0010000009201611994, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 414
=== ep: 415, time 27.741233348846436, eps 0.0010000008752844081, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 415
=== ep: 416, time 25.736021757125854, eps 0.0010000008325962838, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 416
=== ep: 417, time 26.666966915130615, eps 0.001000000791990084, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 417
goal_identified
=== ep: 418, time 26.472103595733643, eps 0.0010000007533642718, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 418
=== ep: 419, time 52.734891414642334, eps 0.0010000007166222626, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 419
=== ep: 420, time 26.2580246925354, eps 0.0010000006816721825, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 420
=== ep: 421, time 26.265991687774658, eps 0.001000000648426638, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 421
=== ep: 422, time 27.519529819488525, eps 0.0010000006168024976, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 422
=== ep: 423, time 26.73129916191101, eps 0.0010000005867206849, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 423
=== ep: 424, time 27.85555934906006, eps 0.0010000005581059794, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 424
=== ep: 425, time 31.412721157073975, eps 0.0010000005308868295, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 425
=== ep: 426, time 28.9441499710083, eps 0.0010000005049951733, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 426
=== ep: 427, time 27.088513135910034, eps 0.001000000480366268, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 427
=== ep: 428, time 26.86439299583435, eps 0.0010000004569385287, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 428
=== ep: 429, time 59.68047785758972, eps 0.0010000004346533736, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 429
=== ep: 430, time 27.048874616622925, eps 0.0010000004134550786, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 430
goal_identified
=== ep: 431, time 26.53874635696411, eps 0.0010000003932906364, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 388
=== ep: 432, time 26.552112579345703, eps 0.0010000003741096257, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 432
goal_identified
=== ep: 433, time 26.834762811660767, eps 0.001000000355864084, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 431
=== ep: 434, time 26.373074531555176, eps 0.0010000003385083878, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 434
goal_identified
=== ep: 435, time 28.698700189590454, eps 0.001000000321999139, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 433
=== ep: 436, time 28.37068819999695, eps 0.0010000003062950555, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 436
=== ep: 437, time 24.213807821273804, eps 0.0010000002913568694, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 437
=== ep: 438, time 26.92410445213318, eps 0.0010000002771472273, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 438
=== ep: 439, time 63.90928792953491, eps 0.0010000002636305976, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 439
=== ep: 440, time 26.421008110046387, eps 0.0010000002507731815, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 440
=== ep: 441, time 36.584803104400635, eps 0.0010000002385428292, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 441
=== ep: 442, time 28.895895957946777, eps 0.0010000002269089582, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 442
=== ep: 443, time 31.915874481201172, eps 0.0010000002158424776, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 443
=== ep: 444, time 29.70278573036194, eps 0.0010000002053157158, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 444
=== ep: 445, time 27.967236757278442, eps 0.0010000001953023503, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 445
goal_identified
=== ep: 446, time 29.15010356903076, eps 0.001000000185777342, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 435
=== ep: 447, time 30.748950481414795, eps 0.0010000001767168742, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 447
=== ep: 448, time 25.397271633148193, eps 0.0010000001680982905, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 448
=== ep: 449, time 64.19039869308472, eps 0.0010000001599000403, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 449
goal_identified
=== ep: 450, time 26.812612771987915, eps 0.0010000001521016232, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 446
=== ep: 451, time 27.101438760757446, eps 0.0010000001446835395, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 451
=== ep: 452, time 26.128541231155396, eps 0.0010000001376272401, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 452
=== ep: 453, time 25.67634892463684, eps 0.0010000001309150804, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 453
=== ep: 454, time 27.002657651901245, eps 0.0010000001245302765, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 454
=== ep: 455, time 27.454375743865967, eps 0.0010000001184568633, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 455
=== ep: 456, time 26.882646560668945, eps 0.0010000001126796538, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 456
=== ep: 457, time 26.630526304244995, eps 0.0010000001071842023, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 457
=== ep: 458, time 28.533610343933105, eps 0.001000000101956767, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 458
=== ep: 459, time 70.92466330528259, eps 0.001000000096984277, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 459
=== ep: 460, time 35.53401017189026, eps 0.001000000092254298, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 460
=== ep: 461, time 26.58781361579895, eps 0.0010000000877550027, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 461
=== ep: 462, time 27.067897081375122, eps 0.0010000000834751407, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 462
=== ep: 463, time 26.979358911514282, eps 0.00100000007940401, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 463
=== ep: 464, time 25.021862983703613, eps 0.0010000000755314307, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 464
=== ep: 465, time 29.31343698501587, eps 0.0010000000718477194, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 465
=== ep: 466, time 28.43997812271118, eps 0.0010000000683436647, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 466
=== ep: 467, time 27.071423292160034, eps 0.001000000065010505, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 467
=== ep: 468, time 29.226998805999756, eps 0.0010000000618399052, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 468
=== ep: 469, time 61.40370965003967, eps 0.0010000000588239375, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 469
=== ep: 470, time 26.8029363155365, eps 0.0010000000559550603, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 470
=== ep: 471, time 33.392659187316895, eps 0.0010000000532260998, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 471
=== ep: 472, time 25.503856897354126, eps 0.0010000000506302322, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 472
=== ep: 473, time 32.70391130447388, eps 0.0010000000481609666, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 473
=== ep: 474, time 27.11347270011902, eps 0.0010000000458121286, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 474
=== ep: 475, time 27.256827116012573, eps 0.0010000000435778447, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 475
=== ep: 476, time 26.638171672821045, eps 0.001000000041452528, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 476
=== ep: 477, time 27.825634479522705, eps 0.0010000000394308644, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 477
=== ep: 478, time 29.168980836868286, eps 0.0010000000375077985, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 478
=== ep: 479, time 61.77462124824524, eps 0.0010000000356785216, sum reward: 0, score_diff -5, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 479
=== ep: 480, time 33.813342332839966, eps 0.0010000000339384595, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 480
=== ep: 481, time 26.54415988922119, eps 0.0010000000322832614, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 481
=== ep: 482, time 27.536354064941406, eps 0.0010000000307087882, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 482
=== ep: 483, time 27.63862705230713, eps 0.001000000029211103, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 483
=== ep: 484, time 26.292327642440796, eps 0.0010000000277864607, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 484
=== ep: 485, time 26.4336359500885, eps 0.0010000000264312988, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 485
=== ep: 486, time 31.003401517868042, eps 0.0010000000251422292, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 486
=== ep: 487, time 30.19593048095703, eps 0.0010000000239160282, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 487
=== ep: 488, time 28.09805393218994, eps 0.00100000002274963, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 488
=== ep: 489, time 72.00426006317139, eps 0.0010000000216401172, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 489
=== ep: 490, time 27.757709741592407, eps 0.0010000000205847162, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 490
=== ep: 491, time 32.5324273109436, eps 0.0010000000195807877, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 491
=== ep: 492, time 28.302078247070312, eps 0.0010000000186258216, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 492
=== ep: 493, time 27.725445985794067, eps 0.0010000000177174295, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 493
=== ep: 494, time 32.49551725387573, eps 0.0010000000168533404, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 494
=== ep: 495, time 27.566108226776123, eps 0.0010000000160313932, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 495
=== ep: 496, time 26.827131271362305, eps 0.001000000015249533, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 496
=== ep: 497, time 26.991193056106567, eps 0.0010000000145058043, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 497
=== ep: 498, time 26.73540759086609, eps 0.001000000013798348, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 498
=== ep: 499, time 83.33156371116638, eps 0.0010000000131253947, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 499
=== ep: 500, time 33.290327310562134, eps 0.0010000000124852615, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 500
=== ep: 501, time 28.462050676345825, eps 0.0010000000118763482, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 501
=== ep: 502, time 27.33567786216736, eps 0.0010000000112971319, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 502
=== ep: 503, time 26.85353660583496, eps 0.0010000000107461642, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 503
=== ep: 504, time 26.61849880218506, eps 0.0010000000102220676, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 504
=== ep: 505, time 31.984339237213135, eps 0.0010000000097235315, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 505
=== ep: 506, time 27.563401222229004, eps 0.0010000000092493092, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 506
=== ep: 507, time 31.939247846603394, eps 0.0010000000087982152, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 507
=== ep: 508, time 29.146694660186768, eps 0.0010000000083691212, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 508
=== ep: 509, time 80.75468230247498, eps 0.0010000000079609542, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 509
=== ep: 510, time 26.720736503601074, eps 0.001000000007572694, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 510
=== ep: 511, time 27.278286695480347, eps 0.0010000000072033692, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 511
=== ep: 512, time 26.9047908782959, eps 0.001000000006852057, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 512
=== ep: 513, time 28.183165788650513, eps 0.001000000006517878, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 513
=== ep: 514, time 26.972140073776245, eps 0.0010000000061999974, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 514
=== ep: 515, time 26.715432167053223, eps 0.0010000000058976199, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 515
=== ep: 516, time 27.293156147003174, eps 0.0010000000056099897, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 516
=== ep: 517, time 27.265145778656006, eps 0.0010000000053363872, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 517
=== ep: 518, time 26.81670117378235, eps 0.0010000000050761286, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 518
=== ep: 519, time 72.37170243263245, eps 0.001000000004828563, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 519
=== ep: 520, time 28.335927963256836, eps 0.001000000004593071, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 520
=== ep: 521, time 28.85746669769287, eps 0.0010000000043690644, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 521
=== ep: 522, time 27.760050058364868, eps 0.0010000000041559827, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 522
goal_identified
=== ep: 523, time 27.609617710113525, eps 0.0010000000039532928, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 523
=== ep: 524, time 29.201964855194092, eps 0.0010000000037604885, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 524
=== ep: 525, time 26.153025150299072, eps 0.0010000000035770874, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 525
=== ep: 526, time 25.62391471862793, eps 0.0010000000034026306, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 526
=== ep: 527, time 27.819316148757935, eps 0.0010000000032366824, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 527
=== ep: 528, time 30.931652784347534, eps 0.0010000000030788276, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 528
=== ep: 529, time 65.99582290649414, eps 0.0010000000029286714, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 529
=== ep: 530, time 27.856700658798218, eps 0.0010000000027858384, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 530
=== ep: 531, time 29.39480710029602, eps 0.0010000000026499714, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 531
=== ep: 532, time 29.07509136199951, eps 0.0010000000025207308, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 532
goal_identified
=== ep: 533, time 26.878687620162964, eps 0.0010000000023977934, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 450
=== ep: 534, time 37.87302613258362, eps 0.0010000000022808515, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 534
=== ep: 535, time 35.74720883369446, eps 0.0010000000021696133, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 535
goal_identified
=== ep: 536, time 27.377406358718872, eps 0.0010000000020637999, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 536
=== ep: 537, time 28.032971382141113, eps 0.0010000000019631471, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 537
=== ep: 538, time 26.342777013778687, eps 0.0010000000018674034, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 538
=== ep: 539, time 65.60734510421753, eps 0.001000000001776329, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 539
=== ep: 540, time 28.353459358215332, eps 0.0010000000016896964, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 540
=== ep: 541, time 27.94658660888672, eps 0.001000000001607289, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 541
=== ep: 542, time 28.142769813537598, eps 0.0010000000015289005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 542
=== ep: 543, time 28.71395254135132, eps 0.0010000000014543352, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 543
=== ep: 544, time 26.35875654220581, eps 0.0010000000013834064, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 544
=== ep: 545, time 27.320444107055664, eps 0.001000000001315937, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 545
=== ep: 546, time 26.73829483985901, eps 0.0010000000012517578, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 546
=== ep: 547, time 28.131388187408447, eps 0.001000000001190709, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 547
=== ep: 548, time 33.34976530075073, eps 0.0010000000011326374, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 548
=== ep: 549, time 52.12803673744202, eps 0.001000000001077398, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 549
=== ep: 550, time 27.53553557395935, eps 0.0010000000010248527, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 550
=== ep: 551, time 26.75647521018982, eps 0.00100000000097487, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 551
=== ep: 552, time 28.394885540008545, eps 0.001000000000927325, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 552
=== ep: 553, time 28.495094299316406, eps 0.0010000000008820989, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 553
=== ep: 554, time 30.737889051437378, eps 0.0010000000008390784, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 554
=== ep: 555, time 26.98051404953003, eps 0.001000000000798156, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 555
=== ep: 556, time 27.72631287574768, eps 0.0010000000007592295, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 556
=== ep: 557, time 28.246634483337402, eps 0.0010000000007222014, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 557
=== ep: 558, time 29.13877844810486, eps 0.0010000000006869794, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 558
goal_identified
=== ep: 559, time 65.97112083435059, eps 0.001000000000653475, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 533
=== ep: 560, time 27.754940032958984, eps 0.0010000000006216046, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 560
=== ep: 561, time 26.73709988594055, eps 0.0010000000005912885, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 561
=== ep: 562, time 27.847883939743042, eps 0.0010000000005624511, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 562
=== ep: 563, time 29.04107117652893, eps 0.00100000000053502, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 563
=== ep: 564, time 28.763277053833008, eps 0.001000000000508927, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 564
=== ep: 565, time 29.815511226654053, eps 0.001000000000484106, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 565
=== ep: 566, time 27.94900131225586, eps 0.001000000000460496, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 566
=== ep: 567, time 26.247441291809082, eps 0.0010000000004380374, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 567
=== ep: 568, time 27.265902042388916, eps 0.001000000000416674, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 568
=== ep: 569, time 59.12480044364929, eps 0.0010000000003963527, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 569
=== ep: 570, time 31.10137939453125, eps 0.0010000000003770222, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 570
=== ep: 571, time 27.31146550178528, eps 0.0010000000003586346, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 571
=== ep: 572, time 28.63676881790161, eps 0.0010000000003411438, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 572
=== ep: 573, time 27.451358556747437, eps 0.001000000000324506, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 573
=== ep: 574, time 34.6043906211853, eps 0.0010000000003086798, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 574
=== ep: 575, time 32.721155881881714, eps 0.0010000000002936252, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 575
=== ep: 576, time 27.73469114303589, eps 0.001000000000279305, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 576
=== ep: 577, time 36.77390503883362, eps 0.0010000000002656831, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 577
=== ep: 578, time 28.33257818222046, eps 0.0010000000002527256, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 578
=== ep: 579, time 59.861316204071045, eps 0.0010000000002404, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 579
=== ep: 580, time 27.07864284515381, eps 0.0010000000002286756, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 580
=== ep: 581, time 32.64139175415039, eps 0.0010000000002175229, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 581
=== ep: 582, time 27.62170124053955, eps 0.0010000000002069142, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 582
=== ep: 583, time 28.062551736831665, eps 0.0010000000001968228, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 583
=== ep: 584, time 29.674161911010742, eps 0.0010000000001872237, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 584
=== ep: 585, time 29.011387825012207, eps 0.0010000000001780928, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 585
=== ep: 586, time 28.618457078933716, eps 0.001000000000169407, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 586
=== ep: 587, time 29.013529777526855, eps 0.001000000000161145, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 587
=== ep: 588, time 26.88962173461914, eps 0.0010000000001532858, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 588
goal_identified
=== ep: 589, time 64.0538821220398, eps 0.00100000000014581, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 559
=== ep: 590, time 34.6304931640625, eps 0.0010000000001386988, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 590
=== ep: 591, time 27.10552406311035, eps 0.0010000000001319344, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 591
=== ep: 592, time 27.895454168319702, eps 0.0010000000001255, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 592
=== ep: 593, time 27.08827304840088, eps 0.0010000000001193791, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 593
=== ep: 594, time 29.120408058166504, eps 0.001000000000113557, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 594
=== ep: 595, time 27.845556497573853, eps 0.0010000000001080186, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 595
=== ep: 596, time 29.350998401641846, eps 0.0010000000001027505, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 596
=== ep: 597, time 29.62129807472229, eps 0.0010000000000977393, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 597
=== ep: 598, time 27.07466149330139, eps 0.0010000000000929725, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 598
=== ep: 599, time 61.80845403671265, eps 0.0010000000000884382, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 599
=== ep: 600, time 33.70345950126648, eps 0.001000000000084125, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 600
=== ep: 601, time 36.741822957992554, eps 0.0010000000000800222, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 601
goal_identified
=== ep: 602, time 28.57764482498169, eps 0.0010000000000761195, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 589
=== ep: 603, time 31.94709348678589, eps 0.0010000000000724072, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 603
=== ep: 604, time 27.10820508003235, eps 0.0010000000000688757, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 604
=== ep: 605, time 29.233850240707397, eps 0.0010000000000655166, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 605
=== ep: 606, time 35.84253931045532, eps 0.0010000000000623215, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 606
=== ep: 607, time 28.202764987945557, eps 0.001000000000059282, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 607
=== ep: 608, time 29.126121044158936, eps 0.0010000000000563907, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 608
=== ep: 609, time 87.75730347633362, eps 0.0010000000000536405, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 609
=== ep: 610, time 33.15773344039917, eps 0.0010000000000510245, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 610
=== ep: 611, time 26.890930652618408, eps 0.0010000000000485358, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 611
=== ep: 612, time 28.335566997528076, eps 0.0010000000000461688, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 612
=== ep: 613, time 27.843839406967163, eps 0.0010000000000439171, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 613
=== ep: 614, time 29.740034580230713, eps 0.0010000000000417752, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 614
=== ep: 615, time 27.11497163772583, eps 0.0010000000000397378, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 615
=== ep: 616, time 32.7604124546051, eps 0.0010000000000377999, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 616
goal_identified
=== ep: 617, time 28.40796399116516, eps 0.0010000000000359563, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 617
=== ep: 618, time 28.41148066520691, eps 0.0010000000000342027, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 618
=== ep: 619, time 70.87779712677002, eps 0.0010000000000325345, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 619
=== ep: 620, time 30.98275899887085, eps 0.001000000000030948, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 620
goal_identified
=== ep: 621, time 26.684830904006958, eps 0.0010000000000294385, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 602
=== ep: 622, time 26.26137113571167, eps 0.0010000000000280028, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 622
=== ep: 623, time 27.37849473953247, eps 0.0010000000000266371, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 623
goal_identified
=== ep: 624, time 27.2501163482666, eps 0.001000000000025338, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 621
=== ep: 625, time 27.91152310371399, eps 0.0010000000000241023, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 625
=== ep: 626, time 27.4099600315094, eps 0.0010000000000229268, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 626
=== ep: 627, time 28.579065084457397, eps 0.0010000000000218085, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 627
=== ep: 628, time 28.917824268341064, eps 0.001000000000020745, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 628
=== ep: 629, time 66.16527795791626, eps 0.0010000000000197332, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 629
=== ep: 630, time 34.22349166870117, eps 0.0010000000000187708, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 630
goal_identified
=== ep: 631, time 27.479103803634644, eps 0.0010000000000178553, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 624
=== ep: 632, time 26.540871381759644, eps 0.0010000000000169845, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 632
=== ep: 633, time 35.657559871673584, eps 0.0010000000000161562, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 633
=== ep: 634, time 27.373451471328735, eps 0.0010000000000153684, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 634
=== ep: 635, time 27.26330256462097, eps 0.0010000000000146188, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 635
=== ep: 636, time 25.270574808120728, eps 0.0010000000000139058, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 636
=== ep: 637, time 28.2977237701416, eps 0.0010000000000132275, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 637
=== ep: 638, time 33.76061272621155, eps 0.0010000000000125824, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 638
=== ep: 639, time 66.56733083724976, eps 0.0010000000000119687, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 639
=== ep: 640, time 27.207924604415894, eps 0.001000000000011385, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 640
=== ep: 641, time 27.44944977760315, eps 0.00100000000001083, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 641
=== ep: 642, time 35.96393871307373, eps 0.0010000000000103017, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 642
=== ep: 643, time 26.875869750976562, eps 0.0010000000000097993, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 643
=== ep: 644, time 28.430318117141724, eps 0.0010000000000093213, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 644
=== ep: 645, time 26.833145141601562, eps 0.0010000000000088666, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 645
=== ep: 646, time 27.18647837638855, eps 0.0010000000000084342, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 646
=== ep: 647, time 27.469406366348267, eps 0.001000000000008023, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 647
goal_identified
=== ep: 648, time 27.555211544036865, eps 0.0010000000000076317, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 648
=== ep: 649, time 76.7314703464508, eps 0.0010000000000072594, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 649
=== ep: 650, time 32.674787282943726, eps 0.0010000000000069055, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 650
=== ep: 651, time 32.04330134391785, eps 0.0010000000000065686, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 651
=== ep: 652, time 26.592766046524048, eps 0.0010000000000062483, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 652
=== ep: 653, time 33.98364043235779, eps 0.0010000000000059436, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 653
=== ep: 654, time 27.968451976776123, eps 0.0010000000000056537, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 654
=== ep: 655, time 30.91952347755432, eps 0.0010000000000053779, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 655
goal_identified
=== ep: 656, time 27.551350593566895, eps 0.0010000000000051157, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 631
=== ep: 657, time 27.422662019729614, eps 0.0010000000000048661, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 657
=== ep: 658, time 27.303874492645264, eps 0.001000000000004629, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 658
=== ep: 659, time 71.42268180847168, eps 0.0010000000000044032, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 659
goal_identified
=== ep: 660, time 32.7300660610199, eps 0.0010000000000041883, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 656
=== ep: 661, time 27.31916308403015, eps 0.001000000000003984, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 661
goal_identified
=== ep: 662, time 29.796336889266968, eps 0.0010000000000037897, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 660
=== ep: 663, time 27.636189699172974, eps 0.001000000000003605, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 663
=== ep: 664, time 25.6544451713562, eps 0.0010000000000034291, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 664
=== ep: 665, time 27.70470094680786, eps 0.001000000000003262, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 665
=== ep: 666, time 28.309725761413574, eps 0.0010000000000031028, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 666
=== ep: 667, time 28.29185652732849, eps 0.0010000000000029514, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 667
=== ep: 668, time 27.64816975593567, eps 0.0010000000000028075, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 668
=== ep: 669, time 64.61187863349915, eps 0.0010000000000026706, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 669
goal_identified
=== ep: 670, time 29.214949131011963, eps 0.0010000000000025403, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 662
goal_identified
=== ep: 671, time 28.702879428863525, eps 0.0010000000000024165, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 671
=== ep: 672, time 28.303069353103638, eps 0.0010000000000022985, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 672
=== ep: 673, time 26.775984048843384, eps 0.0010000000000021864, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 673
=== ep: 674, time 25.51268696784973, eps 0.00100000000000208, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 674
=== ep: 675, time 37.68831276893616, eps 0.0010000000000019785, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 675
=== ep: 676, time 26.851871967315674, eps 0.001000000000001882, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 676
goal_identified
=== ep: 677, time 28.334805250167847, eps 0.0010000000000017903, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 677
=== ep: 678, time 27.40526270866394, eps 0.0010000000000017029, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 678
=== ep: 679, time 64.35504841804504, eps 0.0010000000000016198, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 679
=== ep: 680, time 32.63864850997925, eps 0.0010000000000015409, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 680
=== ep: 681, time 27.86745858192444, eps 0.0010000000000014656, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 681
=== ep: 682, time 34.70211148262024, eps 0.0010000000000013943, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 682
=== ep: 683, time 26.942164421081543, eps 0.0010000000000013262, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 683
=== ep: 684, time 28.000226974487305, eps 0.0010000000000012616, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 684
=== ep: 685, time 27.082545518875122, eps 0.0010000000000012, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 685
=== ep: 686, time 25.909784078598022, eps 0.0010000000000011415, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 686
=== ep: 687, time 28.761900186538696, eps 0.0010000000000010857, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 687
=== ep: 688, time 27.808407068252563, eps 0.0010000000000010328, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 688
=== ep: 689, time 61.86638402938843, eps 0.0010000000000009825, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 689
=== ep: 690, time 31.521308183670044, eps 0.0010000000000009346, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 690
=== ep: 691, time 27.494804859161377, eps 0.001000000000000889, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 691
goal_identified
=== ep: 692, time 28.667286157608032, eps 0.0010000000000008457, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 692
=== ep: 693, time 28.231626510620117, eps 0.0010000000000008045, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 693
=== ep: 694, time 27.921000957489014, eps 0.0010000000000007653, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 694
=== ep: 695, time 27.46825337409973, eps 0.0010000000000007277, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 695
=== ep: 696, time 29.021185398101807, eps 0.0010000000000006924, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 696
=== ep: 697, time 38.952913999557495, eps 0.0010000000000006586, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 697
=== ep: 698, time 28.582501649856567, eps 0.0010000000000006265, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 698
=== ep: 699, time 60.56518912315369, eps 0.001000000000000596, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 699
=== ep: 700, time 27.332391023635864, eps 0.0010000000000005668, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 700
goal_identified
=== ep: 701, time 27.03623104095459, eps 0.0010000000000005393, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 701
=== ep: 702, time 34.535173416137695, eps 0.0010000000000005128, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 702
=== ep: 703, time 27.25516128540039, eps 0.001000000000000488, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 703
=== ep: 704, time 28.11189913749695, eps 0.001000000000000464, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 704
=== ep: 705, time 27.422094345092773, eps 0.0010000000000004415, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 705
=== ep: 706, time 28.32232642173767, eps 0.00100000000000042, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 706
=== ep: 707, time 27.326943397521973, eps 0.0010000000000003994, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 707
goal_identified
=== ep: 708, time 27.167056798934937, eps 0.00100000000000038, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 670
=== ep: 709, time 65.87094497680664, eps 0.0010000000000003615, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 709
=== ep: 710, time 27.693090677261353, eps 0.0010000000000003437, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 710
=== ep: 711, time 27.376882314682007, eps 0.001000000000000327, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 711
=== ep: 712, time 33.05694770812988, eps 0.0010000000000003112, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 712
goal_identified
=== ep: 713, time 27.637829780578613, eps 0.001000000000000296, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 708
=== ep: 714, time 29.21814513206482, eps 0.0010000000000002815, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 714
=== ep: 715, time 30.148979663848877, eps 0.0010000000000002678, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 715
=== ep: 716, time 27.896914958953857, eps 0.0010000000000002548, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 716
=== ep: 717, time 27.54602360725403, eps 0.0010000000000002422, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 717
=== ep: 718, time 32.55526351928711, eps 0.0010000000000002305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 718
goal_identified
=== ep: 719, time 60.925100326538086, eps 0.0010000000000002192, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 713
=== ep: 720, time 27.89758801460266, eps 0.0010000000000002086, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 720
goal_identified
=== ep: 721, time 35.34720039367676, eps 0.0010000000000001984, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 719
=== ep: 722, time 28.67942452430725, eps 0.0010000000000001887, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 722
=== ep: 723, time 29.231109142303467, eps 0.0010000000000001796, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 723
=== ep: 724, time 26.01106333732605, eps 0.0010000000000001707, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 724
=== ep: 725, time 28.613662719726562, eps 0.0010000000000001624, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 725
=== ep: 726, time 28.375693798065186, eps 0.0010000000000001544, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 726
=== ep: 727, time 28.559575080871582, eps 0.001000000000000147, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 727
goal_identified
=== ep: 728, time 28.905750513076782, eps 0.0010000000000001399, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 721
goal_identified
=== ep: 729, time 61.28915739059448, eps 0.001000000000000133, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 729
goal_identified
=== ep: 730, time 27.730162620544434, eps 0.0010000000000001264, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 730
=== ep: 731, time 26.941364288330078, eps 0.0010000000000001204, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 731
=== ep: 732, time 26.856576681137085, eps 0.0010000000000001145, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 732
goal_identified
=== ep: 733, time 27.44916296005249, eps 0.0010000000000001089, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 728
=== ep: 734, time 28.02809453010559, eps 0.0010000000000001037, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 734
goal_identified
=== ep: 735, time 32.96227264404297, eps 0.0010000000000000985, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 733
=== ep: 736, time 27.918238878250122, eps 0.0010000000000000937, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 736
=== ep: 737, time 30.29491877555847, eps 0.0010000000000000891, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 737
=== ep: 738, time 28.40673804283142, eps 0.0010000000000000848, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 738
goal_identified
=== ep: 739, time 64.43826818466187, eps 0.0010000000000000807, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 739
=== ep: 740, time 27.722038984298706, eps 0.0010000000000000768, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 740
=== ep: 741, time 27.192311763763428, eps 0.001000000000000073, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 741
goal_identified
=== ep: 742, time 27.23103141784668, eps 0.0010000000000000694, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 735
=== ep: 743, time 25.87575364112854, eps 0.001000000000000066, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 743
=== ep: 744, time 27.275315523147583, eps 0.001000000000000063, sum reward: 0, score_diff -5, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 744
=== ep: 745, time 26.85872721672058, eps 0.0010000000000000599, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 745
=== ep: 746, time 36.310006856918335, eps 0.0010000000000000568, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 746
=== ep: 747, time 29.71834373474121, eps 0.001000000000000054, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 747
=== ep: 748, time 35.22371459007263, eps 0.0010000000000000514, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 748
=== ep: 749, time 53.813984870910645, eps 0.001000000000000049, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 749
=== ep: 750, time 34.487390756607056, eps 0.0010000000000000466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 750
=== ep: 751, time 32.77151298522949, eps 0.0010000000000000443, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 751
=== ep: 752, time 27.064038515090942, eps 0.001000000000000042, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 752
=== ep: 753, time 27.29676580429077, eps 0.0010000000000000401, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 753
=== ep: 754, time 28.04543447494507, eps 0.0010000000000000382, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 754
=== ep: 755, time 28.111592054367065, eps 0.0010000000000000362, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 755
=== ep: 756, time 27.418228149414062, eps 0.0010000000000000345, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 756
=== ep: 757, time 27.541563987731934, eps 0.0010000000000000328, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 757
=== ep: 758, time 28.07258367538452, eps 0.0010000000000000312, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 758
=== ep: 759, time 72.45205950737, eps 0.0010000000000000297, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 759
=== ep: 760, time 27.55749225616455, eps 0.0010000000000000282, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 760
=== ep: 761, time 27.665324926376343, eps 0.001000000000000027, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 761
=== ep: 762, time 25.2059006690979, eps 0.0010000000000000256, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 762
=== ep: 763, time 27.391404390335083, eps 0.0010000000000000243, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 763
=== ep: 764, time 27.682995796203613, eps 0.0010000000000000232, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 764
goal_identified
=== ep: 765, time 28.28398060798645, eps 0.001000000000000022, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 742
=== ep: 766, time 27.85934567451477, eps 0.0010000000000000208, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 766
=== ep: 767, time 27.137481212615967, eps 0.00100000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 767
=== ep: 768, time 27.674713611602783, eps 0.0010000000000000189, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 768
=== ep: 769, time 93.80846929550171, eps 0.001000000000000018, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 769
=== ep: 770, time 26.190893173217773, eps 0.0010000000000000172, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 770
=== ep: 771, time 28.124477863311768, eps 0.0010000000000000163, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 771
=== ep: 772, time 28.58292317390442, eps 0.0010000000000000154, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 772
=== ep: 773, time 34.43197512626648, eps 0.0010000000000000148, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 773
=== ep: 774, time 26.84525465965271, eps 0.0010000000000000141, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 774
=== ep: 775, time 29.145374298095703, eps 0.0010000000000000132, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 775
=== ep: 776, time 28.390602111816406, eps 0.0010000000000000126, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 776
=== ep: 777, time 28.15097975730896, eps 0.0010000000000000122, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 777
=== ep: 778, time 32.214009046554565, eps 0.0010000000000000115, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 778
=== ep: 779, time 85.65170407295227, eps 0.0010000000000000109, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 779
=== ep: 780, time 29.250200986862183, eps 0.0010000000000000104, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 780
=== ep: 781, time 27.45364499092102, eps 0.00100000000000001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 781
=== ep: 782, time 26.04871368408203, eps 0.0010000000000000093, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 782
=== ep: 783, time 38.818548917770386, eps 0.001000000000000009, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 783
=== ep: 784, time 28.636101007461548, eps 0.0010000000000000085, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 784
=== ep: 785, time 27.022502422332764, eps 0.001000000000000008, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 785
=== ep: 786, time 27.301584482192993, eps 0.0010000000000000076, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 786
=== ep: 787, time 28.277698755264282, eps 0.0010000000000000074, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 787
=== ep: 788, time 28.377270460128784, eps 0.001000000000000007, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 788
goal_identified
=== ep: 789, time 71.10978412628174, eps 0.0010000000000000067, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 765
=== ep: 790, time 28.068866968154907, eps 0.0010000000000000063, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 790
=== ep: 791, time 28.33474349975586, eps 0.001000000000000006, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 791
=== ep: 792, time 27.16252827644348, eps 0.0010000000000000057, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 792
=== ep: 793, time 28.089020252227783, eps 0.0010000000000000054, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 793
=== ep: 794, time 27.461172103881836, eps 0.0010000000000000052, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 794
=== ep: 795, time 27.78160524368286, eps 0.001000000000000005, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 795
=== ep: 796, time 27.963263034820557, eps 0.0010000000000000048, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 796
goal_identified
=== ep: 797, time 27.413513898849487, eps 0.0010000000000000044, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 789
=== ep: 798, time 28.629090547561646, eps 0.0010000000000000041, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 798
goal_identified
=== ep: 799, time 61.72117328643799, eps 0.0010000000000000041, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 797
=== ep: 800, time 28.99092197418213, eps 0.001000000000000004, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 800
goal_identified
=== ep: 801, time 33.65314722061157, eps 0.0010000000000000037, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 799
=== ep: 802, time 33.35886478424072, eps 0.0010000000000000035, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 802
=== ep: 803, time 27.03887152671814, eps 0.0010000000000000033, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 803
goal_identified
=== ep: 804, time 29.30530095100403, eps 0.001000000000000003, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 801
=== ep: 805, time 27.7288339138031, eps 0.001000000000000003, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 805
=== ep: 806, time 28.547871589660645, eps 0.0010000000000000028, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 806
=== ep: 807, time 28.029155492782593, eps 0.0010000000000000026, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 807
=== ep: 808, time 35.285857915878296, eps 0.0010000000000000026, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 808
=== ep: 809, time 56.7126407623291, eps 0.0010000000000000024, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 809
=== ep: 810, time 30.17935085296631, eps 0.0010000000000000024, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 810
=== ep: 811, time 28.09933066368103, eps 0.0010000000000000022, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 811
=== ep: 812, time 27.57875919342041, eps 0.0010000000000000022, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 812
=== ep: 813, time 26.961443662643433, eps 0.001000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 813
goal_identified
=== ep: 814, time 26.467912912368774, eps 0.001000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 804
=== ep: 815, time 28.078315019607544, eps 0.0010000000000000018, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 815
=== ep: 816, time 27.52960181236267, eps 0.0010000000000000018, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 816
=== ep: 817, time 28.004223346710205, eps 0.0010000000000000018, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 817
=== ep: 818, time 27.615636825561523, eps 0.0010000000000000015, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 818
=== ep: 819, time 74.30236029624939, eps 0.0010000000000000015, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 819
=== ep: 820, time 28.042147397994995, eps 0.0010000000000000013, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 820
=== ep: 821, time 35.430612325668335, eps 0.0010000000000000013, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 821
=== ep: 822, time 29.21531653404236, eps 0.0010000000000000013, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 822
=== ep: 823, time 28.58998966217041, eps 0.0010000000000000013, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 823
=== ep: 824, time 26.733044147491455, eps 0.001000000000000001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 824
=== ep: 825, time 28.79611849784851, eps 0.001000000000000001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 825
goal_identified
=== ep: 826, time 29.135376691818237, eps 0.001000000000000001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 814
=== ep: 827, time 29.891663074493408, eps 0.001000000000000001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 827
=== ep: 828, time 27.791504383087158, eps 0.0010000000000000009, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 828
goal_identified
=== ep: 829, time 63.174033641815186, eps 0.0010000000000000009, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 826
=== ep: 830, time 33.6884446144104, eps 0.0010000000000000009, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 830
=== ep: 831, time 27.981873512268066, eps 0.0010000000000000009, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 831
=== ep: 832, time 27.32789134979248, eps 0.0010000000000000009, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 832
=== ep: 833, time 27.03774118423462, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 833
=== ep: 834, time 29.603954076766968, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 834
=== ep: 835, time 28.038897275924683, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 835
=== ep: 836, time 27.69516348838806, eps 0.0010000000000000007, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 836
=== ep: 837, time 27.14348268508911, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 837
=== ep: 838, time 27.818567276000977, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 838
goal_identified
=== ep: 839, time 59.735278367996216, eps 0.0010000000000000007, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 829
=== ep: 840, time 28.29573106765747, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 840
=== ep: 841, time 34.78135681152344, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 841
=== ep: 842, time 28.1200008392334, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 842
=== ep: 843, time 32.3362820148468, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 843
=== ep: 844, time 28.27523684501648, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 844
=== ep: 845, time 27.273353099822998, eps 0.0010000000000000005, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 845
=== ep: 846, time 27.552284955978394, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 846
=== ep: 847, time 27.392189264297485, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 847
=== ep: 848, time 34.021501779556274, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 848
=== ep: 849, time 58.705214738845825, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 849
=== ep: 850, time 28.03584051132202, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 850
=== ep: 851, time 27.509591817855835, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 851
=== ep: 852, time 26.796038389205933, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 852
=== ep: 853, time 33.39238882064819, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 853
goal_identified
=== ep: 854, time 29.118687391281128, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 839
=== ep: 855, time 28.53967833518982, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 855
=== ep: 856, time 28.428394556045532, eps 0.0010000000000000002, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 856
goal_identified
=== ep: 857, time 29.08714509010315, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 854
=== ep: 858, time 28.523778915405273, eps 0.0010000000000000002, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 858
=== ep: 859, time 58.15381217002869, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 859
=== ep: 860, time 27.542422771453857, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 860
=== ep: 861, time 28.23615026473999, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 861
=== ep: 862, time 28.078635454177856, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 862
=== ep: 863, time 28.41767907142639, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 863
=== ep: 864, time 34.99762296676636, eps 0.0010000000000000002, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 864
goal_identified
=== ep: 865, time 31.11369252204895, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 857
goal_identified
=== ep: 866, time 29.998695850372314, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 865
=== ep: 867, time 29.215139627456665, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 867
=== ep: 868, time 28.662054777145386, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 868
=== ep: 869, time 70.74246072769165, eps 0.0010000000000000002, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 869
=== ep: 870, time 28.663947343826294, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 870
goal_identified
=== ep: 871, time 28.76305913925171, eps 0.0010000000000000002, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 871
=== ep: 872, time 30.775798320770264, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 872
=== ep: 873, time 28.670034170150757, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 873
=== ep: 874, time 31.123277187347412, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 874
=== ep: 875, time 29.651827096939087, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 875
=== ep: 876, time 31.720824480056763, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 876
=== ep: 877, time 29.8963360786438, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 877
=== ep: 878, time 30.456253051757812, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 878
=== ep: 879, time 98.99681806564331, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 879
=== ep: 880, time 30.912758350372314, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 880
=== ep: 881, time 32.04601573944092, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 881
=== ep: 882, time 29.439931392669678, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 882
=== ep: 883, time 34.72538685798645, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 883
=== ep: 884, time 30.60969352722168, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 884
goal_identified
=== ep: 885, time 28.9830379486084, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 866
=== ep: 886, time 29.34665322303772, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 886
=== ep: 887, time 33.919840574264526, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 887
=== ep: 888, time 48.715901374816895, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 888
=== ep: 889, time 81.84410357475281, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 889
goal_identified
=== ep: 890, time 29.026654720306396, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 890
goal_identified
=== ep: 891, time 29.01349401473999, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 885
=== ep: 892, time 33.47093844413757, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 892
=== ep: 893, time 32.03672242164612, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 893
goal_identified
=== ep: 894, time 39.540504693984985, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 891
=== ep: 895, time 32.841238021850586, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 895
goal_identified
=== ep: 896, time 30.06143307685852, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 894
=== ep: 897, time 28.17563223838806, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 897
=== ep: 898, time 33.062464475631714, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 898
=== ep: 899, time 90.14899492263794, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 899
=== ep: 900, time 31.593477249145508, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 900
=== ep: 901, time 32.80882215499878, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 901
=== ep: 902, time 33.06838011741638, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 902
=== ep: 903, time 32.57855558395386, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 903
=== ep: 904, time 30.32204794883728, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 904
=== ep: 905, time 31.308231353759766, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 905
=== ep: 906, time 28.807960271835327, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 906
=== ep: 907, time 32.49262475967407, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 907
=== ep: 908, time 30.102086067199707, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 908
=== ep: 909, time 81.69482564926147, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 909
=== ep: 910, time 38.09769153594971, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 910
=== ep: 911, time 32.35344123840332, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 911
=== ep: 912, time 33.04949975013733, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 912
=== ep: 913, time 36.72994089126587, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 913
=== ep: 914, time 34.1252179145813, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 914
=== ep: 915, time 34.07609486579895, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 915
=== ep: 916, time 31.68699622154236, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 916
=== ep: 917, time 31.73919701576233, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 917
=== ep: 918, time 29.152544260025024, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 918
=== ep: 919, time 79.75990796089172, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 919
=== ep: 920, time 28.22959041595459, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 920
=== ep: 921, time 30.799358129501343, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 921
=== ep: 922, time 29.505828619003296, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 922
=== ep: 923, time 31.966872453689575, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 923
=== ep: 924, time 35.048282623291016, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 924
=== ep: 925, time 38.49979090690613, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 925
=== ep: 926, time 30.3365638256073, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 926
=== ep: 927, time 34.53867316246033, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 927
=== ep: 928, time 33.474326610565186, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 928
goal_identified
=== ep: 929, time 82.15783262252808, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 896
=== ep: 930, time 36.296377182006836, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 930
=== ep: 931, time 28.858948230743408, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 931
=== ep: 932, time 31.07470679283142, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 932
=== ep: 933, time 32.11822175979614, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 933
=== ep: 934, time 31.116869688034058, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 934
=== ep: 935, time 29.86918616294861, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 935
=== ep: 936, time 40.27858757972717, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 936
=== ep: 937, time 34.21433639526367, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 937
=== ep: 938, time 37.33026933670044, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 938
=== ep: 939, time 86.17597651481628, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 939
=== ep: 940, time 28.684212923049927, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 940
=== ep: 941, time 31.43170189857483, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 941
=== ep: 942, time 29.7011821269989, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 942
=== ep: 943, time 29.06558585166931, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 943
goal_identified
=== ep: 944, time 29.14296817779541, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 929
=== ep: 945, time 30.476324319839478, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 945
goal_identified
=== ep: 946, time 29.79604935646057, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 944
=== ep: 947, time 34.30427575111389, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 947
=== ep: 948, time 32.40135717391968, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 948
=== ep: 949, time 70.41132235527039, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 949
=== ep: 950, time 29.802151679992676, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 950
=== ep: 951, time 29.936811685562134, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 951
goal_identified
=== ep: 952, time 31.040059089660645, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 946
=== ep: 953, time 29.233623266220093, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 953
goal_identified
=== ep: 954, time 29.92028546333313, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 954
=== ep: 955, time 30.1618812084198, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 955
goal_identified
=== ep: 956, time 28.764963150024414, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 956
goal_identified
goal_identified
=== ep: 957, time 30.986236095428467, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 952
=== ep: 958, time 30.986156225204468, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 958
=== ep: 959, time 90.78088808059692, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 959
goal_identified
=== ep: 960, time 30.45441246032715, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 960
=== ep: 961, time 31.30593466758728, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 961
=== ep: 962, time 29.51423406600952, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 962
=== ep: 963, time 43.43000555038452, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 963
=== ep: 964, time 36.40751504898071, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 964
=== ep: 965, time 31.596115350723267, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 965
goal_identified
=== ep: 966, time 29.146910667419434, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 966
=== ep: 967, time 31.738033533096313, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 967
=== ep: 968, time 29.668608903884888, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 968
=== ep: 969, time 83.62108898162842, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 969
=== ep: 970, time 33.95076870918274, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 970
goal_identified
=== ep: 971, time 32.343626976013184, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 971
goal_identified
=== ep: 972, time 33.95890212059021, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 972
=== ep: 973, time 39.30352568626404, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 973
=== ep: 974, time 30.27519178390503, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 974
=== ep: 975, time 28.747769117355347, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 975
=== ep: 976, time 32.54446005821228, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 976
=== ep: 977, time 29.488516330718994, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 977
=== ep: 978, time 29.82299304008484, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 978
=== ep: 979, time 86.24512577056885, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 979
=== ep: 980, time 33.26241588592529, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 980
=== ep: 981, time 31.06533932685852, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 981
=== ep: 982, time 33.482847452163696, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 982
=== ep: 983, time 29.640989780426025, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 983
=== ep: 984, time 36.06243133544922, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 984
=== ep: 985, time 35.21399188041687, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 985
=== ep: 986, time 30.60794234275818, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 986
=== ep: 987, time 32.179160356521606, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 987
=== ep: 988, time 29.0656635761261, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 988
=== ep: 989, time 64.92312335968018, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 989
goal_identified
=== ep: 990, time 34.307801961898804, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 990
=== ep: 991, time 29.96572756767273, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 991
=== ep: 992, time 29.684955596923828, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 992
=== ep: 993, time 29.998700618743896, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 993
goal_identified
=== ep: 994, time 30.451958656311035, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 994
=== ep: 995, time 40.21436929702759, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 995
=== ep: 996, time 31.92808175086975, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 996
=== ep: 997, time 30.44127631187439, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 997
=== ep: 998, time 31.62162446975708, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 998
=== ep: 999, time 83.05330491065979, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 999
=== ep: 1000, time 30.609703063964844, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1000
goal_identified
=== ep: 1001, time 30.774447202682495, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1001
=== ep: 1002, time 30.42239737510681, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1002
=== ep: 1003, time 31.036656141281128, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1003
=== ep: 1004, time 30.670734643936157, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1004
=== ep: 1005, time 29.905774354934692, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1005
=== ep: 1006, time 31.893896341323853, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1006
=== ep: 1007, time 29.690001249313354, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1007
=== ep: 1008, time 33.821966886520386, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1008
=== ep: 1009, time 68.07078576087952, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1009
goal_identified
=== ep: 1010, time 29.81139302253723, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1010
=== ep: 1011, time 31.81093692779541, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1011
=== ep: 1012, time 34.59811234474182, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1012
=== ep: 1013, time 29.170839548110962, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1013
=== ep: 1014, time 30.055531978607178, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1014
goal_identified
=== ep: 1015, time 33.521692752838135, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1015
=== ep: 1016, time 29.274765968322754, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1016
=== ep: 1017, time 30.27129602432251, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1017
=== ep: 1018, time 30.48329448699951, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1018
=== ep: 1019, time 84.0171172618866, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1019
=== ep: 1020, time 33.563318729400635, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1020
=== ep: 1021, time 32.71790385246277, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1021
=== ep: 1022, time 32.023027658462524, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1022
=== ep: 1023, time 30.06279683113098, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1023
=== ep: 1024, time 29.91655468940735, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1024
=== ep: 1025, time 31.966153383255005, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1025
goal_identified
=== ep: 1026, time 30.10114288330078, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1026
=== ep: 1027, time 33.94279861450195, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1027
goal_identified
=== ep: 1028, time 30.712125062942505, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1028
=== ep: 1029, time 75.94548964500427, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1029
=== ep: 1030, time 33.508153200149536, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1030
=== ep: 1031, time 31.653639316558838, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1031
=== ep: 1032, time 30.017107009887695, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1032
=== ep: 1033, time 30.940872192382812, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1033
=== ep: 1034, time 30.65950584411621, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1034
=== ep: 1035, time 29.06225085258484, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1035
goal_identified
=== ep: 1036, time 29.641783475875854, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1036
=== ep: 1037, time 30.30246925354004, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1037
=== ep: 1038, time 29.560645818710327, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1038
=== ep: 1039, time 68.2325222492218, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1039
=== ep: 1040, time 29.271822690963745, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1040
=== ep: 1041, time 32.06100940704346, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1041
goal_identified
=== ep: 1042, time 37.05294442176819, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1042
=== ep: 1043, time 29.814377546310425, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1043
=== ep: 1044, time 28.018486499786377, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1044
=== ep: 1045, time 30.04039239883423, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1045
=== ep: 1046, time 27.170271396636963, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1046
=== ep: 1047, time 29.35010051727295, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1047
=== ep: 1048, time 28.598907709121704, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1048
=== ep: 1049, time 60.36519002914429, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1049
=== ep: 1050, time 28.989925861358643, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1050
goal_identified
=== ep: 1051, time 31.35842752456665, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1051
=== ep: 1052, time 29.609584093093872, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1052
=== ep: 1053, time 31.253481149673462, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1053
=== ep: 1054, time 31.964756727218628, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1054
=== ep: 1055, time 30.388575553894043, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1055
=== ep: 1056, time 32.888782262802124, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1056
=== ep: 1057, time 29.545128107070923, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1057
=== ep: 1058, time 29.892775774002075, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1058
=== ep: 1059, time 68.02673649787903, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1059
=== ep: 1060, time 30.646026134490967, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1060
=== ep: 1061, time 28.07804775238037, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1061
=== ep: 1062, time 28.641639709472656, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1062
=== ep: 1063, time 29.85835886001587, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1063
goal_identified
=== ep: 1064, time 36.742618560791016, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1064
=== ep: 1065, time 32.15495276451111, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1065
=== ep: 1066, time 29.266495943069458, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1066
=== ep: 1067, time 28.589221239089966, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1067
=== ep: 1068, time 28.179445505142212, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1068
=== ep: 1069, time 65.00052881240845, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1069
goal_identified
=== ep: 1070, time 28.36927580833435, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1070
=== ep: 1071, time 38.158315658569336, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1071
=== ep: 1072, time 28.211187601089478, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1072
=== ep: 1073, time 39.344207525253296, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1073
=== ep: 1074, time 29.511637926101685, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1074
=== ep: 1075, time 31.93843364715576, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1075
=== ep: 1076, time 29.36080002784729, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1076
=== ep: 1077, time 29.829944610595703, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1077
=== ep: 1078, time 28.077234745025635, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1078
=== ep: 1079, time 76.19086623191833, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1079
=== ep: 1080, time 28.581050395965576, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1080
=== ep: 1081, time 27.211951732635498, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1081
=== ep: 1082, time 27.917755126953125, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1082
=== ep: 1083, time 28.949844121932983, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1083
goal_identified
=== ep: 1084, time 28.916901350021362, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1084
=== ep: 1085, time 27.138904571533203, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1085
=== ep: 1086, time 31.13288688659668, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1086
=== ep: 1087, time 35.183767557144165, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1087
=== ep: 1088, time 29.153979063034058, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1088
=== ep: 1089, time 70.0524365901947, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1089
=== ep: 1090, time 28.502370834350586, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1090
=== ep: 1091, time 28.619893550872803, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1091
=== ep: 1092, time 30.261754274368286, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1092
=== ep: 1093, time 28.358112812042236, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1093
=== ep: 1094, time 28.420408248901367, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1094
=== ep: 1095, time 25.0800940990448, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1095
=== ep: 1096, time 28.76973247528076, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1096
=== ep: 1097, time 29.73526930809021, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1097
=== ep: 1098, time 29.68686819076538, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1098
=== ep: 1099, time 65.76688742637634, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1099
=== ep: 1100, time 28.100703716278076, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1100
=== ep: 1101, time 29.115927934646606, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1101
=== ep: 1102, time 28.681389331817627, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1102
=== ep: 1103, time 28.52435874938965, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1103
=== ep: 1104, time 28.762341260910034, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1104
=== ep: 1105, time 28.091877222061157, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1105
=== ep: 1106, time 30.06314754486084, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1106
=== ep: 1107, time 27.737789392471313, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1107
=== ep: 1108, time 30.133556127548218, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1108
goal_identified
=== ep: 1109, time 68.01114439964294, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1109
=== ep: 1110, time 30.537916898727417, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1110
=== ep: 1111, time 30.20359754562378, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1111
=== ep: 1112, time 28.42938470840454, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1112
=== ep: 1113, time 28.70129680633545, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1113
=== ep: 1114, time 38.47366714477539, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1114
=== ep: 1115, time 28.881598234176636, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1115
=== ep: 1116, time 27.596302032470703, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1116
=== ep: 1117, time 26.20116639137268, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1117
=== ep: 1118, time 40.540170192718506, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1118
=== ep: 1119, time 63.57811117172241, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1119
=== ep: 1120, time 29.810896635055542, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1120
=== ep: 1121, time 28.060094833374023, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1121
=== ep: 1122, time 30.033305883407593, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1122
goal_identified
=== ep: 1123, time 33.995651960372925, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1123
=== ep: 1124, time 28.79643678665161, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1124
=== ep: 1125, time 40.44061779975891, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1125
=== ep: 1126, time 28.634429216384888, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1126
=== ep: 1127, time 28.924572467803955, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1127
=== ep: 1128, time 28.324466466903687, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1128
=== ep: 1129, time 61.307722091674805, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1129
=== ep: 1130, time 28.992053031921387, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1130
goal_identified
=== ep: 1131, time 30.57209587097168, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1131
=== ep: 1132, time 35.66858649253845, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1132
=== ep: 1133, time 28.83473253250122, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1133
=== ep: 1134, time 29.474149227142334, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1134
=== ep: 1135, time 29.569737911224365, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1135
=== ep: 1136, time 28.85711908340454, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1136
=== ep: 1137, time 28.650620222091675, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1137
=== ep: 1138, time 29.10514521598816, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1138
=== ep: 1139, time 62.85266947746277, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1139
=== ep: 1140, time 29.84050178527832, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1140
=== ep: 1141, time 30.868964433670044, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1141
goal_identified
=== ep: 1142, time 29.829360008239746, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1142
=== ep: 1143, time 39.938947916030884, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1143
=== ep: 1144, time 30.16424298286438, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1144
=== ep: 1145, time 33.40100049972534, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1145
=== ep: 1146, time 29.45253825187683, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1146
=== ep: 1147, time 29.425347566604614, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1147
=== ep: 1148, time 28.71871304512024, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1148
=== ep: 1149, time 58.36275911331177, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1149
goal_identified
=== ep: 1150, time 29.427319526672363, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1150
=== ep: 1151, time 34.16507291793823, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1151
=== ep: 1152, time 29.338152647018433, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1152
=== ep: 1153, time 30.57766342163086, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1153
=== ep: 1154, time 33.407387018203735, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1154
=== ep: 1155, time 40.293272495269775, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1155
=== ep: 1156, time 32.35966157913208, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1156
=== ep: 1157, time 30.569005966186523, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1157
=== ep: 1158, time 32.49407505989075, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1158
=== ep: 1159, time 67.37227058410645, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1159
=== ep: 1160, time 28.7149395942688, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1160
=== ep: 1161, time 25.90240168571472, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1161
=== ep: 1162, time 27.791396141052246, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1162
=== ep: 1163, time 29.10795545578003, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1163
=== ep: 1164, time 29.599570512771606, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1164
=== ep: 1165, time 32.16219186782837, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1165
=== ep: 1166, time 29.226308584213257, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1166
=== ep: 1167, time 28.270127534866333, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1167
=== ep: 1168, time 29.02385711669922, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1168
=== ep: 1169, time 70.17238569259644, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1169
=== ep: 1170, time 31.739851713180542, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1170
=== ep: 1171, time 38.41096353530884, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1171
=== ep: 1172, time 28.85720944404602, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1172
goal_identified
=== ep: 1173, time 29.16141176223755, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1173
=== ep: 1174, time 30.615450382232666, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1174
=== ep: 1175, time 31.95921277999878, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1175
=== ep: 1176, time 32.03038668632507, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1176
=== ep: 1177, time 29.315574645996094, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1177
=== ep: 1178, time 28.799973964691162, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1178
=== ep: 1179, time 74.94281792640686, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1179
=== ep: 1180, time 28.420345306396484, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1180
=== ep: 1181, time 30.15569519996643, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1181
goal_identified
=== ep: 1182, time 30.738019227981567, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1182
=== ep: 1183, time 29.273742198944092, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1183
=== ep: 1184, time 32.42548942565918, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1184
=== ep: 1185, time 31.776419401168823, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1185
=== ep: 1186, time 33.250067710876465, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1186
=== ep: 1187, time 29.126505374908447, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1187
goal_identified
=== ep: 1188, time 29.446417093276978, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1188
=== ep: 1189, time 79.4360785484314, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1189
=== ep: 1190, time 29.616012811660767, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1190
=== ep: 1191, time 30.24977445602417, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1191
=== ep: 1192, time 28.037337064743042, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1192
=== ep: 1193, time 28.356049299240112, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1193
=== ep: 1194, time 29.86218762397766, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1194
goal_identified
=== ep: 1195, time 28.670981407165527, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1195
=== ep: 1196, time 30.72161626815796, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1196
=== ep: 1197, time 28.876003742218018, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1197
=== ep: 1198, time 28.78337264060974, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1198
=== ep: 1199, time 73.18657660484314, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1199
=== ep: 1200, time 30.389421224594116, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1200
=== ep: 1201, time 28.18348526954651, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1201
=== ep: 1202, time 28.796329975128174, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1202
=== ep: 1203, time 26.396763563156128, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1203
=== ep: 1204, time 29.15913462638855, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1204
=== ep: 1205, time 33.80240082740784, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1205
=== ep: 1206, time 34.39229369163513, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1206
=== ep: 1207, time 28.673301219940186, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1207
=== ep: 1208, time 31.320486307144165, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1208
=== ep: 1209, time 69.09660291671753, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1209
=== ep: 1210, time 29.3973171710968, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1210
=== ep: 1211, time 30.29473042488098, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1211
=== ep: 1212, time 27.549872636795044, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1212
=== ep: 1213, time 33.0416259765625, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1213
=== ep: 1214, time 38.037068605422974, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1214
=== ep: 1215, time 31.437560319900513, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1215
=== ep: 1216, time 28.229048013687134, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1216
=== ep: 1217, time 28.492480516433716, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1217
=== ep: 1218, time 29.05641508102417, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1218
=== ep: 1219, time 70.45431780815125, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1219
=== ep: 1220, time 29.484318733215332, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1220
=== ep: 1221, time 31.865567445755005, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1221
=== ep: 1222, time 28.784581899642944, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1222
goal_identified
=== ep: 1223, time 28.84154772758484, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1223
=== ep: 1224, time 29.626139640808105, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1224
goal_identified
=== ep: 1225, time 28.36198854446411, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1225
=== ep: 1226, time 29.142229318618774, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1226
=== ep: 1227, time 27.92745804786682, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1227
=== ep: 1228, time 33.10371232032776, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1228
goal_identified
=== ep: 1229, time 62.45055294036865, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1229
goal_identified
=== ep: 1230, time 29.5222749710083, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1230
=== ep: 1231, time 29.74438977241516, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1231
goal_identified
=== ep: 1232, time 32.08177828788757, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1232
=== ep: 1233, time 28.667253494262695, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1233
=== ep: 1234, time 38.96858239173889, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1234
=== ep: 1235, time 35.85710692405701, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1235
=== ep: 1236, time 28.043076992034912, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1236
=== ep: 1237, time 28.233092784881592, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1237
=== ep: 1238, time 28.612255334854126, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1238
=== ep: 1239, time 59.983957052230835, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1239
goal_identified
=== ep: 1240, time 27.68012571334839, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1240
=== ep: 1241, time 29.484748601913452, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1241
=== ep: 1242, time 28.628958463668823, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1242
=== ep: 1243, time 28.38080906867981, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1243
=== ep: 1244, time 28.413373947143555, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1244
=== ep: 1245, time 28.58056902885437, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1245
=== ep: 1246, time 27.35326099395752, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1246
=== ep: 1247, time 27.15994906425476, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1247
=== ep: 1248, time 28.783800840377808, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1248
=== ep: 1249, time 60.627312421798706, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1249
=== ep: 1250, time 28.53507089614868, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1250
=== ep: 1251, time 27.46355366706848, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1251
=== ep: 1252, time 28.190062284469604, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1252
=== ep: 1253, time 28.148166179656982, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1253
=== ep: 1254, time 29.84806251525879, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1254
=== ep: 1255, time 30.24757170677185, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1255
=== ep: 1256, time 27.901516675949097, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1256
=== ep: 1257, time 27.838285446166992, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1257
=== ep: 1258, time 27.629618406295776, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1258
goal_identified
=== ep: 1259, time 61.55582332611084, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1259
=== ep: 1260, time 29.277557611465454, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1260
=== ep: 1261, time 28.00831627845764, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1261
=== ep: 1262, time 26.049466371536255, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1262
=== ep: 1263, time 29.358431577682495, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1263
=== ep: 1264, time 26.7344388961792, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1264
goal_identified
=== ep: 1265, time 27.601561307907104, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1265
=== ep: 1266, time 28.471663236618042, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1266
=== ep: 1267, time 29.08368706703186, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1267
=== ep: 1268, time 36.11842703819275, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1268
goal_identified
=== ep: 1269, time 57.98849821090698, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1269
=== ep: 1270, time 28.063428163528442, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1270
=== ep: 1271, time 27.500729084014893, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1271
=== ep: 1272, time 27.791504859924316, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1272
=== ep: 1273, time 28.184249877929688, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1273
=== ep: 1274, time 28.599483251571655, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1274
=== ep: 1275, time 27.311877727508545, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1275
=== ep: 1276, time 30.30152940750122, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1276
=== ep: 1277, time 35.97282338142395, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1277
=== ep: 1278, time 27.900450229644775, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1278
=== ep: 1279, time 52.250521421432495, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1279
=== ep: 1280, time 35.37615704536438, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1280
=== ep: 1281, time 28.033103466033936, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1281
goal_identified
=== ep: 1282, time 28.01023507118225, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1282
=== ep: 1283, time 27.294053554534912, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1283
goal_identified
=== ep: 1284, time 35.151265382766724, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1284
=== ep: 1285, time 28.580852031707764, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1285
=== ep: 1286, time 34.830979347229004, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1286
=== ep: 1287, time 28.03938889503479, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1287
goal_identified
=== ep: 1288, time 28.15356206893921, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1288
=== ep: 1289, time 79.37254762649536, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1289
=== ep: 1290, time 29.321831703186035, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1290
=== ep: 1291, time 27.18247628211975, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1291
=== ep: 1292, time 28.025654077529907, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1292
=== ep: 1293, time 27.712000846862793, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1293
=== ep: 1294, time 29.655139684677124, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1294
=== ep: 1295, time 28.12039303779602, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1295
=== ep: 1296, time 27.128206491470337, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1296
=== ep: 1297, time 30.27488613128662, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1297
=== ep: 1298, time 28.093169927597046, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1298
=== ep: 1299, time 74.15891122817993, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1299
=== ep: 1300, time 30.26798391342163, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1300
=== ep: 1301, time 28.501717567443848, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1301
=== ep: 1302, time 28.96354079246521, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1302
=== ep: 1303, time 27.800267219543457, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1303
=== ep: 1304, time 28.00883436203003, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1304
=== ep: 1305, time 27.919180870056152, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1305
=== ep: 1306, time 26.671101570129395, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1306
=== ep: 1307, time 30.267213582992554, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1307
=== ep: 1308, time 33.653316497802734, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1308
=== ep: 1309, time 67.63490724563599, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1309
=== ep: 1310, time 29.03387975692749, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1310
=== ep: 1311, time 29.640286684036255, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1311
=== ep: 1312, time 27.621113300323486, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1312
=== ep: 1313, time 29.804952144622803, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1313
=== ep: 1314, time 28.511104822158813, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1314
=== ep: 1315, time 27.638779163360596, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1315
=== ep: 1316, time 28.457950592041016, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1316
=== ep: 1317, time 31.333368062973022, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1317
=== ep: 1318, time 28.987820625305176, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1318
=== ep: 1319, time 55.527409076690674, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1319
=== ep: 1320, time 30.41584873199463, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1320
=== ep: 1321, time 40.89848875999451, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1321
=== ep: 1322, time 29.132769107818604, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1322
=== ep: 1323, time 29.000166416168213, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1323
=== ep: 1324, time 29.0813250541687, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1324
=== ep: 1325, time 28.114116430282593, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1325
=== ep: 1326, time 40.47433829307556, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1326
=== ep: 1327, time 28.05296802520752, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1327
=== ep: 1328, time 28.38001799583435, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1328
=== ep: 1329, time 55.073856592178345, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1329
goal_identified
=== ep: 1330, time 27.872747659683228, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1330
=== ep: 1331, time 30.005221605300903, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1331
=== ep: 1332, time 28.1950044631958, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1332
=== ep: 1333, time 26.936691284179688, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1333
=== ep: 1334, time 36.77955603599548, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1334
=== ep: 1335, time 29.253641366958618, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1335
=== ep: 1336, time 36.05455923080444, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1336
=== ep: 1337, time 36.16903209686279, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1337
=== ep: 1338, time 27.6264545917511, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1338
=== ep: 1339, time 58.29330921173096, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1339
=== ep: 1340, time 26.35978412628174, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1340
=== ep: 1341, time 31.296095848083496, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1341
=== ep: 1342, time 29.236077785491943, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1342
=== ep: 1343, time 27.204827547073364, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1343
=== ep: 1344, time 27.002022743225098, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1344
=== ep: 1345, time 28.9072163105011, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1345
=== ep: 1346, time 27.733880519866943, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1346
=== ep: 1347, time 28.52709436416626, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1347
=== ep: 1348, time 26.97441291809082, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1348
=== ep: 1349, time 61.60758566856384, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1349
=== ep: 1350, time 26.472504377365112, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1350
=== ep: 1351, time 28.567573070526123, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1351
=== ep: 1352, time 28.518396139144897, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1352
=== ep: 1353, time 33.771331787109375, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1353
=== ep: 1354, time 27.718265771865845, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1354
=== ep: 1355, time 26.34068751335144, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1355
goal_identified
=== ep: 1356, time 26.466073513031006, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1356
=== ep: 1357, time 26.394140243530273, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1357
=== ep: 1358, time 27.095000982284546, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1358
goal_identified
=== ep: 1359, time 75.56651735305786, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1359
=== ep: 1360, time 31.053910970687866, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1360
=== ep: 1361, time 26.65551471710205, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1361
=== ep: 1362, time 27.219176530838013, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1362
=== ep: 1363, time 26.982460260391235, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1363
=== ep: 1364, time 26.8647518157959, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1364
=== ep: 1365, time 26.61085057258606, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1365
=== ep: 1366, time 37.04764437675476, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1366
=== ep: 1367, time 34.337607622146606, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1367
=== ep: 1368, time 26.964040517807007, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1368
=== ep: 1369, time 65.63005995750427, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1369
goal_identified
=== ep: 1370, time 35.51130652427673, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1370
goal_identified
=== ep: 1371, time 28.755481481552124, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1371
=== ep: 1372, time 26.900212287902832, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1372
=== ep: 1373, time 26.22486901283264, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1373
=== ep: 1374, time 27.694684743881226, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1374
=== ep: 1375, time 35.95537734031677, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1375
=== ep: 1376, time 27.08802056312561, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1376
=== ep: 1377, time 26.576066732406616, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1377
=== ep: 1378, time 27.423599243164062, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1378
goal_identified
=== ep: 1379, time 62.12905406951904, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1379
=== ep: 1380, time 29.467238903045654, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1380
=== ep: 1381, time 28.594181299209595, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1381
=== ep: 1382, time 27.327573537826538, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1382
=== ep: 1383, time 28.95434832572937, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1383
=== ep: 1384, time 26.609224557876587, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1384
=== ep: 1385, time 37.70578622817993, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1385
=== ep: 1386, time 26.528101921081543, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1386
goal_identified
=== ep: 1387, time 26.853164196014404, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1387
=== ep: 1388, time 26.901637077331543, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1388
=== ep: 1389, time 58.94311833381653, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1389
=== ep: 1390, time 26.33248782157898, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1390
=== ep: 1391, time 27.41645336151123, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1391
=== ep: 1392, time 34.92801833152771, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1392
=== ep: 1393, time 27.548604726791382, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1393
=== ep: 1394, time 28.08993697166443, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1394
=== ep: 1395, time 30.09368395805359, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1395
goal_identified
=== ep: 1396, time 26.017452001571655, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1396
=== ep: 1397, time 27.37248921394348, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1397
=== ep: 1398, time 31.458426237106323, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1398
=== ep: 1399, time 54.958624601364136, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1399
=== ep: 1400, time 25.63384199142456, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1400
=== ep: 1401, time 37.468807220458984, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1401
=== ep: 1402, time 26.69326949119568, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1402
=== ep: 1403, time 34.08529305458069, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1403
=== ep: 1404, time 28.89569354057312, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1404
=== ep: 1405, time 37.504658699035645, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1405
=== ep: 1406, time 28.333646059036255, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1406
=== ep: 1407, time 28.771130084991455, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1407
=== ep: 1408, time 26.427082777023315, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1408
goal_identified
=== ep: 1409, time 62.088531732559204, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1409
=== ep: 1410, time 25.421035289764404, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1410
=== ep: 1411, time 28.135618448257446, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1411
=== ep: 1412, time 26.452470541000366, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1412
=== ep: 1413, time 27.218374729156494, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1413
goal_identified
=== ep: 1414, time 28.743675470352173, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1414
=== ep: 1415, time 25.705292224884033, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1415
=== ep: 1416, time 27.189922332763672, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1416
goal_identified
=== ep: 1417, time 27.276194095611572, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1417
goal_identified
=== ep: 1418, time 27.28448247909546, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1418
=== ep: 1419, time 67.9191107749939, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1419
=== ep: 1420, time 26.86040186882019, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1420
=== ep: 1421, time 24.765705585479736, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1421
=== ep: 1422, time 29.6153085231781, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1422
=== ep: 1423, time 27.413482904434204, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1423
=== ep: 1424, time 27.37367534637451, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1424
=== ep: 1425, time 26.925159692764282, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1425
=== ep: 1426, time 26.99065923690796, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1426
=== ep: 1427, time 27.752659797668457, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1427
=== ep: 1428, time 26.578704357147217, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1428
=== ep: 1429, time 75.70307278633118, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1429
=== ep: 1430, time 28.363544940948486, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1430
=== ep: 1431, time 26.16312885284424, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1431
=== ep: 1432, time 27.24113893508911, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1432
=== ep: 1433, time 30.259039402008057, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1433
=== ep: 1434, time 31.254960775375366, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1434
=== ep: 1435, time 27.26748561859131, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1435
=== ep: 1436, time 30.44246244430542, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1436
=== ep: 1437, time 31.857388257980347, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1437
=== ep: 1438, time 26.236835718154907, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1438
=== ep: 1439, time 73.05322551727295, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1439
=== ep: 1440, time 26.62586212158203, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1440
=== ep: 1441, time 32.03754472732544, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1441
=== ep: 1442, time 30.626272201538086, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1442
=== ep: 1443, time 27.967036724090576, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1443
=== ep: 1444, time 26.68850088119507, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1444
=== ep: 1445, time 28.558902263641357, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1445
=== ep: 1446, time 26.098838806152344, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1446
=== ep: 1447, time 26.70423913002014, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1447
=== ep: 1448, time 26.45620083808899, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1448
=== ep: 1449, time 70.84983611106873, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1449
=== ep: 1450, time 26.288957118988037, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1450
=== ep: 1451, time 26.96551489830017, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1451
=== ep: 1452, time 30.032673835754395, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1452
=== ep: 1453, time 28.743582248687744, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1453
=== ep: 1454, time 28.858078956604004, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1454
=== ep: 1455, time 28.000932931900024, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1455
=== ep: 1456, time 26.883050441741943, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1456
=== ep: 1457, time 26.620508432388306, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1457
=== ep: 1458, time 26.958308458328247, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1458
goal_identified
=== ep: 1459, time 56.788204193115234, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1459
=== ep: 1460, time 26.94445300102234, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1460
=== ep: 1461, time 32.96365761756897, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1461
=== ep: 1462, time 27.931173086166382, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1462
=== ep: 1463, time 27.20815896987915, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1463
=== ep: 1464, time 26.78866171836853, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1464
=== ep: 1465, time 37.917747020721436, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1465
=== ep: 1466, time 28.844279050827026, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1466
=== ep: 1467, time 27.632339239120483, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1467
=== ep: 1468, time 28.611281871795654, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1468
=== ep: 1469, time 61.57728314399719, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1469
=== ep: 1470, time 27.308215618133545, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1470
=== ep: 1471, time 25.988927602767944, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1471
=== ep: 1472, time 27.16152286529541, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1472
=== ep: 1473, time 26.54155683517456, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1473
goal_identified
=== ep: 1474, time 27.371717929840088, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1474
=== ep: 1475, time 27.909628868103027, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1475
=== ep: 1476, time 26.880581617355347, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1476
=== ep: 1477, time 29.392744302749634, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1477
=== ep: 1478, time 30.182488679885864, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1478
=== ep: 1479, time 91.12262988090515, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1479
goal_identified
=== ep: 1480, time 27.064030647277832, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1480
=== ep: 1481, time 26.84550452232361, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1481
=== ep: 1482, time 25.66503143310547, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1482
=== ep: 1483, time 28.361320734024048, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1483
=== ep: 1484, time 26.8363618850708, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1484
=== ep: 1485, time 26.812649250030518, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1485
=== ep: 1486, time 27.067968130111694, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1486
=== ep: 1487, time 27.70987296104431, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1487
=== ep: 1488, time 27.563403367996216, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1488
=== ep: 1489, time 64.330570936203, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1489
=== ep: 1490, time 27.130523920059204, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1490
=== ep: 1491, time 27.815815687179565, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1491
=== ep: 1492, time 28.495813608169556, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1492
=== ep: 1493, time 29.301419258117676, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1493
=== ep: 1494, time 38.359565019607544, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1494
=== ep: 1495, time 27.992696523666382, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1495
goal_identified
goal_identified
goal_identified
=== ep: 1496, time 27.228599309921265, eps 0.001, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 0
=== ep: 1497, time 36.24495887756348, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1497
=== ep: 1498, time 26.058276891708374, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1498
=== ep: 1499, time 58.68814516067505, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1499
=== ep: 1500, time 27.202062368392944, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1500
goal_identified
=== ep: 1501, time 28.534292936325073, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1501
goal_identified
=== ep: 1502, time 29.19759202003479, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1502
=== ep: 1503, time 42.567301988601685, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1503
=== ep: 1504, time 27.863248109817505, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1504
=== ep: 1505, time 26.53115749359131, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1505
=== ep: 1506, time 30.535019397735596, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1506
=== ep: 1507, time 27.840943574905396, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1507
=== ep: 1508, time 26.941827297210693, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1508
=== ep: 1509, time 60.21363544464111, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1509
=== ep: 1510, time 26.943118572235107, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1510
=== ep: 1511, time 38.72509741783142, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1511
goal_identified
=== ep: 1512, time 29.506470918655396, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1512
=== ep: 1513, time 28.024490356445312, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1513
=== ep: 1514, time 27.766138553619385, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1514
=== ep: 1515, time 26.742666721343994, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1515
=== ep: 1516, time 26.81944966316223, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1516
=== ep: 1517, time 26.668604373931885, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1517
goal_identified
=== ep: 1518, time 27.068935871124268, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1518
=== ep: 1519, time 66.23892736434937, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1519
=== ep: 1520, time 28.12400770187378, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1520
=== ep: 1521, time 37.50306820869446, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1521
=== ep: 1522, time 26.078351259231567, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1522
=== ep: 1523, time 37.27073788642883, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1523
=== ep: 1524, time 28.73629403114319, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1524
goal_identified
=== ep: 1525, time 28.567370891571045, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1525
=== ep: 1526, time 38.828712940216064, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1526
=== ep: 1527, time 26.870660305023193, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1527
=== ep: 1528, time 29.313611268997192, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1528
=== ep: 1529, time 87.46219253540039, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1529
=== ep: 1530, time 27.88789391517639, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1530
=== ep: 1531, time 27.221855878829956, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1531
=== ep: 1532, time 26.677144765853882, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1532
=== ep: 1533, time 36.206502199172974, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1533
=== ep: 1534, time 26.23777174949646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1534
=== ep: 1535, time 30.65979313850403, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1535
=== ep: 1536, time 29.53346872329712, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1536
=== ep: 1537, time 27.908310651779175, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1537
=== ep: 1538, time 25.50483512878418, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1538
=== ep: 1539, time 64.40612506866455, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1539
=== ep: 1540, time 27.301071643829346, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1540
=== ep: 1541, time 27.645561456680298, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1541
goal_identified
=== ep: 1542, time 26.201395988464355, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1542
=== ep: 1543, time 26.689993619918823, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1543
=== ep: 1544, time 29.50544810295105, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1544
goal_identified
=== ep: 1545, time 27.434608221054077, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1545
=== ep: 1546, time 28.177056789398193, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1546
goal_identified
=== ep: 1547, time 30.774757385253906, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1547
goal_identified
=== ep: 1548, time 27.274117708206177, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1548
=== ep: 1549, time 66.99601984024048, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1549
=== ep: 1550, time 27.651686906814575, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1550
=== ep: 1551, time 28.054742097854614, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1551
=== ep: 1552, time 26.170158863067627, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1552
=== ep: 1553, time 33.480098247528076, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1553
=== ep: 1554, time 27.38833785057068, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1554
=== ep: 1555, time 27.05849027633667, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1555
=== ep: 1556, time 28.188747882843018, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1556
=== ep: 1557, time 26.56657886505127, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1557
=== ep: 1558, time 27.20393133163452, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1558
=== ep: 1559, time 98.53263878822327, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1559
=== ep: 1560, time 31.43848204612732, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1560
=== ep: 1561, time 26.7507107257843, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1561
goal_identified
=== ep: 1562, time 29.36921977996826, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1562
goal_identified
=== ep: 1563, time 28.481086254119873, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1563
=== ep: 1564, time 27.78764033317566, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1564
=== ep: 1565, time 27.193774461746216, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1565
=== ep: 1566, time 26.207616090774536, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1566
=== ep: 1567, time 27.20293354988098, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1567
=== ep: 1568, time 26.99197220802307, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1568
=== ep: 1569, time 74.86940169334412, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1569
goal_identified
=== ep: 1570, time 35.83499097824097, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1570
=== ep: 1571, time 27.506102561950684, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1571
=== ep: 1572, time 27.61458110809326, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1572
=== ep: 1573, time 27.07564401626587, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1573
=== ep: 1574, time 31.520204544067383, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1574
=== ep: 1575, time 35.469947814941406, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1575
=== ep: 1576, time 25.96887707710266, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1576
=== ep: 1577, time 26.209527015686035, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1577
=== ep: 1578, time 30.03370952606201, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1578
=== ep: 1579, time 72.46118664741516, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1579
=== ep: 1580, time 28.71068811416626, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1580
=== ep: 1581, time 27.326992988586426, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1581
=== ep: 1582, time 27.84705662727356, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1582
=== ep: 1583, time 27.632157564163208, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1583
goal_identified
=== ep: 1584, time 27.62973713874817, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1584
=== ep: 1585, time 27.157023668289185, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1585
=== ep: 1586, time 26.970139026641846, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1586
=== ep: 1587, time 39.85470914840698, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1587
=== ep: 1588, time 34.662596702575684, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1588
=== ep: 1589, time 72.24944043159485, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1589
=== ep: 1590, time 35.477437257766724, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1590
=== ep: 1591, time 28.987144947052002, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1591
=== ep: 1592, time 28.429101705551147, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1592
goal_identified
=== ep: 1593, time 28.839450359344482, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1593
=== ep: 1594, time 30.452716827392578, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1594
=== ep: 1595, time 27.391753435134888, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1595
=== ep: 1596, time 27.38751530647278, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1596
=== ep: 1597, time 27.437170267105103, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1597
=== ep: 1598, time 27.847100019454956, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1598
=== ep: 1599, time 62.21619272232056, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1599
=== ep: 1600, time 36.22575306892395, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1600
=== ep: 1601, time 27.839839696884155, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1601
=== ep: 1602, time 37.4147367477417, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1602
goal_identified
=== ep: 1603, time 28.80810022354126, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1603
=== ep: 1604, time 37.28895449638367, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1604
=== ep: 1605, time 28.429847478866577, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1605
=== ep: 1606, time 28.834686040878296, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1606
=== ep: 1607, time 30.69664978981018, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1607
=== ep: 1608, time 28.09153151512146, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1608
=== ep: 1609, time 62.1487295627594, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1609
=== ep: 1610, time 29.322535037994385, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1610
=== ep: 1611, time 28.127346515655518, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1611
=== ep: 1612, time 30.72997784614563, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1612
=== ep: 1613, time 27.54912042617798, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1613
=== ep: 1614, time 27.525765895843506, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1614
=== ep: 1615, time 29.649251461029053, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1615
=== ep: 1616, time 39.43358302116394, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1616
=== ep: 1617, time 29.04879355430603, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1617
=== ep: 1618, time 28.0443696975708, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1618
=== ep: 1619, time 68.70151042938232, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1619
=== ep: 1620, time 26.916918992996216, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1620
=== ep: 1621, time 28.323997020721436, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1621
=== ep: 1622, time 28.02271556854248, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1622
=== ep: 1623, time 32.32378387451172, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1623
=== ep: 1624, time 25.977155208587646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1624
goal_identified
=== ep: 1625, time 27.879321813583374, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1625
=== ep: 1626, time 27.930423259735107, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1626
=== ep: 1627, time 29.895045518875122, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1627
=== ep: 1628, time 30.7048921585083, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1628
=== ep: 1629, time 75.74223017692566, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1629
=== ep: 1630, time 27.258215188980103, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1630
=== ep: 1631, time 29.915382146835327, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1631
=== ep: 1632, time 31.871063232421875, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1632
=== ep: 1633, time 31.51391100883484, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1633
=== ep: 1634, time 27.880672693252563, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1634
=== ep: 1635, time 27.505849838256836, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1635
=== ep: 1636, time 31.455187797546387, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1636
=== ep: 1637, time 27.95616054534912, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1637
=== ep: 1638, time 27.701455116271973, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1638
=== ep: 1639, time 77.63518595695496, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1639
=== ep: 1640, time 27.97480058670044, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1640
goal_identified
=== ep: 1641, time 30.10345458984375, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1641
=== ep: 1642, time 27.00209331512451, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1642
=== ep: 1643, time 27.383849620819092, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1643
=== ep: 1644, time 31.646620750427246, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1644
=== ep: 1645, time 34.34548497200012, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1645
=== ep: 1646, time 27.74334979057312, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1646
=== ep: 1647, time 27.78375744819641, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1647
=== ep: 1648, time 26.204553604125977, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1648
=== ep: 1649, time 80.66955828666687, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1649
=== ep: 1650, time 29.28192162513733, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1650
=== ep: 1651, time 28.54605722427368, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1651
=== ep: 1652, time 37.21693277359009, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1652
=== ep: 1653, time 31.543335676193237, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1653
=== ep: 1654, time 27.47331690788269, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1654
=== ep: 1655, time 28.61580204963684, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1655
=== ep: 1656, time 26.964680433273315, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1656
=== ep: 1657, time 27.67084288597107, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1657
=== ep: 1658, time 28.15497875213623, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1658
=== ep: 1659, time 64.691232919693, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1659
=== ep: 1660, time 28.581012725830078, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1660
=== ep: 1661, time 29.329753875732422, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1661
goal_identified
=== ep: 1662, time 29.094013690948486, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1662
=== ep: 1663, time 25.34216284751892, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1663
=== ep: 1664, time 27.69505214691162, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1664
=== ep: 1665, time 34.801677942276, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1665
=== ep: 1666, time 28.3440101146698, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1666
=== ep: 1667, time 27.146003484725952, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1667
=== ep: 1668, time 26.896712064743042, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1668
=== ep: 1669, time 71.70304036140442, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1669
=== ep: 1670, time 27.50339412689209, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1670
=== ep: 1671, time 29.104249715805054, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1671
=== ep: 1672, time 27.634487628936768, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1672
=== ep: 1673, time 30.44280767440796, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1673
=== ep: 1674, time 29.34559154510498, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1674
=== ep: 1675, time 28.354865074157715, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1675
=== ep: 1676, time 35.30514049530029, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1676
=== ep: 1677, time 26.721665143966675, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1677
=== ep: 1678, time 29.300495862960815, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1678
=== ep: 1679, time 68.26760935783386, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1679
=== ep: 1680, time 28.006924390792847, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1680
=== ep: 1681, time 26.91538405418396, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1681
=== ep: 1682, time 32.7403404712677, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1682
=== ep: 1683, time 29.57857894897461, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1683
=== ep: 1684, time 29.39624786376953, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1684
=== ep: 1685, time 29.770291328430176, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1685
=== ep: 1686, time 35.39820408821106, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1686
goal_identified
=== ep: 1687, time 27.931525707244873, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1687
=== ep: 1688, time 25.129892349243164, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1688
=== ep: 1689, time 87.41632914543152, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1689
goal_identified
=== ep: 1690, time 27.490407466888428, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1690
=== ep: 1691, time 33.81549143791199, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1691
=== ep: 1692, time 29.58809232711792, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1692
=== ep: 1693, time 33.85536074638367, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1693
=== ep: 1694, time 29.314250946044922, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1694
=== ep: 1695, time 29.298067808151245, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1695
=== ep: 1696, time 29.17025661468506, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1696
=== ep: 1697, time 27.035553216934204, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1697
goal_identified
=== ep: 1698, time 28.23876929283142, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1698
=== ep: 1699, time 75.40955662727356, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1699
=== ep: 1700, time 27.139042377471924, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1700
=== ep: 1701, time 33.8649799823761, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1701
=== ep: 1702, time 27.78765368461609, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1702
=== ep: 1703, time 34.09255027770996, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1703
=== ep: 1704, time 27.660074949264526, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1704
=== ep: 1705, time 28.879956245422363, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1705
=== ep: 1706, time 30.261173009872437, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1706
=== ep: 1707, time 29.1635479927063, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1707
=== ep: 1708, time 27.74863839149475, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1708
=== ep: 1709, time 66.29882383346558, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1709
=== ep: 1710, time 30.813371658325195, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1710
=== ep: 1711, time 27.560805797576904, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1711
=== ep: 1712, time 29.021697998046875, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1712
=== ep: 1713, time 28.144559144973755, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1713
=== ep: 1714, time 28.039567947387695, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1714
=== ep: 1715, time 27.623445749282837, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1715
=== ep: 1716, time 28.488280534744263, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1716
=== ep: 1717, time 28.162553071975708, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1717
=== ep: 1718, time 29.31193256378174, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1718
=== ep: 1719, time 84.93704581260681, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1719
=== ep: 1720, time 28.009196043014526, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1720
=== ep: 1721, time 30.12124800682068, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1721
=== ep: 1722, time 27.7500422000885, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1722
goal_identified
=== ep: 1723, time 27.84393548965454, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1723
=== ep: 1724, time 27.493530988693237, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1724
=== ep: 1725, time 29.15910506248474, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1725
=== ep: 1726, time 26.890347003936768, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1726
=== ep: 1727, time 27.45460057258606, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1727
=== ep: 1728, time 27.692387342453003, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1728
=== ep: 1729, time 69.82887840270996, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1729
=== ep: 1730, time 28.191761255264282, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1730
=== ep: 1731, time 27.904716730117798, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1731
=== ep: 1732, time 29.982270002365112, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1732
=== ep: 1733, time 32.25756597518921, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1733
=== ep: 1734, time 28.261444568634033, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1734
=== ep: 1735, time 28.59581446647644, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1735
=== ep: 1736, time 28.93731141090393, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1736
=== ep: 1737, time 27.53218102455139, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1737
goal_identified
=== ep: 1738, time 29.056146383285522, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1738
=== ep: 1739, time 64.74286484718323, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1739
=== ep: 1740, time 28.79765272140503, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1740
=== ep: 1741, time 29.249248504638672, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1741
=== ep: 1742, time 28.764225721359253, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1742
=== ep: 1743, time 30.351819276809692, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1743
=== ep: 1744, time 28.901857137680054, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1744
=== ep: 1745, time 38.97452211380005, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1745
=== ep: 1746, time 28.69713020324707, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1746
=== ep: 1747, time 27.82570171356201, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1747
goal_identified
=== ep: 1748, time 28.357722997665405, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1748
goal_identified
=== ep: 1749, time 73.46636509895325, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1749
=== ep: 1750, time 28.17917799949646, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1750
=== ep: 1751, time 27.50116801261902, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1751
=== ep: 1752, time 31.693544387817383, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1752
=== ep: 1753, time 28.450602293014526, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1753
=== ep: 1754, time 29.601780891418457, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1754
=== ep: 1755, time 30.991599082946777, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1755
=== ep: 1756, time 28.005007028579712, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1756
=== ep: 1757, time 27.366390705108643, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1757
=== ep: 1758, time 28.134021997451782, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1758
=== ep: 1759, time 66.03399181365967, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1759
=== ep: 1760, time 28.870096683502197, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1760
=== ep: 1761, time 28.261815547943115, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1761
=== ep: 1762, time 26.944196939468384, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1762
=== ep: 1763, time 29.54626750946045, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1763
=== ep: 1764, time 30.062847137451172, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1764
=== ep: 1765, time 28.62071967124939, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1765
goal_identified
=== ep: 1766, time 28.986042737960815, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1766
=== ep: 1767, time 27.57973289489746, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1767
=== ep: 1768, time 29.387019872665405, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1768
=== ep: 1769, time 87.11100220680237, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1769
=== ep: 1770, time 29.081127643585205, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1770
=== ep: 1771, time 27.877191066741943, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1771
=== ep: 1772, time 32.69035577774048, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1772
=== ep: 1773, time 28.784536600112915, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1773
=== ep: 1774, time 30.56910538673401, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1774
=== ep: 1775, time 30.693668603897095, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1775
=== ep: 1776, time 29.15204930305481, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1776
goal_identified
=== ep: 1777, time 32.39830279350281, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1777
=== ep: 1778, time 38.27341365814209, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1778
=== ep: 1779, time 71.62072014808655, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1779
=== ep: 1780, time 28.026435136795044, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1780
=== ep: 1781, time 37.84209895133972, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1781
=== ep: 1782, time 27.76638889312744, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1782
=== ep: 1783, time 27.994871616363525, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1783
=== ep: 1784, time 44.29831671714783, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1784
goal_identified
goal_identified
=== ep: 1785, time 28.108917951583862, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 9
=== ep: 1786, time 40.16876411437988, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1786
=== ep: 1787, time 28.472036600112915, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1787
=== ep: 1788, time 27.281704425811768, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1788
=== ep: 1789, time 78.92948913574219, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1789
=== ep: 1790, time 27.472516298294067, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1790
=== ep: 1791, time 27.362746953964233, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1791
goal_identified
=== ep: 1792, time 29.571871519088745, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1792
=== ep: 1793, time 28.698362350463867, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1793
=== ep: 1794, time 28.5623517036438, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1794
goal_identified
=== ep: 1795, time 26.580442428588867, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1795
=== ep: 1796, time 28.425770044326782, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1796
=== ep: 1797, time 27.92496371269226, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1797
=== ep: 1798, time 31.552116870880127, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1798
=== ep: 1799, time 80.07703757286072, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1799
goal_identified
=== ep: 1800, time 28.057793617248535, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1800
=== ep: 1801, time 27.770482778549194, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1801
=== ep: 1802, time 25.801087617874146, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1802
=== ep: 1803, time 29.43719244003296, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1803
=== ep: 1804, time 26.876050233840942, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1804
goal_identified
=== ep: 1805, time 26.733875036239624, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1805
=== ep: 1806, time 27.93496537208557, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1806
=== ep: 1807, time 28.23270320892334, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1807
=== ep: 1808, time 29.131386518478394, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1808
=== ep: 1809, time 82.34569358825684, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1809
=== ep: 1810, time 28.132599592208862, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1810
=== ep: 1811, time 27.01054811477661, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1811
=== ep: 1812, time 37.106765270233154, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1812
goal_identified
=== ep: 1813, time 27.52069664001465, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1813
=== ep: 1814, time 27.68222737312317, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1814
=== ep: 1815, time 28.286144971847534, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1815
=== ep: 1816, time 28.50939130783081, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1816
=== ep: 1817, time 27.62178325653076, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1817
=== ep: 1818, time 27.85188865661621, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1818
=== ep: 1819, time 59.97814702987671, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1819
=== ep: 1820, time 32.232606649398804, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1820
=== ep: 1821, time 29.499741315841675, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1821
=== ep: 1822, time 29.316842794418335, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1822
=== ep: 1823, time 27.956158876419067, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1823
goal_identified
=== ep: 1824, time 27.585357904434204, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1824
=== ep: 1825, time 31.64001441001892, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1825
=== ep: 1826, time 28.524675846099854, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1826
=== ep: 1827, time 27.416468143463135, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1827
=== ep: 1828, time 27.49614977836609, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1828
=== ep: 1829, time 85.66746854782104, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1829
=== ep: 1830, time 27.827189922332764, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1830
=== ep: 1831, time 28.790353536605835, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1831
=== ep: 1832, time 29.97541379928589, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1832
=== ep: 1833, time 29.966785192489624, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1833
=== ep: 1834, time 27.409879446029663, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1834
=== ep: 1835, time 28.063098669052124, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1835
=== ep: 1836, time 28.082823514938354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1836
=== ep: 1837, time 27.135600090026855, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1837
=== ep: 1838, time 31.07854652404785, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1838
=== ep: 1839, time 71.7315936088562, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1839
=== ep: 1840, time 28.195427894592285, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1840
=== ep: 1841, time 27.47694444656372, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1841
=== ep: 1842, time 27.50437569618225, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1842
=== ep: 1843, time 35.919835329055786, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1843
=== ep: 1844, time 39.56450271606445, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1844
=== ep: 1845, time 30.827099561691284, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1845
=== ep: 1846, time 29.41236448287964, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1846
=== ep: 1847, time 29.444265604019165, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1847
=== ep: 1848, time 27.84694242477417, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1848
goal_identified
goal_identified
=== ep: 1849, time 68.33935570716858, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 10
=== ep: 1850, time 27.380179405212402, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1850
goal_identified
=== ep: 1851, time 28.371705770492554, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1851
=== ep: 1852, time 31.800148963928223, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1852
goal_identified
=== ep: 1853, time 29.296741008758545, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1853
goal_identified
=== ep: 1854, time 28.199899196624756, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1854
=== ep: 1855, time 29.887620449066162, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1855
=== ep: 1856, time 36.619540214538574, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1856
=== ep: 1857, time 27.238874435424805, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1857
goal_identified
=== ep: 1858, time 27.232987880706787, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1858
=== ep: 1859, time 87.70937013626099, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1859
=== ep: 1860, time 29.015014171600342, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1860
=== ep: 1861, time 26.16014838218689, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1861
=== ep: 1862, time 39.127779483795166, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1862
=== ep: 1863, time 27.597203731536865, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1863
=== ep: 1864, time 27.57861089706421, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1864
=== ep: 1865, time 29.15970540046692, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1865
=== ep: 1866, time 27.985554456710815, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1866
=== ep: 1867, time 28.0273756980896, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1867
=== ep: 1868, time 32.87356233596802, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1868
goal_identified
=== ep: 1869, time 79.71457123756409, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1869
=== ep: 1870, time 27.545291662216187, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1870
=== ep: 1871, time 28.460192441940308, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1871
=== ep: 1872, time 28.252300024032593, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1872
=== ep: 1873, time 27.503108501434326, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1873
=== ep: 1874, time 28.457146406173706, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1874
=== ep: 1875, time 26.898860931396484, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1875
=== ep: 1876, time 27.5765483379364, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1876
goal_identified
=== ep: 1877, time 29.92759871482849, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1877
goal_identified
=== ep: 1878, time 28.8043954372406, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1878
=== ep: 1879, time 86.4557421207428, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1879
=== ep: 1880, time 27.73714280128479, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1880
=== ep: 1881, time 28.119537353515625, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1881
=== ep: 1882, time 28.376031160354614, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1882
=== ep: 1883, time 36.279425382614136, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1883
=== ep: 1884, time 28.106098413467407, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1884
=== ep: 1885, time 28.145599365234375, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1885
=== ep: 1886, time 27.924274444580078, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1886
goal_identified
=== ep: 1887, time 28.10058903694153, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1887
=== ep: 1888, time 27.84713053703308, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1888
=== ep: 1889, time 84.1345443725586, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1889
=== ep: 1890, time 29.34540104866028, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1890
=== ep: 1891, time 27.93552017211914, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1891
goal_identified
=== ep: 1892, time 30.22804856300354, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1892
=== ep: 1893, time 26.13475251197815, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1893
=== ep: 1894, time 27.514195919036865, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1894
=== ep: 1895, time 27.97007703781128, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1895
=== ep: 1896, time 28.611518144607544, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1896
=== ep: 1897, time 38.70820140838623, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1897
=== ep: 1898, time 25.2047119140625, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1898
=== ep: 1899, time 79.69138979911804, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1899
=== ep: 1900, time 29.840704679489136, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1900
=== ep: 1901, time 37.12061262130737, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1901
=== ep: 1902, time 28.36238431930542, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1902
=== ep: 1903, time 28.09016180038452, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1903
=== ep: 1904, time 30.99085021018982, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1904
=== ep: 1905, time 28.25355863571167, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1905
=== ep: 1906, time 27.232990980148315, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1906
=== ep: 1907, time 31.64872455596924, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1907
=== ep: 1908, time 27.880176782608032, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1908
goal_identified
=== ep: 1909, time 81.89457273483276, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1909
goal_identified
=== ep: 1910, time 30.069835901260376, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1910
=== ep: 1911, time 29.21014094352722, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1911
=== ep: 1912, time 29.96146249771118, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1912
=== ep: 1913, time 40.39716029167175, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1913
=== ep: 1914, time 28.844629049301147, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1914
goal_identified
=== ep: 1915, time 28.234211444854736, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1915
=== ep: 1916, time 27.55203604698181, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1916
=== ep: 1917, time 34.3850576877594, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1917
=== ep: 1918, time 28.915332078933716, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1918
=== ep: 1919, time 80.92312479019165, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1919
=== ep: 1920, time 28.17876124382019, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1920
=== ep: 1921, time 29.233691930770874, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1921
goal_identified
=== ep: 1922, time 30.4206645488739, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1922
=== ep: 1923, time 30.054389238357544, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1923
=== ep: 1924, time 30.073024034500122, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1924
=== ep: 1925, time 27.23988628387451, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1925
=== ep: 1926, time 28.132931232452393, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1926
=== ep: 1927, time 27.90524387359619, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1927
=== ep: 1928, time 33.97940945625305, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1928
=== ep: 1929, time 65.45355796813965, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1929
=== ep: 1930, time 28.760417461395264, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1930
=== ep: 1931, time 27.124690294265747, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1931
=== ep: 1932, time 29.0782208442688, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1932
=== ep: 1933, time 35.9951696395874, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1933
=== ep: 1934, time 28.76769781112671, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1934
=== ep: 1935, time 28.604353666305542, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1935
=== ep: 1936, time 32.867132902145386, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1936
=== ep: 1937, time 29.84412980079651, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1937
=== ep: 1938, time 37.623157024383545, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1938
=== ep: 1939, time 93.6925323009491, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1939
=== ep: 1940, time 27.00438141822815, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1940
=== ep: 1941, time 28.140817403793335, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1941
=== ep: 1942, time 27.910524129867554, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1942
=== ep: 1943, time 28.179421186447144, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1943
=== ep: 1944, time 28.167110204696655, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1944
=== ep: 1945, time 30.630325317382812, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1945
goal_identified
=== ep: 1946, time 29.58090090751648, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1946
=== ep: 1947, time 29.44005537033081, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1947
=== ep: 1948, time 27.845125198364258, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1948
=== ep: 1949, time 78.87643241882324, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1949
=== ep: 1950, time 27.386279344558716, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1950
=== ep: 1951, time 27.44314408302307, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1951
=== ep: 1952, time 27.43666362762451, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1952
=== ep: 1953, time 27.6581974029541, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1953
goal_identified
=== ep: 1954, time 27.95608162879944, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1954
=== ep: 1955, time 29.374921798706055, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1955
=== ep: 1956, time 38.33561062812805, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1956
=== ep: 1957, time 28.539619207382202, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1957
=== ep: 1958, time 28.194643259048462, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1958
=== ep: 1959, time 75.21895575523376, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1959
goal_identified
=== ep: 1960, time 27.937971115112305, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1960
=== ep: 1961, time 30.980074167251587, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1961
=== ep: 1962, time 35.548409938812256, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1962
=== ep: 1963, time 29.044358491897583, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1963
=== ep: 1964, time 27.626444578170776, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1964
=== ep: 1965, time 28.644420862197876, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1965
=== ep: 1966, time 27.216398000717163, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1966
=== ep: 1967, time 27.97776508331299, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1967
=== ep: 1968, time 30.037757873535156, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1968
=== ep: 1969, time 95.02460312843323, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1969
=== ep: 1970, time 28.21891164779663, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1970
=== ep: 1971, time 35.16102933883667, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1971
=== ep: 1972, time 28.0345938205719, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1972
=== ep: 1973, time 32.979076862335205, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1973
=== ep: 1974, time 32.51519012451172, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1974
=== ep: 1975, time 27.84756326675415, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1975
=== ep: 1976, time 37.88198971748352, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1976
goal_identified
=== ep: 1977, time 28.576861143112183, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1977
=== ep: 1978, time 28.855241775512695, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1978
=== ep: 1979, time 93.8942129611969, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1979
=== ep: 1980, time 28.735934734344482, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1980
=== ep: 1981, time 29.081199407577515, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1981
=== ep: 1982, time 32.654563903808594, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1982
goal_identified
=== ep: 1983, time 28.135960340499878, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1983
=== ep: 1984, time 27.46615695953369, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1984
=== ep: 1985, time 28.001418828964233, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1985
=== ep: 1986, time 28.775904893875122, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1986
=== ep: 1987, time 29.524498224258423, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1987
=== ep: 1988, time 28.99532151222229, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1988
=== ep: 1989, time 90.85151934623718, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1989
=== ep: 1990, time 30.294607400894165, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1990
=== ep: 1991, time 29.59321665763855, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1991
=== ep: 1992, time 28.24884843826294, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1992
=== ep: 1993, time 33.86097025871277, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1993
=== ep: 1994, time 28.692603826522827, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1994
=== ep: 1995, time 32.576343059539795, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1995
=== ep: 1996, time 30.5860435962677, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1996
=== ep: 1997, time 32.45316481590271, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1997
=== ep: 1998, time 27.941805124282837, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1998
goal_identified
=== ep: 1999, time 73.82967710494995, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1999
goal_identified
=== ep: 2000, time 30.76987051963806, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2000
goal_identified
=== ep: 2001, time 29.114510536193848, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2001
=== ep: 2002, time 34.48124837875366, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2002
=== ep: 2003, time 30.669944524765015, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2003
=== ep: 2004, time 29.930800676345825, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2004
=== ep: 2005, time 27.95853066444397, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2005
=== ep: 2006, time 27.696993112564087, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2006
=== ep: 2007, time 27.467466592788696, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2007
=== ep: 2008, time 28.437216997146606, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2008
=== ep: 2009, time 86.33503437042236, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2009
goal_identified
=== ep: 2010, time 28.470354080200195, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2010
=== ep: 2011, time 28.348686456680298, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2011
=== ep: 2012, time 30.51462697982788, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2012
=== ep: 2013, time 29.146114587783813, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2013
goal_identified
=== ep: 2014, time 29.736999034881592, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2014
=== ep: 2015, time 27.890297651290894, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2015
goal_identified
=== ep: 2016, time 27.612791538238525, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2016
=== ep: 2017, time 28.227754831314087, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2017
=== ep: 2018, time 27.126343965530396, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2018
=== ep: 2019, time 75.00755381584167, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2019
=== ep: 2020, time 28.474316120147705, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2020
=== ep: 2021, time 28.40660834312439, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2021
=== ep: 2022, time 28.678598165512085, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2022
=== ep: 2023, time 34.348724365234375, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2023
=== ep: 2024, time 28.750614643096924, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2024
=== ep: 2025, time 31.233408451080322, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2025
goal_identified
=== ep: 2026, time 29.049301385879517, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2026
=== ep: 2027, time 30.75275206565857, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2027
=== ep: 2028, time 28.098978996276855, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2028
goal_identified
=== ep: 2029, time 89.9626989364624, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2029
=== ep: 2030, time 30.272729873657227, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2030
=== ep: 2031, time 27.89665174484253, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2031
=== ep: 2032, time 28.77847671508789, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2032
goal_identified
=== ep: 2033, time 38.56276535987854, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2033
=== ep: 2034, time 28.54560422897339, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2034
=== ep: 2035, time 29.013501167297363, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2035
=== ep: 2036, time 34.35154700279236, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2036
=== ep: 2037, time 29.833884477615356, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2037
=== ep: 2038, time 28.563960790634155, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2038
=== ep: 2039, time 73.35892987251282, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2039
=== ep: 2040, time 34.44143843650818, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2040
=== ep: 2041, time 40.999568700790405, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2041
=== ep: 2042, time 33.55340218544006, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2042
=== ep: 2043, time 27.734692335128784, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2043
=== ep: 2044, time 28.81510305404663, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2044
=== ep: 2045, time 28.28680682182312, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2045
=== ep: 2046, time 29.7294180393219, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2046
goal_identified
=== ep: 2047, time 31.137537956237793, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2047
=== ep: 2048, time 30.432499647140503, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2048
=== ep: 2049, time 78.52752757072449, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2049
=== ep: 2050, time 26.845593452453613, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2050
=== ep: 2051, time 27.609641790390015, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2051
=== ep: 2052, time 28.43671464920044, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2052
=== ep: 2053, time 36.840529918670654, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2053
=== ep: 2054, time 38.90856599807739, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2054
goal_identified
=== ep: 2055, time 28.577540636062622, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2055
=== ep: 2056, time 27.82104516029358, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2056
=== ep: 2057, time 30.560769081115723, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2057
goal_identified
=== ep: 2058, time 28.956313610076904, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2058
=== ep: 2059, time 83.74654078483582, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2059
=== ep: 2060, time 28.786770343780518, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2060
goal_identified
=== ep: 2061, time 27.735999584197998, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2061
goal_identified
=== ep: 2062, time 27.989809036254883, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2062
=== ep: 2063, time 30.102585554122925, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2063
=== ep: 2064, time 28.51144766807556, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2064
=== ep: 2065, time 27.460416316986084, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2065
=== ep: 2066, time 27.53830623626709, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2066
=== ep: 2067, time 27.834628343582153, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2067
=== ep: 2068, time 28.403788566589355, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2068
=== ep: 2069, time 89.0601909160614, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2069
=== ep: 2070, time 28.701465368270874, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2070
=== ep: 2071, time 29.542300939559937, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2071
=== ep: 2072, time 27.790656089782715, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2072
=== ep: 2073, time 29.543416500091553, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2073
=== ep: 2074, time 28.021387815475464, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2074
=== ep: 2075, time 30.43295383453369, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2075
=== ep: 2076, time 27.59325933456421, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2076
=== ep: 2077, time 28.452457666397095, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2077
=== ep: 2078, time 28.836508989334106, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2078
=== ep: 2079, time 107.0055570602417, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2079
=== ep: 2080, time 29.933255910873413, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2080
goal_identified
goal_identified
=== ep: 2081, time 28.832916975021362, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2081
=== ep: 2082, time 27.597670555114746, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2082
=== ep: 2083, time 28.508658409118652, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2083
=== ep: 2084, time 28.11364436149597, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2084
=== ep: 2085, time 27.135980367660522, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2085
=== ep: 2086, time 28.555766105651855, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2086
=== ep: 2087, time 34.34192419052124, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2087
=== ep: 2088, time 31.97750759124756, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2088
=== ep: 2089, time 69.75643563270569, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2089
=== ep: 2090, time 30.900736093521118, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2090
=== ep: 2091, time 28.63905620574951, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2091
=== ep: 2092, time 30.39958620071411, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2092
=== ep: 2093, time 29.247960805892944, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2093
=== ep: 2094, time 27.01202917098999, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2094
=== ep: 2095, time 28.1057345867157, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2095
=== ep: 2096, time 27.876606225967407, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2096
=== ep: 2097, time 28.130505561828613, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2097
=== ep: 2098, time 29.30467462539673, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2098
=== ep: 2099, time 68.33698868751526, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2099
=== ep: 2100, time 35.30067205429077, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2100
=== ep: 2101, time 28.06338334083557, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2101
goal_identified
=== ep: 2102, time 30.39513635635376, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2102
=== ep: 2103, time 29.190326690673828, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2103
=== ep: 2104, time 34.80045700073242, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2104
=== ep: 2105, time 29.766900300979614, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2105
=== ep: 2106, time 32.16317439079285, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2106
=== ep: 2107, time 29.43062973022461, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2107
=== ep: 2108, time 29.512397527694702, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2108
=== ep: 2109, time 85.70567393302917, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2109
=== ep: 2110, time 27.824262619018555, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2110
=== ep: 2111, time 28.621480464935303, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2111
=== ep: 2112, time 27.219313621520996, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2112
goal_identified
=== ep: 2113, time 28.686189889907837, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2113
=== ep: 2114, time 28.70261526107788, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2114
goal_identified
