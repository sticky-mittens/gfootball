==> Playing in 11_vs_11_easy_stochastic.
==>Level 1
==>OTs in this level are dict_keys(['win_game'])
==>Currently learning win_game
==>using device cuda
==>critic has 5 layers and 500 hidden units.
goal_identified
=== ep: 0, time 25.08857035636902, eps 0.9, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
goal_identified
goal_identified
=== ep: 1, time 24.25985884666443, eps 0.8561552526261419, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 2, time 24.588953256607056, eps 0.8144488388143276, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 3, time 24.99534034729004, eps 0.774776470806127, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 4, time 24.710089206695557, eps 0.7370389470171057, sum reward: 6, score_diff 5, tot learning steps 0 (total env steps 3001)
goal_identified
goal_identified
=== ep: 5, time 24.774230241775513, eps 0.701141903981193, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 6, time 25.273333311080933, eps 0.6669955803928644, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 7, time 24.958112478256226, eps 0.6345145926571234, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 8, time 24.99062490463257, eps 0.6036177213860398, sum reward: 5, score_diff 5, tot learning steps 0 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 9, time 41.284953355789185, eps 0.5742277083079742, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 10, time 24.903920888900757, eps 0.5462710630816575, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 11, time 25.14443063735962, eps 0.5196778795320575, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 12, time 25.104198455810547, eps 0.49438166084852986, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 13, time 25.21032977104187, eps 0.47031915330815344, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001)
goal_identified
goal_identified
=== ep: 14, time 24.694654941558838, eps 0.4474301881084772, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 15, time 24.82814645767212, eps 0.42565753091417224, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 16, time 25.34372305870056, eps 0.4049467387413822, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 17, time 25.27881622314453, eps 0.3852460238219053, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 18, time 25.383596420288086, eps 0.3665061241067986, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 19, time 53.68326544761658, eps 0.3486801800855966, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 11
goal_identified
goal_identified
goal_identified
=== ep: 20, time 25.747739553451538, eps 0.3317236176131267, sum reward: 3, score_diff 2, tot learning steps 0 (total env steps 3001)
/home/ksridhar/GRF/scripts/policies_baseline.py:437: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 21 > 20.0 and we are deleting ep 3
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 21, time 25.861446380615234, eps 0.31559403645092865, sum reward: 6, score_diff 6, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 6
goal_identified
=== ep: 22, time 25.79535460472107, eps 0.3002511042445735, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 7
goal_identified
goal_identified
=== ep: 23, time 25.924432516098022, eps 0.2856564556717689, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 0
goal_identified
=== ep: 24, time 25.918671131134033, eps 0.27177359650906974, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 12
goal_identified
goal_identified
=== ep: 25, time 25.744807720184326, eps 0.2585678123773109, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 15
goal_identified
goal_identified
=== ep: 26, time 26.02836298942566, eps 0.24600608193757734, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 16
goal_identified
=== ep: 27, time 26.186593294143677, eps 0.23405699432065646, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 18
goal_identified
goal_identified
=== ep: 28, time 26.113609790802002, eps 0.22269067058350425, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 19
goal_identified
=== ep: 29, time 56.943806171417236, eps 0.2118786889963241, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 29
goal_identified
goal_identified
goal_identified
=== ep: 30, time 25.861111640930176, eps 0.2015940139734384, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 22
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 31, time 26.053902864456177, eps 0.191810928470242, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 23
=== ep: 32, time 26.20797872543335, eps 0.1825049696771952, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 32
goal_identified
goal_identified
=== ep: 33, time 25.759829998016357, eps 0.17365286785005798, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 24
goal_identified
=== ep: 34, time 25.7133629322052, eps 0.16523248812340846, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 27
goal_identified
goal_identified
=== ep: 35, time 25.931723833084106, eps 0.15722277516195018, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 34
goal_identified
goal_identified
=== ep: 36, time 26.609278917312622, eps 0.1496037005112063, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 35
goal_identified
goal_identified
goal_identified
=== ep: 37, time 26.479209423065186, eps 0.14235621251595124, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1
goal_identified
goal_identified
=== ep: 38, time 25.91697096824646, eps 0.13546218868114893, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 38
goal_identified
goal_identified
=== ep: 39, time 57.81122183799744, eps 0.1289043903562757, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 39
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 40, time 25.691138982772827, eps 0.12266641962971482, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 5
goal_identified
goal_identified
=== ep: 41, time 25.9397189617157, eps 0.116732678325436, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 14
goal_identified
=== ep: 42, time 26.256251573562622, eps 0.11108832899943073, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 42
goal_identified
goal_identified
goal_identified
=== ep: 43, time 26.02709150314331, eps 0.10571925783837377, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 17
=== ep: 44, time 26.155031442642212, eps 0.10061203936773815, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 44
goal_identified
=== ep: 45, time 26.235110759735107, eps 0.09575390288111604, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 45
=== ep: 46, time 26.29513454437256, eps 0.09113270050680057, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 46
goal_identified
=== ep: 47, time 26.901960611343384, eps 0.08673687683177911, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 47
=== ep: 48, time 26.353660583496094, eps 0.08255544000718185, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 48
=== ep: 49, time 58.276785373687744, eps 0.07857793426293408, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 49
=== ep: 50, time 26.158238410949707, eps 0.07479441376288502, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 50
=== ep: 51, time 25.991730213165283, eps 0.0711954177350367, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 51
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 52, time 26.157679796218872, eps 0.06777194681468615, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 20
goal_identified
=== ep: 53, time 26.349265813827515, eps 0.06451544054132621, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 53
goal_identified
goal_identified
=== ep: 54, time 26.315584659576416, eps 0.06141775595303503, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 25
goal_identified
goal_identified
goal_identified
=== ep: 55, time 25.873443126678467, eps 0.05847114722483011, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 26
goal_identified
=== ep: 56, time 26.456610918045044, eps 0.05566824630007096, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 56
goal_identified
=== ep: 57, time 26.875284910202026, eps 0.05300204446647978, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 57
goal_identified
=== ep: 58, time 26.654375553131104, eps 0.050465874830710106, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 58
goal_identified
=== ep: 59, time 68.80022287368774, eps 0.04805339564764071, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 59
goal_identified
=== ep: 60, time 26.280918836593628, eps 0.045758574462709686, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 60
=== ep: 61, time 26.297282934188843, eps 0.043575673027635695, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 61
goal_identified
goal_identified
=== ep: 62, time 26.825217723846436, eps 0.04149923295180846, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 28
=== ep: 63, time 26.020123720169067, eps 0.03952406205346913, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 63
=== ep: 64, time 25.735401153564453, eps 0.03764522137655123, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 64
goal_identified
goal_identified
=== ep: 65, time 26.239553451538086, eps 0.03585801284071809, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 65
=== ep: 66, time 26.54680371284485, eps 0.034157967493714775, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 66
goal_identified
=== ep: 67, time 26.141683101654053, eps 0.03254083433665968, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 67
=== ep: 68, time 26.1476047039032, eps 0.031002569694333147, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 68
=== ep: 69, time 78.0547525882721, eps 0.02953932710388308, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 69
goal_identified
=== ep: 70, time 25.973296403884888, eps 0.028147447696664333, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 70
goal_identified
=== ep: 71, time 26.54708194732666, eps 0.026823451049161253, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 71
goal_identified
=== ep: 72, time 25.931145906448364, eps 0.025564026480116013, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 72
goal_identified
=== ep: 73, time 26.002536058425903, eps 0.02436602477210106, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 73
goal_identified
=== ep: 74, time 26.483250856399536, eps 0.02322645029683511, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 74
goal_identified
=== ep: 75, time 26.914244413375854, eps 0.02214245352455219, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 75
=== ep: 76, time 26.939820051193237, eps 0.02111132389869288, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 76
goal_identified
=== ep: 77, time 26.168925046920776, eps 0.020130483058101077, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 77
=== ep: 78, time 26.376731157302856, eps 0.019197478389778148, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 78
=== ep: 79, time 78.1760048866272, eps 0.018309976896072843, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 79
goal_identified
=== ep: 80, time 26.709898471832275, eps 0.017465759360972027, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 80
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 81, time 27.264418125152588, eps 0.01666271480090467, sum reward: 4, score_diff 4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 33
=== ep: 82, time 25.782346725463867, eps 0.015898835186183367, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 82
=== ep: 83, time 26.488961696624756, eps 0.015172210419884185, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 83
goal_identified
=== ep: 84, time 24.67179799079895, eps 0.014481023561609456, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 84
goal_identified
=== ep: 85, time 28.549819231033325, eps 0.01382354628419033, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 85
=== ep: 86, time 24.98840832710266, eps 0.013198134551968641, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 86
goal_identified
goal_identified
goal_identified
=== ep: 87, time 26.80525803565979, eps 0.012603224509851407, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 36
=== ep: 88, time 26.744519233703613, eps 0.012037328572858524, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 88
goal_identified
goal_identified
goal_identified
=== ep: 89, time 78.99525094032288, eps 0.011499031706385502, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 41
=== ep: 90, time 26.71747636795044, eps 0.010986987887879832, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 90
=== ep: 91, time 26.24952983856201, eps 0.010499916741083536, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 91
=== ep: 92, time 27.062707662582397, eps 0.010036600334425595, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 92
goal_identified
=== ep: 93, time 26.983423709869385, eps 0.00959588013555861, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 93
=== ep: 94, time 27.3861722946167, eps 0.009176654114424539, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 94
goal_identified
goal_identified
=== ep: 95, time 27.747647523880005, eps 0.00877787398760545, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 95
goal_identified
goal_identified
=== ep: 96, time 26.57241678237915, eps 0.008398542597069007, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 54
=== ep: 97, time 27.053189992904663, eps 0.008037711416753971, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 97
=== ep: 98, time 25.992383241653442, eps 0.00769447818076098, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 98
goal_identified
goal_identified
goal_identified
=== ep: 99, time 76.11118984222412, eps 0.007367984627217855, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 62
=== ep: 100, time 28.472723722457886, eps 0.007057414352177835, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 100
=== ep: 101, time 27.311622142791748, eps 0.006761990768184489, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 101
=== ep: 102, time 27.339187622070312, eps 0.006480975162398559, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 102
=== ep: 103, time 25.434693336486816, eps 0.006213664849431085, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 103
goal_identified
=== ep: 104, time 28.066303491592407, eps 0.005959391414263934, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 104
=== ep: 105, time 27.058870792388916, eps 0.005717519040864065, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 105
=== ep: 106, time 26.326851844787598, eps 0.005487442922312285, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 106
goal_identified
=== ep: 107, time 27.751713514328003, eps 0.005268587748470919, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 107
goal_identified
=== ep: 108, time 27.369486093521118, eps 0.005060406267408787, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 108
=== ep: 109, time 84.8752794265747, eps 0.004862377916986354, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 109
=== ep: 110, time 26.274561643600464, eps 0.004674007523179196, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 110
=== ep: 111, time 25.96854877471924, eps 0.004494824061885041, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 111
goal_identified
=== ep: 112, time 27.277066230773926, eps 0.0043243794811181555, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 112
=== ep: 113, time 27.82587742805481, eps 0.0041622475806460035, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 113
=== ep: 114, time 26.938201427459717, eps 0.0040080229462666735, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 114
=== ep: 115, time 27.576346158981323, eps 0.0038613199360621906, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 115
=== ep: 116, time 27.106906414031982, eps 0.003721771716092858, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 116
=== ep: 117, time 28.15246820449829, eps 0.0035890293431213305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 117
goal_identified
=== ep: 118, time 26.697794198989868, eps 0.0034627608920727634, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 118
=== ep: 119, time 82.80022597312927, eps 0.00334265062604924, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 119
=== ep: 120, time 27.583998680114746, eps 0.0032283982068230565, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 120
=== ep: 121, time 25.811134099960327, eps 0.0031197179438347193, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 121
=== ep: 122, time 26.76485514640808, eps 0.0030163380798177374, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 122
=== ep: 123, time 29.71331238746643, eps 0.0029180001112638996, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 123
=== ep: 124, time 28.8867290019989, eps 0.002824458142029865, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 124
=== ep: 125, time 27.39595055580139, eps 0.0027354782684687108, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 125
=== ep: 126, time 25.795721769332886, eps 0.0026508379945489875, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 126
=== ep: 127, time 26.931325674057007, eps 0.0025703256754987464, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 127
=== ep: 128, time 26.38895010948181, eps 0.0024937399885833667, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 128
=== ep: 129, time 86.11200451850891, eps 0.0024208894296938593, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 129
=== ep: 130, time 25.458239793777466, eps 0.0023515918344868374, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 130
goal_identified
=== ep: 131, time 27.185645580291748, eps 0.002285673922878779, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 131
=== ep: 132, time 30.15000867843628, eps 0.0022229708657555565, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 132
goal_identified
goal_identified
=== ep: 133, time 27.108721017837524, eps 0.0021633258728137976, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 133
=== ep: 134, time 25.936139345169067, eps 0.0021065898005034594, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 134
goal_identified
=== ep: 135, time 25.719815015792847, eps 0.002052620779091266, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 135
=== ep: 136, time 26.06944489479065, eps 0.0020012838579124784, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 136
=== ep: 137, time 27.56296420097351, eps 0.0019524506679239415, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 137
=== ep: 138, time 25.64237403869629, eps 0.001905999100714611, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 138
=== ep: 139, time 90.90908312797546, eps 0.001861813003170924, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 139
=== ep: 140, time 25.392545461654663, eps 0.0018197818870335101, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 140
=== ep: 141, time 28.542936086654663, eps 0.0017798006526189953, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 141
=== ep: 142, time 27.364989519119263, eps 0.0017417693260160481, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 142
=== ep: 143, time 27.602328062057495, eps 0.0017055928090985275, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 143
goal_identified
=== ep: 144, time 29.226369857788086, eps 0.0016711806417306348, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 144
=== ep: 145, time 24.905691385269165, eps 0.0016384467755694515, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 145
goal_identified
=== ep: 146, time 25.894712448120117, eps 0.0016073093588992661, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 146
=== ep: 147, time 26.09158444404602, eps 0.0015776905319596466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 147
=== ep: 148, time 28.06858468055725, eps 0.0015495162322554856, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 148
=== ep: 149, time 92.40127086639404, eps 0.0015227160093621863, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 149
=== ep: 150, time 27.361745357513428, eps 0.0014972228487629025, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 150
goal_identified
=== ep: 151, time 29.734362363815308, eps 0.0014729730042773413, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 151
=== ep: 152, time 28.175995588302612, eps 0.001449905838663109, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 152
=== ep: 153, time 27.84162950515747, eps 0.00142796367199102, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 153
=== ep: 154, time 29.151939153671265, eps 0.0014070916374152305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 154
=== ep: 155, time 25.588541269302368, eps 0.001387237543977543, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 155
goal_identified
=== ep: 156, time 27.75345253944397, eps 0.0013683517461028282, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 156
=== ep: 157, time 27.893743991851807, eps 0.0013503870194592265, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 157
=== ep: 158, time 24.530195713043213, eps 0.0013332984428727204, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 158
=== ep: 159, time 87.51784610748291, eps 0.001317043286000802, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 159
=== ep: 160, time 30.46352481842041, eps 0.0013015809024843582, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 160
=== ep: 161, time 27.464107036590576, eps 0.0012868726283106018, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 161
=== ep: 162, time 26.87471842765808, eps 0.0012728816851329014, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 162
=== ep: 163, time 28.190752506256104, eps 0.0012595730883057546, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 163
=== ep: 164, time 27.240610599517822, eps 0.001246913559404956, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 164
=== ep: 165, time 25.164164543151855, eps 0.0012348714430141991, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 165
=== ep: 166, time 26.148118019104004, eps 0.0012234166275700486, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 166
=== ep: 167, time 26.8542697429657, eps 0.001212520470067348, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 167
=== ep: 168, time 28.42362952232361, eps 0.0012021557244367845, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 168
=== ep: 169, time 95.3789291381836, eps 0.0011922964734155277, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 169
=== ep: 170, time 27.17074465751648, eps 0.001182918063740569, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 170
=== ep: 171, time 25.683770895004272, eps 0.0011739970445027263, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 171
=== ep: 172, time 28.06922435760498, eps 0.0011655111085071537, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 172
=== ep: 173, time 27.518394231796265, eps 0.001157439036493735, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 173
=== ep: 174, time 28.625155448913574, eps 0.0011497606440778825, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 174
=== ep: 175, time 28.963484048843384, eps 0.0011424567312790603, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 175
=== ep: 176, time 27.322821378707886, eps 0.0011355090345108335, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 176
=== ep: 177, time 27.85829997062683, eps 0.0011289001809123877, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 177
=== ep: 178, time 27.18403458595276, eps 0.0011226136449073282, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 178
=== ep: 179, time 108.32332491874695, eps 0.001116633706881133, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 179
=== ep: 180, time 27.318408727645874, eps 0.001110945413873925, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 180
=== ep: 181, time 27.511993646621704, eps 0.001105534542190287, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 181
=== ep: 182, time 27.135414361953735, eps 0.0011003875618326132, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 182
=== ep: 183, time 27.407925844192505, eps 0.0010954916026690664, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 183
=== ep: 184, time 29.71941065788269, eps 0.001090834422251547, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 184
=== ep: 185, time 28.192869424819946, eps 0.0010864043752031938, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 185
=== ep: 186, time 27.85712766647339, eps 0.0010821903840988777, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 186
=== ep: 187, time 26.313698053359985, eps 0.0010781819117658682, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 187
=== ep: 188, time 27.127219676971436, eps 0.0010743689349354123, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 188
=== ep: 189, time 92.17407941818237, eps 0.0010707419191793434, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 189
=== ep: 190, time 28.97874355316162, eps 0.0010672917950690429, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 190
=== ep: 191, time 23.827255725860596, eps 0.0010640099354971456, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 191
=== ep: 192, time 25.550848484039307, eps 0.0010608881341052777, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 192
=== ep: 193, time 27.503629207611084, eps 0.0010579185847638855, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 193
=== ep: 194, time 26.723989486694336, eps 0.0010550938620528466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 194
=== ep: 195, time 25.40297818183899, eps 0.001052406902694051, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 195
=== ep: 196, time 28.256031274795532, eps 0.001049850987889527, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 196
=== ep: 197, time 25.922943115234375, eps 0.0010474197265209469, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 197
=== ep: 198, time 27.154406785964966, eps 0.0010451070391685015, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 198
=== ep: 199, time 101.30760598182678, eps 0.001042907142909185, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 199
goal_identified
=== ep: 200, time 25.120266437530518, eps 0.001040814536856474, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 200
=== ep: 201, time 25.971133947372437, eps 0.0010388239884052469, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 201
=== ep: 202, time 26.640473127365112, eps 0.0010369305201475454, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 202
=== ep: 203, time 24.842798233032227, eps 0.0010351293974264616, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 203
=== ep: 204, time 24.368043661117554, eps 0.00103341611649703, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 204
=== ep: 205, time 26.491477489471436, eps 0.0010317863932645186, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 205
=== ep: 206, time 27.559024810791016, eps 0.0010302361525719613, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 206
goal_identified
=== ep: 207, time 25.10535430908203, eps 0.0010287615180101426, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 207
=== ep: 208, time 29.002453804016113, eps 0.001027358802224555, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 208
=== ep: 209, time 84.37381553649902, eps 0.0010260244976950921, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 209
=== ep: 210, time 26.039942979812622, eps 0.0010247552679654227, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 210
=== ep: 211, time 26.26310682296753, eps 0.00102354793930011, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 211
=== ep: 212, time 25.53329586982727, eps 0.0010223994927486214, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 212
goal_identified
=== ep: 213, time 25.723224401474, eps 0.001021307056596379, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 213
=== ep: 214, time 27.261985778808594, eps 0.0010202678991839778, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 214
=== ep: 215, time 24.475470781326294, eps 0.0010192794220766138, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 215
=== ep: 216, time 26.17703676223755, eps 0.0010183391535666436, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 216
=== ep: 217, time 25.7532217502594, eps 0.0010174447424930286, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 217
=== ep: 218, time 28.374122858047485, eps 0.0010165939523622068, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 218
=== ep: 219, time 86.69886207580566, eps 0.0010157846557556941, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 219
=== ep: 220, time 24.919371366500854, eps 0.001015014829010431, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 220
=== ep: 221, time 25.91749119758606, eps 0.0010142825471585687, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 221
=== ep: 222, time 26.530131101608276, eps 0.0010135859791140496, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 222
=== ep: 223, time 25.2361843585968, eps 0.0010129233830939361, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 223
=== ep: 224, time 26.724207878112793, eps 0.0010122931022630473, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 224
=== ep: 225, time 25.20403265953064, eps 0.001011693560591007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 225
=== ep: 226, time 26.082486867904663, eps 0.0010111232589113477, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 226
=== ep: 227, time 27.126557111740112, eps 0.0010105807711728136, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 227
=== ep: 228, time 28.830501317977905, eps 0.0010100647408734893, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 228
=== ep: 229, time 94.10175633430481, eps 0.001009573877668838, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 229
=== ep: 230, time 26.654141187667847, eps 0.001009106954145169, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 230
goal_identified
goal_identified
=== ep: 231, time 27.62470054626465, eps 0.0010086628027504636, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 96
=== ep: 232, time 25.860840320587158, eps 0.0010082403128748867, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 232
=== ep: 233, time 26.030218839645386, eps 0.0010078384280736842, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 233
=== ep: 234, time 25.633435010910034, eps 0.001007456143425521, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 234
=== ep: 235, time 25.519314765930176, eps 0.001007092503019653, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 235
=== ep: 236, time 26.251800298690796, eps 0.001006746597565654, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 236
=== ep: 237, time 30.221731424331665, eps 0.001006417562119715, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 237
=== ep: 238, time 27.845614194869995, eps 0.0010061045739218342, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 238
=== ep: 239, time 91.37601399421692, eps 0.0010058068503384884, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 239
goal_identified
=== ep: 240, time 25.06320858001709, eps 0.001005523646905642, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 240
=== ep: 241, time 27.60270071029663, eps 0.001005254255467199, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 241
=== ep: 242, time 25.821364164352417, eps 0.0010049980024042435, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 242
=== ep: 243, time 26.830564975738525, eps 0.0010047542469506416, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 243
=== ep: 244, time 26.429437398910522, eps 0.0010045223795907931, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 244
=== ep: 245, time 26.79821491241455, eps 0.001004301820535524, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 245
=== ep: 246, time 27.18031597137451, eps 0.0010040920182723119, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 246
=== ep: 247, time 25.919705867767334, eps 0.0010038924481862177, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 247
goal_identified
=== ep: 248, time 26.926530122756958, eps 0.0010037026112480747, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 248
=== ep: 249, time 96.85758686065674, eps 0.0010035220327666559, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 249
goal_identified
=== ep: 250, time 26.31657075881958, eps 0.0010033502612016988, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 250
=== ep: 251, time 28.83235740661621, eps 0.001003186867034819, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 251
=== ep: 252, time 24.955528736114502, eps 0.001003031441695491, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 252
=== ep: 253, time 27.905481815338135, eps 0.0010028835965394094, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 253
=== ep: 254, time 29.23502230644226, eps 0.0010027429618766747, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 254
=== ep: 255, time 27.349119663238525, eps 0.0010026091860473767, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 255
=== ep: 256, time 28.568726778030396, eps 0.0010024819345422614, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 256
=== ep: 257, time 26.08870506286621, eps 0.0010023608891662839, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 257
=== ep: 258, time 24.446727991104126, eps 0.001002245747242954, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 258
=== ep: 259, time 102.9738175868988, eps 0.0010021362208574892, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 259
=== ep: 260, time 24.781209468841553, eps 0.001002032036136876, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 260
=== ep: 261, time 26.3304443359375, eps 0.0010019329325650452, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 261
=== ep: 262, time 25.275833129882812, eps 0.0010018386623314465, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 262
=== ep: 263, time 23.666285514831543, eps 0.0010017489897113931, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 263
goal_identified
=== ep: 264, time 25.67698836326599, eps 0.0010016636904766263, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 264
=== ep: 265, time 25.409770250320435, eps 0.0010015825513346283, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 265
=== ep: 266, time 25.867467403411865, eps 0.0010015053693952815, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 266
=== ep: 267, time 25.983452320098877, eps 0.0010014319516635345, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 267
=== ep: 268, time 24.7888445854187, eps 0.0010013621145568167, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 268
=== ep: 269, time 95.05796837806702, eps 0.0010012956834459848, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 269
goal_identified
=== ep: 270, time 27.55794668197632, eps 0.0010012324922186594, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 270
goal_identified
=== ep: 271, time 26.800291776657104, eps 0.001001172382863857, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 271
goal_identified
=== ep: 272, time 25.77475643157959, eps 0.0010011152050768812, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 272
=== ep: 273, time 28.091878175735474, eps 0.0010010608158834819, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 273
=== ep: 274, time 29.312906980514526, eps 0.0010010090792823456, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 274
=== ep: 275, time 26.696491241455078, eps 0.0010009598659050213, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 275
=== ep: 276, time 24.94240713119507, eps 0.0010009130526924313, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 276
=== ep: 277, time 23.38304305076599, eps 0.0010008685225871602, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 277
=== ep: 278, time 27.632699728012085, eps 0.0010008261642407504, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 278
=== ep: 279, time 85.1790223121643, eps 0.001000785871735272, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 279
=== ep: 280, time 26.365890502929688, eps 0.0010007475443184742, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 280
=== ep: 281, time 24.879822492599487, eps 0.001000711086151851, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 281
=== ep: 282, time 27.304407596588135, eps 0.0010006764060709957, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 282
=== ep: 283, time 25.01063370704651, eps 0.001000643417357642, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 283
=== ep: 284, time 27.14967393875122, eps 0.0010006120375228235, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 284
=== ep: 285, time 26.459980249404907, eps 0.0010005821881006083, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 285
=== ep: 286, time 25.899486303329468, eps 0.0010005537944518927, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 286
=== ep: 287, time 24.307331085205078, eps 0.0010005267855777657, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 287
=== ep: 288, time 28.12799882888794, eps 0.0010005010939419733, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 288
=== ep: 289, time 86.41688013076782, eps 0.001000476655302044, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 289
goal_identified
=== ep: 290, time 25.817158460617065, eps 0.0010004534085486486, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 290
=== ep: 291, time 26.937771320343018, eps 0.0010004312955527947, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 291
goal_identified
goal_identified
=== ep: 292, time 26.266356468200684, eps 0.0010004102610204745, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 231
goal_identified
goal_identified
=== ep: 293, time 24.326920747756958, eps 0.0010003902523544011, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 293
=== ep: 294, time 23.93727159500122, eps 0.0010003712195224871, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 294
=== ep: 295, time 25.603418827056885, eps 0.0010003531149327387, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 295
=== ep: 296, time 28.40119194984436, eps 0.0010003358933142518, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 296
goal_identified
=== ep: 297, time 25.3586642742157, eps 0.0010003195116040093, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 297
=== ep: 298, time 30.182223081588745, eps 0.0010003039288392032, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 298
goal_identified
goal_identified
goal_identified
=== ep: 299, time 104.67874121665955, eps 0.0010002891060548044, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 292
=== ep: 300, time 25.340102672576904, eps 0.0010002750061861312, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 300
=== ep: 301, time 26.913377285003662, eps 0.0010002615939761676, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 301
=== ep: 302, time 25.876336336135864, eps 0.001000248835887403, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 302
=== ep: 303, time 23.462465047836304, eps 0.0010002367000179694, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 303
=== ep: 304, time 25.142723560333252, eps 0.0010002251560218723, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 304
=== ep: 305, time 24.827985048294067, eps 0.0010002141750331084, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 305
=== ep: 306, time 27.150702953338623, eps 0.0010002037295934862, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 306
=== ep: 307, time 25.276803493499756, eps 0.0010001937935839656, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 307
=== ep: 308, time 28.27099061012268, eps 0.0010001843421593476, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 308
=== ep: 309, time 106.12853741645813, eps 0.0010001753516861473, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 309
=== ep: 310, time 26.896435976028442, eps 0.0010001667996834991, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 310
=== ep: 311, time 24.156453609466553, eps 0.001000158664766942, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 311
=== ep: 312, time 27.35048222541809, eps 0.0010001509265949466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 312
=== ep: 313, time 24.739529848098755, eps 0.001000143565818053, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 313
=== ep: 314, time 26.355973482131958, eps 0.0010001365640304844, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 314
=== ep: 315, time 26.68160605430603, eps 0.0010001299037241253, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 315
=== ep: 316, time 24.748011589050293, eps 0.0010001235682447402, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 316
=== ep: 317, time 25.34351348876953, eps 0.0010001175417503308, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 317
=== ep: 318, time 25.157411575317383, eps 0.0010001118091715218, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 318
=== ep: 319, time 95.90986442565918, eps 0.0010001063561738807, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 319
=== ep: 320, time 30.607672214508057, eps 0.0010001011691220727, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 320
=== ep: 321, time 27.73172640800476, eps 0.0010000962350457665, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 321
=== ep: 322, time 24.532238960266113, eps 0.0010000915416072012, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 322
=== ep: 323, time 25.643290519714355, eps 0.0010000870770703358, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 323
=== ep: 324, time 24.534127712249756, eps 0.0010000828302715028, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 324
=== ep: 325, time 24.415403842926025, eps 0.0010000787905914928, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 325
=== ep: 326, time 25.76694941520691, eps 0.0010000749479290019, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 326
=== ep: 327, time 25.292274236679077, eps 0.001000071292675372, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 327
=== ep: 328, time 24.88472294807434, eps 0.001000067815690565, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 328
=== ep: 329, time 107.98711943626404, eps 0.0010000645082803084, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 329
=== ep: 330, time 25.92805242538452, eps 0.0010000613621743532, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 330
=== ep: 331, time 25.4581298828125, eps 0.0010000583695057963, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 331
=== ep: 332, time 27.378652811050415, eps 0.0010000555227914069, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 332
=== ep: 333, time 26.915624141693115, eps 0.0010000528149129166, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 333
goal_identified
=== ep: 334, time 23.617767810821533, eps 0.0010000502390992187, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 334
=== ep: 335, time 26.274372577667236, eps 0.0010000477889094373, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 335
=== ep: 336, time 26.727614879608154, eps 0.0010000454582168217, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 336
=== ep: 337, time 23.445024013519287, eps 0.001000043241193426, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 337
=== ep: 338, time 25.31036901473999, eps 0.0010000411322955373, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 338
goal_identified
=== ep: 339, time 101.01524567604065, eps 0.0010000391262498123, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 339
=== ep: 340, time 25.51323390007019, eps 0.001000037218040092, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 340
=== ep: 341, time 23.740464687347412, eps 0.0010000354028948577, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 341
=== ep: 342, time 24.47696852684021, eps 0.0010000336762753012, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 342
goal_identified
=== ep: 343, time 26.147104263305664, eps 0.001000032033863974, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 343
=== ep: 344, time 25.804193019866943, eps 0.0010000304715539925, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 344
=== ep: 345, time 26.202319860458374, eps 0.001000028985438768, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 345
=== ep: 346, time 29.221479892730713, eps 0.001000027571802238, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 346
=== ep: 347, time 28.01771569252014, eps 0.0010000262271095755, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 347
=== ep: 348, time 28.687637090682983, eps 0.0010000249479983478, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 348
=== ep: 349, time 91.18137240409851, eps 0.0010000237312701107, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 349
=== ep: 350, time 26.154552698135376, eps 0.00100002257388241, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 350
goal_identified
=== ep: 351, time 26.504252433776855, eps 0.0010000214729411737, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 351
=== ep: 352, time 24.18159008026123, eps 0.0010000204256934752, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 352
=== ep: 353, time 24.59424352645874, eps 0.0010000194295206493, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 353
=== ep: 354, time 24.71608257293701, eps 0.0010000184819317455, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 354
=== ep: 355, time 25.247570037841797, eps 0.001000017580557298, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 355
=== ep: 356, time 26.206777572631836, eps 0.001000016723143401, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 356
=== ep: 357, time 27.08441686630249, eps 0.0010000159075460732, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 357
=== ep: 358, time 24.54614758491516, eps 0.0010000151317258964, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 358
=== ep: 359, time 96.7364559173584, eps 0.0010000143937429161, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 359
=== ep: 360, time 26.351027727127075, eps 0.0010000136917517905, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 360
goal_identified
goal_identified
=== ep: 361, time 25.223787546157837, eps 0.001000013023997176, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 361
=== ep: 362, time 25.986158847808838, eps 0.0010000123888093385, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 362
=== ep: 363, time 24.54414939880371, eps 0.0010000117845999773, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 363
=== ep: 364, time 25.818591833114624, eps 0.0010000112098582543, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 364
=== ep: 365, time 26.63675832748413, eps 0.001000010663147016, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 365
=== ep: 366, time 27.290088176727295, eps 0.0010000101430991996, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 366
=== ep: 367, time 26.48133897781372, eps 0.0010000096484144142, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 367
=== ep: 368, time 25.334991455078125, eps 0.0010000091778556905, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 368
=== ep: 369, time 98.52115607261658, eps 0.0010000087302463867, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 369
=== ep: 370, time 25.38666844367981, eps 0.001000008304467246, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 370
=== ep: 371, time 25.275310516357422, eps 0.0010000078994535993, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 371
=== ep: 372, time 24.377752542495728, eps 0.0010000075141927012, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 372
=== ep: 373, time 26.221419095993042, eps 0.0010000071477211988, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 373
=== ep: 374, time 25.34866452217102, eps 0.0010000067991227223, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 374
=== ep: 375, time 27.535301446914673, eps 0.0010000064675255943, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 375
=== ep: 376, time 26.34141230583191, eps 0.001000006152100649, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 376
=== ep: 377, time 24.687938451766968, eps 0.0010000058520591598, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 377
=== ep: 378, time 25.238721132278442, eps 0.0010000055666508666, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 378
=== ep: 379, time 89.20617818832397, eps 0.0010000052951621003, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 379
goal_identified
=== ep: 380, time 29.01334500312805, eps 0.0010000050369139975, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 380
goal_identified
=== ep: 381, time 27.008068323135376, eps 0.001000004791260803, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 381
=== ep: 382, time 25.932721853256226, eps 0.0010000045575882562, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 382
goal_identified
=== ep: 383, time 25.642754793167114, eps 0.001000004335312054, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 383
=== ep: 384, time 25.079856395721436, eps 0.0010000041238763903, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 384
=== ep: 385, time 27.52196955680847, eps 0.0010000039227525655, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 385
=== ep: 386, time 26.04847002029419, eps 0.0010000037314376652, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 386
=== ep: 387, time 23.919499397277832, eps 0.001000003549453303, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 387
=== ep: 388, time 23.381012439727783, eps 0.0010000033763444226, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 388
=== ep: 389, time 99.84828901290894, eps 0.001000003211678162, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 389
=== ep: 390, time 25.143766164779663, eps 0.0010000030550427698, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 390
=== ep: 391, time 24.32206654548645, eps 0.0010000029060465757, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 391
goal_identified
=== ep: 392, time 25.57177472114563, eps 0.0010000027643170119, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 392
=== ep: 393, time 26.515533208847046, eps 0.0010000026294996803, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 393
=== ep: 394, time 25.610536575317383, eps 0.0010000025012574677, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 394
=== ep: 395, time 25.77990412712097, eps 0.0010000023792697014, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 395
=== ep: 396, time 27.04945945739746, eps 0.0010000022632313489, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 396
=== ep: 397, time 27.62109375, eps 0.0010000021528522535, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 397
=== ep: 398, time 25.405136585235596, eps 0.00100000204785641, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 398
=== ep: 399, time 99.95166182518005, eps 0.0010000019479812744, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 399
=== ep: 400, time 24.155579090118408, eps 0.0010000018529771066, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 400
goal_identified
goal_identified
goal_identified
=== ep: 401, time 25.515204191207886, eps 0.0010000017626063467, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2
goal_identified
=== ep: 402, time 25.560729026794434, eps 0.0010000016766430208, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 402
=== ep: 403, time 28.417842388153076, eps 0.0010000015948721758, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 403
=== ep: 404, time 26.795408725738525, eps 0.001000001517089342, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 404
=== ep: 405, time 23.11886978149414, eps 0.0010000014431000217, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 405
=== ep: 406, time 25.83178186416626, eps 0.001000001372719203, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 406
goal_identified
=== ep: 407, time 24.860251665115356, eps 0.0010000013057708975, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 407
goal_identified
=== ep: 408, time 24.972097158432007, eps 0.0010000012420876994, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 408
=== ep: 409, time 97.98227667808533, eps 0.0010000011815103674, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 409
=== ep: 410, time 26.262643814086914, eps 0.001000001123887427, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 410
=== ep: 411, time 25.118143558502197, eps 0.0010000010690747903, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 411
goal_identified
=== ep: 412, time 27.08761215209961, eps 0.0010000010169353975, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 412
=== ep: 413, time 27.94477152824402, eps 0.0010000009673388729, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 413
=== ep: 414, time 24.570915699005127, eps 0.0010000009201611994, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 414
goal_identified
=== ep: 415, time 26.500272512435913, eps 0.0010000008752844081, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 415
=== ep: 416, time 24.24132490158081, eps 0.0010000008325962838, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 416
=== ep: 417, time 26.7497456073761, eps 0.001000000791990084, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 417
=== ep: 418, time 24.830214262008667, eps 0.0010000007533642718, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 418
=== ep: 419, time 108.58366084098816, eps 0.0010000007166222626, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 419
=== ep: 420, time 25.106817960739136, eps 0.0010000006816721825, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 420
=== ep: 421, time 26.33937644958496, eps 0.001000000648426638, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 421
=== ep: 422, time 29.29002571105957, eps 0.0010000006168024976, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 422
=== ep: 423, time 26.537985801696777, eps 0.0010000005867206849, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 423
=== ep: 424, time 25.29971694946289, eps 0.0010000005581059794, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 424
=== ep: 425, time 25.662672758102417, eps 0.0010000005308868295, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 425
=== ep: 426, time 25.327019691467285, eps 0.0010000005049951733, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 426
=== ep: 427, time 26.618335962295532, eps 0.001000000480366268, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 427
=== ep: 428, time 26.900695323944092, eps 0.0010000004569385287, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 428
goal_identified
=== ep: 429, time 97.17793560028076, eps 0.0010000004346533736, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 429
=== ep: 430, time 26.246579885482788, eps 0.0010000004134550786, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 430
=== ep: 431, time 30.402318239212036, eps 0.0010000003932906364, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 431
=== ep: 432, time 26.398342609405518, eps 0.0010000003741096257, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 432
=== ep: 433, time 24.064466953277588, eps 0.001000000355864084, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 433
=== ep: 434, time 25.997287034988403, eps 0.0010000003385083878, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 434
=== ep: 435, time 25.923663854599, eps 0.001000000321999139, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 435
goal_identified
=== ep: 436, time 23.873520374298096, eps 0.0010000003062950555, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 436
=== ep: 437, time 23.558874130249023, eps 0.0010000002913568694, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 437
=== ep: 438, time 24.576143980026245, eps 0.0010000002771472273, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 438
=== ep: 439, time 101.04868078231812, eps 0.0010000002636305976, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 439
=== ep: 440, time 26.55462884902954, eps 0.0010000002507731815, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 440
=== ep: 441, time 31.611234426498413, eps 0.0010000002385428292, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 441
=== ep: 442, time 26.17995595932007, eps 0.0010000002269089582, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 442
=== ep: 443, time 24.823427200317383, eps 0.0010000002158424776, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 443
=== ep: 444, time 24.010042190551758, eps 0.0010000002053157158, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 444
=== ep: 445, time 26.37939143180847, eps 0.0010000001953023503, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 445
=== ep: 446, time 25.663862705230713, eps 0.001000000185777342, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 446
=== ep: 447, time 25.6740825176239, eps 0.0010000001767168742, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 447
=== ep: 448, time 25.838905334472656, eps 0.0010000001680982905, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 448
=== ep: 449, time 113.2713553905487, eps 0.0010000001599000403, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 449
=== ep: 450, time 24.421725034713745, eps 0.0010000001521016232, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 450
=== ep: 451, time 23.793989181518555, eps 0.0010000001446835395, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 451
=== ep: 452, time 26.394993782043457, eps 0.0010000001376272401, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 452
=== ep: 453, time 26.823997259140015, eps 0.0010000001309150804, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 453
=== ep: 454, time 24.012163877487183, eps 0.0010000001245302765, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 454
=== ep: 455, time 25.832618474960327, eps 0.0010000001184568633, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 455
=== ep: 456, time 24.695927381515503, eps 0.0010000001126796538, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 456
=== ep: 457, time 27.449743270874023, eps 0.0010000001071842023, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 457
=== ep: 458, time 25.449448823928833, eps 0.001000000101956767, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 458
=== ep: 459, time 109.64750170707703, eps 0.001000000096984277, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 459
goal_identified
=== ep: 460, time 26.11574935913086, eps 0.001000000092254298, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 460
=== ep: 461, time 26.141111612319946, eps 0.0010000000877550027, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 461
goal_identified
=== ep: 462, time 25.689846992492676, eps 0.0010000000834751407, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 462
=== ep: 463, time 25.84633755683899, eps 0.00100000007940401, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 463
=== ep: 464, time 26.061278820037842, eps 0.0010000000755314307, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 464
=== ep: 465, time 28.10547375679016, eps 0.0010000000718477194, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 465
=== ep: 466, time 25.321176052093506, eps 0.0010000000683436647, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 466
=== ep: 467, time 25.977331399917603, eps 0.001000000065010505, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 467
goal_identified
=== ep: 468, time 26.337926387786865, eps 0.0010000000618399052, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 468
=== ep: 469, time 115.23802590370178, eps 0.0010000000588239375, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 469
=== ep: 470, time 25.00309729576111, eps 0.0010000000559550603, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 470
=== ep: 471, time 25.866523265838623, eps 0.0010000000532260998, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 471
=== ep: 472, time 26.004439115524292, eps 0.0010000000506302322, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 472
=== ep: 473, time 28.139141082763672, eps 0.0010000000481609666, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 473
=== ep: 474, time 26.043776035308838, eps 0.0010000000458121286, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 474
=== ep: 475, time 26.916260719299316, eps 0.0010000000435778447, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 475
=== ep: 476, time 29.497230529785156, eps 0.001000000041452528, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 476
=== ep: 477, time 27.759551525115967, eps 0.0010000000394308644, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 477
=== ep: 478, time 24.88710331916809, eps 0.0010000000375077985, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 478
=== ep: 479, time 115.79907178878784, eps 0.0010000000356785216, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 479
=== ep: 480, time 25.05286455154419, eps 0.0010000000339384595, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 480
=== ep: 481, time 26.69088625907898, eps 0.0010000000322832614, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 481
=== ep: 482, time 27.091180086135864, eps 0.0010000000307087882, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 482
=== ep: 483, time 25.9474835395813, eps 0.001000000029211103, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 483
=== ep: 484, time 25.85644221305847, eps 0.0010000000277864607, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 484
=== ep: 485, time 26.21453070640564, eps 0.0010000000264312988, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 485
=== ep: 486, time 26.456618785858154, eps 0.0010000000251422292, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 486
goal_identified
=== ep: 487, time 25.851754426956177, eps 0.0010000000239160282, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 487
goal_identified
=== ep: 488, time 27.01271891593933, eps 0.00100000002274963, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 488
=== ep: 489, time 121.83475303649902, eps 0.0010000000216401172, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 489
=== ep: 490, time 24.462021350860596, eps 0.0010000000205847162, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 490
=== ep: 491, time 26.203315258026123, eps 0.0010000000195807877, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 491
=== ep: 492, time 27.67123532295227, eps 0.0010000000186258216, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 492
=== ep: 493, time 29.487733125686646, eps 0.0010000000177174295, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 493
=== ep: 494, time 23.401734352111816, eps 0.0010000000168533404, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 494
=== ep: 495, time 25.751023769378662, eps 0.0010000000160313932, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 495
=== ep: 496, time 25.46113896369934, eps 0.001000000015249533, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 496
=== ep: 497, time 24.230366706848145, eps 0.0010000000145058043, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 497
=== ep: 498, time 26.047969102859497, eps 0.001000000013798348, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 498
=== ep: 499, time 117.90374708175659, eps 0.0010000000131253947, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 499
=== ep: 500, time 24.92663860321045, eps 0.0010000000124852615, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 500
goal_identified
=== ep: 501, time 25.60864806175232, eps 0.0010000000118763482, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 501
=== ep: 502, time 27.837982177734375, eps 0.0010000000112971319, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 502
=== ep: 503, time 27.382543325424194, eps 0.0010000000107461642, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 503
=== ep: 504, time 24.86559534072876, eps 0.0010000000102220676, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 504
=== ep: 505, time 24.418461322784424, eps 0.0010000000097235315, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 505
=== ep: 506, time 34.863635540008545, eps 0.0010000000092493092, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 506
=== ep: 507, time 24.313830137252808, eps 0.0010000000087982152, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 507
=== ep: 508, time 29.991917848587036, eps 0.0010000000083691212, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 508
=== ep: 509, time 109.27267789840698, eps 0.0010000000079609542, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 509
=== ep: 510, time 26.54300546646118, eps 0.001000000007572694, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 510
=== ep: 511, time 23.93804693222046, eps 0.0010000000072033692, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 511
=== ep: 512, time 26.01025366783142, eps 0.001000000006852057, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 512
=== ep: 513, time 26.981761693954468, eps 0.001000000006517878, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 513
=== ep: 514, time 25.25727367401123, eps 0.0010000000061999974, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 514
=== ep: 515, time 27.59535241127014, eps 0.0010000000058976199, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 515
=== ep: 516, time 27.05498719215393, eps 0.0010000000056099897, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 516
=== ep: 517, time 26.46150040626526, eps 0.0010000000053363872, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 517
=== ep: 518, time 27.815311193466187, eps 0.0010000000050761286, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 518
goal_identified
=== ep: 519, time 98.78105974197388, eps 0.001000000004828563, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 519
=== ep: 520, time 25.431112051010132, eps 0.001000000004593071, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 520
goal_identified
=== ep: 521, time 27.677640199661255, eps 0.0010000000043690644, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 521
=== ep: 522, time 26.36747455596924, eps 0.0010000000041559827, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 522
=== ep: 523, time 24.585643529891968, eps 0.0010000000039532928, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 523
=== ep: 524, time 25.507815837860107, eps 0.0010000000037604885, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 524
=== ep: 525, time 26.800098657608032, eps 0.0010000000035770874, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 525
=== ep: 526, time 27.424696445465088, eps 0.0010000000034026306, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 526
=== ep: 527, time 27.44829559326172, eps 0.0010000000032366824, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 527
goal_identified
=== ep: 528, time 26.267430305480957, eps 0.0010000000030788276, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 528
=== ep: 529, time 111.83656311035156, eps 0.0010000000029286714, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 529
=== ep: 530, time 27.824361562728882, eps 0.0010000000027858384, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 530
=== ep: 531, time 26.492181539535522, eps 0.0010000000026499714, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 531
=== ep: 532, time 26.77988290786743, eps 0.0010000000025207308, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 532
=== ep: 533, time 25.5021812915802, eps 0.0010000000023977934, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 533
=== ep: 534, time 29.830624103546143, eps 0.0010000000022808515, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 534
=== ep: 535, time 27.520915269851685, eps 0.0010000000021696133, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 535
=== ep: 536, time 25.77049684524536, eps 0.0010000000020637999, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 536
=== ep: 537, time 26.176087141036987, eps 0.0010000000019631471, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 537
=== ep: 538, time 25.73802876472473, eps 0.0010000000018674034, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 538
=== ep: 539, time 105.07850050926208, eps 0.001000000001776329, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 539
=== ep: 540, time 25.187102556228638, eps 0.0010000000016896964, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 540
=== ep: 541, time 24.715198278427124, eps 0.001000000001607289, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 541
=== ep: 542, time 27.239765644073486, eps 0.0010000000015289005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 542
=== ep: 543, time 26.192670583724976, eps 0.0010000000014543352, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 543
goal_identified
=== ep: 544, time 29.504290342330933, eps 0.0010000000013834064, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 544
=== ep: 545, time 26.15047001838684, eps 0.001000000001315937, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 545
=== ep: 546, time 25.633169412612915, eps 0.0010000000012517578, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 546
=== ep: 547, time 24.86292290687561, eps 0.001000000001190709, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 547
=== ep: 548, time 25.984861373901367, eps 0.0010000000011326374, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 548
=== ep: 549, time 117.05965328216553, eps 0.001000000001077398, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 549
=== ep: 550, time 25.13720726966858, eps 0.0010000000010248527, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 550
=== ep: 551, time 26.001212120056152, eps 0.00100000000097487, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 551
goal_identified
=== ep: 552, time 27.240589141845703, eps 0.001000000000927325, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 552
=== ep: 553, time 28.620320320129395, eps 0.0010000000008820989, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 553
=== ep: 554, time 28.86285924911499, eps 0.0010000000008390784, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 554
=== ep: 555, time 25.541617155075073, eps 0.001000000000798156, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 555
=== ep: 556, time 27.411004304885864, eps 0.0010000000007592295, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 556
=== ep: 557, time 27.40609645843506, eps 0.0010000000007222014, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 557
=== ep: 558, time 25.36888313293457, eps 0.0010000000006869794, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 558
goal_identified
=== ep: 559, time 100.30478549003601, eps 0.001000000000653475, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 559
=== ep: 560, time 28.72834062576294, eps 0.0010000000006216046, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 560
=== ep: 561, time 27.276827096939087, eps 0.0010000000005912885, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 561
=== ep: 562, time 31.073163747787476, eps 0.0010000000005624511, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 562
=== ep: 563, time 24.887983798980713, eps 0.00100000000053502, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 563
=== ep: 564, time 28.034905195236206, eps 0.001000000000508927, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 564
=== ep: 565, time 24.924237728118896, eps 0.001000000000484106, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 565
=== ep: 566, time 27.590211868286133, eps 0.001000000000460496, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 566
=== ep: 567, time 26.592403888702393, eps 0.0010000000004380374, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 567
=== ep: 568, time 26.900263786315918, eps 0.001000000000416674, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 568
=== ep: 569, time 114.10546064376831, eps 0.0010000000003963527, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 569
=== ep: 570, time 26.975194215774536, eps 0.0010000000003770222, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 570
=== ep: 571, time 25.667144775390625, eps 0.0010000000003586346, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 571
=== ep: 572, time 28.197669982910156, eps 0.0010000000003411438, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 572
=== ep: 573, time 26.926390886306763, eps 0.001000000000324506, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 573
=== ep: 574, time 26.55381178855896, eps 0.0010000000003086798, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 574
=== ep: 575, time 23.69142246246338, eps 0.0010000000002936252, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 575
=== ep: 576, time 29.506409168243408, eps 0.001000000000279305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 576
=== ep: 577, time 26.1374773979187, eps 0.0010000000002656831, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 577
=== ep: 578, time 27.855743646621704, eps 0.0010000000002527256, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 578
=== ep: 579, time 105.65229773521423, eps 0.0010000000002404, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 579
goal_identified
=== ep: 580, time 27.43970274925232, eps 0.0010000000002286756, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 580
=== ep: 581, time 28.609979391098022, eps 0.0010000000002175229, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 581
=== ep: 582, time 27.40752124786377, eps 0.0010000000002069142, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 582
=== ep: 583, time 28.021061897277832, eps 0.0010000000001968228, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 583
goal_identified
=== ep: 584, time 27.130011320114136, eps 0.0010000000001872237, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 584
goal_identified
=== ep: 585, time 27.485987901687622, eps 0.0010000000001780928, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 585
=== ep: 586, time 26.34760594367981, eps 0.001000000000169407, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 586
=== ep: 587, time 25.405308485031128, eps 0.001000000000161145, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 587
=== ep: 588, time 27.46488332748413, eps 0.0010000000001532858, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 588
=== ep: 589, time 104.88069701194763, eps 0.00100000000014581, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 589
=== ep: 590, time 29.30591320991516, eps 0.0010000000001386988, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 590
=== ep: 591, time 27.95409369468689, eps 0.0010000000001319344, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 591
goal_identified
=== ep: 592, time 27.028127431869507, eps 0.0010000000001255, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 592
=== ep: 593, time 27.26360845565796, eps 0.0010000000001193791, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 593
=== ep: 594, time 24.81766104698181, eps 0.001000000000113557, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 594
=== ep: 595, time 27.619413137435913, eps 0.0010000000001080186, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 595
=== ep: 596, time 27.25676703453064, eps 0.0010000000001027505, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 596
=== ep: 597, time 26.82550311088562, eps 0.0010000000000977393, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 597
=== ep: 598, time 25.241775512695312, eps 0.0010000000000929725, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 598
=== ep: 599, time 115.2157998085022, eps 0.0010000000000884382, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 599
=== ep: 600, time 27.11211657524109, eps 0.001000000000084125, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 600
goal_identified
=== ep: 601, time 27.396944046020508, eps 0.0010000000000800222, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 601
=== ep: 602, time 28.320277452468872, eps 0.0010000000000761195, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 602
=== ep: 603, time 26.697659015655518, eps 0.0010000000000724072, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 603
=== ep: 604, time 25.703503131866455, eps 0.0010000000000688757, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 604
=== ep: 605, time 26.225338459014893, eps 0.0010000000000655166, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 605
=== ep: 606, time 25.858067274093628, eps 0.0010000000000623215, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 606
=== ep: 607, time 26.828996658325195, eps 0.001000000000059282, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 607
=== ep: 608, time 26.450352430343628, eps 0.0010000000000563907, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 608
=== ep: 609, time 118.88760280609131, eps 0.0010000000000536405, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 609
=== ep: 610, time 26.693418502807617, eps 0.0010000000000510245, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 610
goal_identified
=== ep: 611, time 27.612680673599243, eps 0.0010000000000485358, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 611
=== ep: 612, time 26.20085597038269, eps 0.0010000000000461688, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 612
goal_identified
=== ep: 613, time 29.2098171710968, eps 0.0010000000000439171, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 613
=== ep: 614, time 26.742608547210693, eps 0.0010000000000417752, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 614
=== ep: 615, time 27.301680326461792, eps 0.0010000000000397378, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 615
=== ep: 616, time 27.26823592185974, eps 0.0010000000000377999, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 616
=== ep: 617, time 27.890557050704956, eps 0.0010000000000359563, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 617
goal_identified
=== ep: 618, time 27.942647218704224, eps 0.0010000000000342027, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 618
=== ep: 619, time 117.30587601661682, eps 0.0010000000000325345, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 619
=== ep: 620, time 27.15111804008484, eps 0.001000000000030948, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 620
=== ep: 621, time 25.714069604873657, eps 0.0010000000000294385, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 621
=== ep: 622, time 26.036510705947876, eps 0.0010000000000280028, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 622
=== ep: 623, time 27.310508012771606, eps 0.0010000000000266371, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 623
=== ep: 624, time 24.91798233985901, eps 0.001000000000025338, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 624
goal_identified
=== ep: 625, time 26.787951469421387, eps 0.0010000000000241023, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 625
=== ep: 626, time 25.628908157348633, eps 0.0010000000000229268, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 626
=== ep: 627, time 27.44323754310608, eps 0.0010000000000218085, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 627
=== ep: 628, time 26.48018980026245, eps 0.001000000000020745, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 628
goal_identified
=== ep: 629, time 119.02576661109924, eps 0.0010000000000197332, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 629
=== ep: 630, time 26.836500644683838, eps 0.0010000000000187708, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 630
=== ep: 631, time 26.44968891143799, eps 0.0010000000000178553, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 631
=== ep: 632, time 26.635013818740845, eps 0.0010000000000169845, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 632
=== ep: 633, time 27.847784519195557, eps 0.0010000000000161562, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 633
=== ep: 634, time 28.88854217529297, eps 0.0010000000000153684, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 634
=== ep: 635, time 27.5288188457489, eps 0.0010000000000146188, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 635
goal_identified
=== ep: 636, time 26.636241912841797, eps 0.0010000000000139058, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 636
=== ep: 637, time 25.839660167694092, eps 0.0010000000000132275, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 637
goal_identified
=== ep: 638, time 30.1451256275177, eps 0.0010000000000125824, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 638
=== ep: 639, time 105.19417953491211, eps 0.0010000000000119687, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 639
goal_identified
=== ep: 640, time 28.376791954040527, eps 0.001000000000011385, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 640
=== ep: 641, time 27.307799816131592, eps 0.00100000000001083, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 641
goal_identified
=== ep: 642, time 27.269387006759644, eps 0.0010000000000103017, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 642
goal_identified
goal_identified
=== ep: 643, time 26.826804161071777, eps 0.0010000000000097993, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 643
goal_identified
=== ep: 644, time 28.4284770488739, eps 0.0010000000000093213, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 644
=== ep: 645, time 28.613057374954224, eps 0.0010000000000088666, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 645
=== ep: 646, time 26.338358402252197, eps 0.0010000000000084342, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 646
goal_identified
=== ep: 647, time 27.710958003997803, eps 0.001000000000008023, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 647
goal_identified
=== ep: 648, time 26.22687005996704, eps 0.0010000000000076317, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 648
=== ep: 649, time 114.18964767456055, eps 0.0010000000000072594, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 649
=== ep: 650, time 25.410295724868774, eps 0.0010000000000069055, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 650
=== ep: 651, time 27.538555145263672, eps 0.0010000000000065686, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 651
=== ep: 652, time 28.015536546707153, eps 0.0010000000000062483, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 652
=== ep: 653, time 28.21575117111206, eps 0.0010000000000059436, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 653
goal_identified
=== ep: 654, time 26.94014549255371, eps 0.0010000000000056537, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 654
=== ep: 655, time 25.19121551513672, eps 0.0010000000000053779, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 655
=== ep: 656, time 28.85636568069458, eps 0.0010000000000051157, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 656
=== ep: 657, time 26.077316522598267, eps 0.0010000000000048661, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 657
=== ep: 658, time 28.708603143692017, eps 0.001000000000004629, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 658
=== ep: 659, time 108.1607255935669, eps 0.0010000000000044032, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 659
=== ep: 660, time 26.380520820617676, eps 0.0010000000000041883, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 660
=== ep: 661, time 28.67420530319214, eps 0.001000000000003984, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 661
=== ep: 662, time 25.80934429168701, eps 0.0010000000000037897, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 662
=== ep: 663, time 25.635244369506836, eps 0.001000000000003605, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 663
=== ep: 664, time 27.48037815093994, eps 0.0010000000000034291, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 664
=== ep: 665, time 29.60696315765381, eps 0.001000000000003262, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 665
=== ep: 666, time 28.823981046676636, eps 0.0010000000000031028, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 666
=== ep: 667, time 25.18390703201294, eps 0.0010000000000029514, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 667
=== ep: 668, time 26.27037525177002, eps 0.0010000000000028075, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 668
=== ep: 669, time 114.40842604637146, eps 0.0010000000000026706, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 669
goal_identified
=== ep: 670, time 29.043169498443604, eps 0.0010000000000025403, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 670
=== ep: 671, time 27.249025344848633, eps 0.0010000000000024165, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 671
goal_identified
=== ep: 672, time 36.329256534576416, eps 0.0010000000000022985, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 672
=== ep: 673, time 27.57293176651001, eps 0.0010000000000021864, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 673
=== ep: 674, time 24.401989936828613, eps 0.00100000000000208, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 674
=== ep: 675, time 26.245507955551147, eps 0.0010000000000019785, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 675
=== ep: 676, time 27.4726984500885, eps 0.001000000000001882, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 676
=== ep: 677, time 27.878478050231934, eps 0.0010000000000017903, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 677
=== ep: 678, time 27.45455527305603, eps 0.0010000000000017029, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 678
=== ep: 679, time 98.26171541213989, eps 0.0010000000000016198, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 679
=== ep: 680, time 26.747737407684326, eps 0.0010000000000015409, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 680
=== ep: 681, time 26.011213064193726, eps 0.0010000000000014656, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 681
=== ep: 682, time 29.785864114761353, eps 0.0010000000000013943, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 682
=== ep: 683, time 28.90259861946106, eps 0.0010000000000013262, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 683
=== ep: 684, time 27.198997259140015, eps 0.0010000000000012616, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 684
goal_identified
=== ep: 685, time 27.03347659111023, eps 0.0010000000000012, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 685
=== ep: 686, time 24.9889817237854, eps 0.0010000000000011415, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 686
=== ep: 687, time 27.116968393325806, eps 0.0010000000000010857, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 687
=== ep: 688, time 27.767533540725708, eps 0.0010000000000010328, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 688
=== ep: 689, time 95.96568298339844, eps 0.0010000000000009825, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 689
=== ep: 690, time 26.833040475845337, eps 0.0010000000000009346, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 690
=== ep: 691, time 27.512974739074707, eps 0.001000000000000889, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 691
goal_identified
=== ep: 692, time 27.833026885986328, eps 0.0010000000000008457, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 692
goal_identified
=== ep: 693, time 27.22868585586548, eps 0.0010000000000008045, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 693
=== ep: 694, time 26.40456795692444, eps 0.0010000000000007653, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 694
=== ep: 695, time 26.766600847244263, eps 0.0010000000000007277, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 695
=== ep: 696, time 29.90820288658142, eps 0.0010000000000006924, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 696
=== ep: 697, time 26.004718542099, eps 0.0010000000000006586, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 697
=== ep: 698, time 26.769387006759644, eps 0.0010000000000006265, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 698
=== ep: 699, time 109.81420731544495, eps 0.001000000000000596, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 699
=== ep: 700, time 25.84570074081421, eps 0.0010000000000005668, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 700
=== ep: 701, time 24.93666434288025, eps 0.0010000000000005393, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 701
goal_identified
=== ep: 702, time 26.644185781478882, eps 0.0010000000000005128, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 702
=== ep: 703, time 26.389981031417847, eps 0.001000000000000488, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 703
=== ep: 704, time 27.64217233657837, eps 0.001000000000000464, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 704
=== ep: 705, time 25.46000838279724, eps 0.0010000000000004415, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 705
=== ep: 706, time 26.93212389945984, eps 0.00100000000000042, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 706
=== ep: 707, time 26.726318359375, eps 0.0010000000000003994, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 707
goal_identified
=== ep: 708, time 27.737082958221436, eps 0.00100000000000038, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 708
=== ep: 709, time 100.94239377975464, eps 0.0010000000000003615, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 709
=== ep: 710, time 28.675188779830933, eps 0.0010000000000003437, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 710
=== ep: 711, time 27.5002658367157, eps 0.001000000000000327, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 711
=== ep: 712, time 27.60079789161682, eps 0.0010000000000003112, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 712
=== ep: 713, time 29.69762110710144, eps 0.001000000000000296, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 713
=== ep: 714, time 26.180737257003784, eps 0.0010000000000002815, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 714
=== ep: 715, time 25.949431657791138, eps 0.0010000000000002678, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 715
=== ep: 716, time 25.327552795410156, eps 0.0010000000000002548, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 716
=== ep: 717, time 26.83475351333618, eps 0.0010000000000002422, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 717
=== ep: 718, time 25.234694480895996, eps 0.0010000000000002305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 718
=== ep: 719, time 123.33658790588379, eps 0.0010000000000002192, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 719
=== ep: 720, time 24.71457862854004, eps 0.0010000000000002086, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 720
=== ep: 721, time 26.67133331298828, eps 0.0010000000000001984, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 721
=== ep: 722, time 30.199098825454712, eps 0.0010000000000001887, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 722
=== ep: 723, time 26.251214265823364, eps 0.0010000000000001796, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 723
=== ep: 724, time 26.694701671600342, eps 0.0010000000000001707, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 724
=== ep: 725, time 24.06087327003479, eps 0.0010000000000001624, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 725
=== ep: 726, time 29.517003536224365, eps 0.0010000000000001544, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 726
=== ep: 727, time 25.534321069717407, eps 0.001000000000000147, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 727
=== ep: 728, time 36.67357087135315, eps 0.0010000000000001399, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 728
=== ep: 729, time 125.02654719352722, eps 0.001000000000000133, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 729
=== ep: 730, time 26.893928289413452, eps 0.0010000000000001264, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 730
=== ep: 731, time 24.508965730667114, eps 0.0010000000000001204, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 731
=== ep: 732, time 24.702226877212524, eps 0.0010000000000001145, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 732
=== ep: 733, time 26.822203874588013, eps 0.0010000000000001089, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 733
goal_identified
=== ep: 734, time 28.05939292907715, eps 0.0010000000000001037, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 734
=== ep: 735, time 25.310017108917236, eps 0.0010000000000000985, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 735
=== ep: 736, time 26.47816276550293, eps 0.0010000000000000937, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 736
=== ep: 737, time 26.976284503936768, eps 0.0010000000000000891, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 737
=== ep: 738, time 28.7089946269989, eps 0.0010000000000000848, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 738
=== ep: 739, time 110.12215089797974, eps 0.0010000000000000807, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 739
=== ep: 740, time 26.589394092559814, eps 0.0010000000000000768, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 740
goal_identified
=== ep: 741, time 27.15071129798889, eps 0.001000000000000073, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 741
goal_identified
=== ep: 742, time 26.737027645111084, eps 0.0010000000000000694, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 742
goal_identified
=== ep: 743, time 26.204073429107666, eps 0.001000000000000066, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 743
=== ep: 744, time 25.48112440109253, eps 0.001000000000000063, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 744
=== ep: 745, time 28.301578760147095, eps 0.0010000000000000599, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 745
=== ep: 746, time 25.213183403015137, eps 0.0010000000000000568, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 746
=== ep: 747, time 26.787530422210693, eps 0.001000000000000054, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 747
=== ep: 748, time 26.348772287368774, eps 0.0010000000000000514, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 748
goal_identified
=== ep: 749, time 116.14153456687927, eps 0.001000000000000049, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 749
=== ep: 750, time 26.81239104270935, eps 0.0010000000000000466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 750
=== ep: 751, time 24.526610612869263, eps 0.0010000000000000443, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 751
=== ep: 752, time 23.462958335876465, eps 0.001000000000000042, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 752
=== ep: 753, time 27.000758171081543, eps 0.0010000000000000401, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 753
=== ep: 754, time 25.504045486450195, eps 0.0010000000000000382, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 754
=== ep: 755, time 25.886828422546387, eps 0.0010000000000000362, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 755
=== ep: 756, time 26.543198347091675, eps 0.0010000000000000345, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 756
=== ep: 757, time 25.90654230117798, eps 0.0010000000000000328, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 757
=== ep: 758, time 26.73675036430359, eps 0.0010000000000000312, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 758
=== ep: 759, time 98.30016279220581, eps 0.0010000000000000297, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 759
=== ep: 760, time 26.85892605781555, eps 0.0010000000000000282, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 760
goal_identified
goal_identified
=== ep: 761, time 27.987921953201294, eps 0.001000000000000027, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 761
=== ep: 762, time 27.811065673828125, eps 0.0010000000000000256, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 762
=== ep: 763, time 26.07098913192749, eps 0.0010000000000000243, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 763
goal_identified
=== ep: 764, time 26.495439529418945, eps 0.0010000000000000232, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 764
=== ep: 765, time 24.64812445640564, eps 0.001000000000000022, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 765
=== ep: 766, time 26.424978256225586, eps 0.0010000000000000208, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 766
=== ep: 767, time 27.591619968414307, eps 0.00100000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 767
=== ep: 768, time 29.307355165481567, eps 0.0010000000000000189, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 768
=== ep: 769, time 110.13525557518005, eps 0.001000000000000018, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 769
=== ep: 770, time 27.33586812019348, eps 0.0010000000000000172, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 770
=== ep: 771, time 24.972261428833008, eps 0.0010000000000000163, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 771
=== ep: 772, time 27.828895807266235, eps 0.0010000000000000154, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 772
=== ep: 773, time 26.589531183242798, eps 0.0010000000000000148, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 773
=== ep: 774, time 27.07540488243103, eps 0.0010000000000000141, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 774
goal_identified
=== ep: 775, time 26.67011332511902, eps 0.0010000000000000132, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 775
=== ep: 776, time 23.329288482666016, eps 0.0010000000000000126, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 776
=== ep: 777, time 29.037884950637817, eps 0.0010000000000000122, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 777
=== ep: 778, time 29.233234405517578, eps 0.0010000000000000115, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 778
=== ep: 779, time 101.50460505485535, eps 0.0010000000000000109, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 779
=== ep: 780, time 26.37095856666565, eps 0.0010000000000000104, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 780
goal_identified
=== ep: 781, time 27.438268661499023, eps 0.00100000000000001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 781
=== ep: 782, time 26.71669840812683, eps 0.0010000000000000093, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 782
=== ep: 783, time 25.305045127868652, eps 0.001000000000000009, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 783
=== ep: 784, time 25.797465324401855, eps 0.0010000000000000085, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 784
=== ep: 785, time 26.25400733947754, eps 0.001000000000000008, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 785
=== ep: 786, time 30.1481192111969, eps 0.0010000000000000076, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 786
=== ep: 787, time 26.99253559112549, eps 0.0010000000000000074, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 787
goal_identified
=== ep: 788, time 26.017362356185913, eps 0.001000000000000007, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 788
=== ep: 789, time 113.83746433258057, eps 0.0010000000000000067, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 789
=== ep: 790, time 24.128159999847412, eps 0.0010000000000000063, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 790
goal_identified
=== ep: 791, time 26.659741640090942, eps 0.001000000000000006, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 791
=== ep: 792, time 25.540244817733765, eps 0.0010000000000000057, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 792
=== ep: 793, time 28.38370442390442, eps 0.0010000000000000054, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 793
goal_identified
=== ep: 794, time 26.37124514579773, eps 0.0010000000000000052, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 794
=== ep: 795, time 30.22291588783264, eps 0.001000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 795
=== ep: 796, time 24.882567882537842, eps 0.0010000000000000048, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 796
=== ep: 797, time 26.32182765007019, eps 0.0010000000000000044, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 797
goal_identified
=== ep: 798, time 25.99565291404724, eps 0.0010000000000000041, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 798
=== ep: 799, time 106.17557835578918, eps 0.0010000000000000041, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 799
goal_identified
=== ep: 800, time 25.899986743927002, eps 0.001000000000000004, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 800
goal_identified
=== ep: 801, time 25.708181381225586, eps 0.0010000000000000037, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 801
goal_identified
=== ep: 802, time 25.38981342315674, eps 0.0010000000000000035, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 802
=== ep: 803, time 25.939703226089478, eps 0.0010000000000000033, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 803
=== ep: 804, time 26.30312418937683, eps 0.001000000000000003, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 804
=== ep: 805, time 26.622763872146606, eps 0.001000000000000003, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 805
=== ep: 806, time 25.93681526184082, eps 0.0010000000000000028, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 806
goal_identified
=== ep: 807, time 27.394461631774902, eps 0.0010000000000000026, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 807
=== ep: 808, time 25.413408279418945, eps 0.0010000000000000026, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 808
=== ep: 809, time 117.70382452011108, eps 0.0010000000000000024, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 809
goal_identified
=== ep: 810, time 30.49316716194153, eps 0.0010000000000000024, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 810
=== ep: 811, time 39.45875525474548, eps 0.0010000000000000022, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 811
goal_identified
=== ep: 812, time 29.60547375679016, eps 0.0010000000000000022, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 812
=== ep: 813, time 30.775444984436035, eps 0.001000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 813
=== ep: 814, time 28.261430501937866, eps 0.001000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 814
=== ep: 815, time 27.430169582366943, eps 0.0010000000000000018, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 815
goal_identified
=== ep: 816, time 28.584237098693848, eps 0.0010000000000000018, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 816
=== ep: 817, time 30.17331314086914, eps 0.0010000000000000018, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 817
=== ep: 818, time 30.8850417137146, eps 0.0010000000000000015, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 818
=== ep: 819, time 153.78561401367188, eps 0.0010000000000000015, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 819
=== ep: 820, time 30.864914417266846, eps 0.0010000000000000013, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 820
=== ep: 821, time 31.200922966003418, eps 0.0010000000000000013, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 821
=== ep: 822, time 31.820441007614136, eps 0.0010000000000000013, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 822
goal_identified
=== ep: 823, time 28.664169788360596, eps 0.0010000000000000013, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 823
=== ep: 824, time 29.55608057975769, eps 0.001000000000000001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 824
=== ep: 825, time 31.246567010879517, eps 0.001000000000000001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 825
=== ep: 826, time 27.619213819503784, eps 0.001000000000000001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 826
=== ep: 827, time 28.335859298706055, eps 0.001000000000000001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 827
=== ep: 828, time 24.05964231491089, eps 0.0010000000000000009, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 828
=== ep: 829, time 182.91812252998352, eps 0.0010000000000000009, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 829
=== ep: 830, time 28.059459686279297, eps 0.0010000000000000009, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 830
=== ep: 831, time 31.021142721176147, eps 0.0010000000000000009, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 831
=== ep: 832, time 32.38396883010864, eps 0.0010000000000000009, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 832
goal_identified
=== ep: 833, time 29.648134231567383, eps 0.0010000000000000007, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 833
=== ep: 834, time 30.135069370269775, eps 0.0010000000000000007, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 834
goal_identified
=== ep: 835, time 30.78847908973694, eps 0.0010000000000000007, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 835
=== ep: 836, time 31.8548686504364, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 836
=== ep: 837, time 25.84220790863037, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 837
=== ep: 838, time 30.79116916656494, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 838
=== ep: 839, time 179.1405107975006, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 839
=== ep: 840, time 29.19656753540039, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 840
=== ep: 841, time 33.19574046134949, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 841
=== ep: 842, time 31.29468870162964, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 842
goal_identified
=== ep: 843, time 33.1640989780426, eps 0.0010000000000000005, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 843
=== ep: 844, time 29.11579179763794, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 844
=== ep: 845, time 33.30778431892395, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 845
=== ep: 846, time 29.805373191833496, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 846
=== ep: 847, time 30.024734020233154, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 847
=== ep: 848, time 37.46024036407471, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 848
=== ep: 849, time 178.89247059822083, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 849
=== ep: 850, time 30.09633159637451, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 850
=== ep: 851, time 41.436530351638794, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 851
=== ep: 852, time 32.31481599807739, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 852
=== ep: 853, time 30.331390142440796, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 853
=== ep: 854, time 31.370731830596924, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 854
=== ep: 855, time 33.197059631347656, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 855
goal_identified
=== ep: 856, time 27.19861602783203, eps 0.0010000000000000002, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 856
=== ep: 857, time 30.313743114471436, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 857
=== ep: 858, time 28.381934642791748, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 858
goal_identified
=== ep: 859, time 150.54329824447632, eps 0.0010000000000000002, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 859
=== ep: 860, time 31.181544542312622, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 860
=== ep: 861, time 31.530345916748047, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 861
=== ep: 862, time 23.806833505630493, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 862
=== ep: 863, time 35.800496339797974, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 863
=== ep: 864, time 34.4322235584259, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 864
=== ep: 865, time 30.263923406600952, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 865
goal_identified
=== ep: 866, time 27.188387870788574, eps 0.0010000000000000002, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 866
=== ep: 867, time 30.3023841381073, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 867
=== ep: 868, time 30.226757049560547, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 868
=== ep: 869, time 160.40450382232666, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 869
=== ep: 870, time 26.785627365112305, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 870
=== ep: 871, time 30.765589475631714, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 871
=== ep: 872, time 35.13584065437317, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 872
=== ep: 873, time 31.75006914138794, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 873
=== ep: 874, time 33.24241542816162, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 874
=== ep: 875, time 29.037871599197388, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 875
goal_identified
=== ep: 876, time 30.225823640823364, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 876
=== ep: 877, time 29.243711948394775, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 877
=== ep: 878, time 30.367164611816406, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 878
=== ep: 879, time 159.01571941375732, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 879
=== ep: 880, time 31.690654277801514, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 880
=== ep: 881, time 30.250850915908813, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 881
=== ep: 882, time 30.370216369628906, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 882
=== ep: 883, time 30.551998376846313, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 883
=== ep: 884, time 33.10175156593323, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 884
=== ep: 885, time 30.754571676254272, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 885
=== ep: 886, time 28.15470552444458, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 886
=== ep: 887, time 30.29207944869995, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 887
=== ep: 888, time 26.765437364578247, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 888
=== ep: 889, time 168.0898060798645, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 889
=== ep: 890, time 29.071045637130737, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 890
=== ep: 891, time 31.567786693572998, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 891
=== ep: 892, time 30.699944734573364, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 892
=== ep: 893, time 29.511438131332397, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 893
=== ep: 894, time 27.678072214126587, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 894
=== ep: 895, time 28.150781393051147, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 895
=== ep: 896, time 33.213600873947144, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 896
=== ep: 897, time 30.951528549194336, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 897
=== ep: 898, time 32.72420310974121, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 898
=== ep: 899, time 161.98141169548035, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 899
=== ep: 900, time 33.12567377090454, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 900
=== ep: 901, time 28.654362201690674, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 901
=== ep: 902, time 33.4563672542572, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 902
=== ep: 903, time 28.30077624320984, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 903
=== ep: 904, time 29.268664598464966, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 904
=== ep: 905, time 28.459028244018555, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 905
goal_identified
=== ep: 906, time 32.561580181121826, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 906
=== ep: 907, time 33.21964359283447, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 907
=== ep: 908, time 31.078644275665283, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 908
goal_identified
=== ep: 909, time 147.47494292259216, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 909
=== ep: 910, time 30.51352620124817, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 910
=== ep: 911, time 30.76905655860901, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 911
goal_identified
=== ep: 912, time 28.892380475997925, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 912
=== ep: 913, time 29.38936161994934, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 913
goal_identified
=== ep: 914, time 30.06607723236084, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 914
=== ep: 915, time 29.79053020477295, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 915
=== ep: 916, time 29.57034420967102, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 916
=== ep: 917, time 29.097442626953125, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 917
=== ep: 918, time 28.24777340888977, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 918
=== ep: 919, time 143.13633680343628, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 919
=== ep: 920, time 31.245466232299805, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 920
=== ep: 921, time 29.343763828277588, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 921
=== ep: 922, time 32.223015785217285, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 922
=== ep: 923, time 35.22342371940613, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 923
goal_identified
=== ep: 924, time 32.067452907562256, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 924
=== ep: 925, time 27.381568670272827, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 925
=== ep: 926, time 29.31897258758545, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 926
=== ep: 927, time 30.879809856414795, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 927
=== ep: 928, time 29.468756437301636, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 928
=== ep: 929, time 140.3630406856537, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 929
=== ep: 930, time 31.47068428993225, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 930
=== ep: 931, time 32.922995805740356, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 931
=== ep: 932, time 33.04519248008728, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 932
=== ep: 933, time 41.6512987613678, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 933
=== ep: 934, time 26.395153045654297, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 934
=== ep: 935, time 30.24443006515503, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 935
=== ep: 936, time 28.94993281364441, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 936
=== ep: 937, time 31.670459747314453, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 937
=== ep: 938, time 29.14173126220703, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 938
=== ep: 939, time 162.1932339668274, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 939
=== ep: 940, time 30.051928758621216, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 940
=== ep: 941, time 30.53836727142334, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 941
=== ep: 942, time 30.977142810821533, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 942
=== ep: 943, time 30.283656358718872, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 943
=== ep: 944, time 25.970938444137573, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 944
=== ep: 945, time 29.804925203323364, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 945
=== ep: 946, time 32.38122487068176, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 946
=== ep: 947, time 30.355180025100708, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 947
=== ep: 948, time 27.34274387359619, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 948
=== ep: 949, time 143.77213335037231, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 949
goal_identified
=== ep: 950, time 28.652178287506104, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 950
goal_identified
=== ep: 951, time 29.515503644943237, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 951
=== ep: 952, time 29.526233911514282, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 952
=== ep: 953, time 26.526575088500977, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 953
=== ep: 954, time 30.985474824905396, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 954
=== ep: 955, time 28.87611484527588, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 955
goal_identified
=== ep: 956, time 29.259296655654907, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 956
=== ep: 957, time 27.37086796760559, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 957
goal_identified
=== ep: 958, time 31.74245595932007, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 958
goal_identified
=== ep: 959, time 125.7704849243164, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 959
=== ep: 960, time 27.673096418380737, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 960
=== ep: 961, time 29.31393051147461, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 961
=== ep: 962, time 27.127014636993408, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 962
=== ep: 963, time 26.899810075759888, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 963
=== ep: 964, time 27.022600173950195, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 964
=== ep: 965, time 26.94294047355652, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 965
=== ep: 966, time 29.812241554260254, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 966
=== ep: 967, time 32.00218200683594, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 967
=== ep: 968, time 28.794219970703125, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 968
=== ep: 969, time 131.8388614654541, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 969
=== ep: 970, time 30.30604577064514, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 970
goal_identified
=== ep: 971, time 27.9190833568573, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 971
=== ep: 972, time 25.956287622451782, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 972
=== ep: 973, time 27.711241722106934, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 973
=== ep: 974, time 28.009963274002075, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 974
=== ep: 975, time 30.631895542144775, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 975
=== ep: 976, time 29.94660472869873, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 976
=== ep: 977, time 26.24286651611328, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 977
=== ep: 978, time 25.321940183639526, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 978
=== ep: 979, time 133.2886083126068, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 979
=== ep: 980, time 32.099241971969604, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 980
=== ep: 981, time 27.608004093170166, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 981
=== ep: 982, time 27.63719081878662, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 982
=== ep: 983, time 28.75411343574524, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 983
=== ep: 984, time 29.736152410507202, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 984
goal_identified
goal_identified
=== ep: 985, time 28.222826719284058, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 985
=== ep: 986, time 27.745348930358887, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 986
goal_identified
goal_identified
=== ep: 987, time 27.08040761947632, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 987
=== ep: 988, time 27.25654411315918, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 988
=== ep: 989, time 129.75620532035828, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 989
=== ep: 990, time 25.085676431655884, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 990
=== ep: 991, time 30.42042112350464, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 991
=== ep: 992, time 30.703110218048096, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 992
=== ep: 993, time 29.27194833755493, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 993
=== ep: 994, time 27.05145001411438, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 994
=== ep: 995, time 26.817394971847534, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 995
=== ep: 996, time 26.318084955215454, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 996
=== ep: 997, time 28.206334352493286, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 997
goal_identified
=== ep: 998, time 29.631322860717773, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 998
=== ep: 999, time 112.99088597297668, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 999
=== ep: 1000, time 29.75734257698059, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1000
goal_identified
=== ep: 1001, time 31.634900093078613, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1001
=== ep: 1002, time 28.768308639526367, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1002
goal_identified
=== ep: 1003, time 27.175935983657837, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1003
=== ep: 1004, time 27.33560085296631, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1004
=== ep: 1005, time 27.305676698684692, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1005
=== ep: 1006, time 28.261030673980713, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1006
goal_identified
=== ep: 1007, time 28.496735334396362, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1007
goal_identified
=== ep: 1008, time 26.913132429122925, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1008
=== ep: 1009, time 123.81252694129944, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1009
=== ep: 1010, time 30.264577388763428, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1010
=== ep: 1011, time 26.124906301498413, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1011
=== ep: 1012, time 26.82597064971924, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1012
=== ep: 1013, time 26.83759379386902, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1013
=== ep: 1014, time 27.08256506919861, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1014
goal_identified
=== ep: 1015, time 27.39608120918274, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1015
=== ep: 1016, time 29.835821628570557, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1016
=== ep: 1017, time 26.69895076751709, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1017
=== ep: 1018, time 26.279284238815308, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1018
=== ep: 1019, time 136.17500567436218, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1019
=== ep: 1020, time 28.607629776000977, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1020
=== ep: 1021, time 25.40722346305847, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1021
=== ep: 1022, time 28.584433794021606, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1022
=== ep: 1023, time 27.7584331035614, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1023
=== ep: 1024, time 26.73346519470215, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1024
=== ep: 1025, time 26.514025449752808, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1025
=== ep: 1026, time 27.55519390106201, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1026
=== ep: 1027, time 28.61360263824463, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1027
=== ep: 1028, time 27.73754596710205, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1028
goal_identified
=== ep: 1029, time 120.26921224594116, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1029
=== ep: 1030, time 29.38741183280945, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1030
=== ep: 1031, time 28.078957319259644, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1031
=== ep: 1032, time 28.31807279586792, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1032
goal_identified
=== ep: 1033, time 29.190173864364624, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1033
=== ep: 1034, time 27.393160104751587, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1034
=== ep: 1035, time 29.99441623687744, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1035
goal_identified
=== ep: 1036, time 28.48536205291748, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1036
=== ep: 1037, time 26.086522817611694, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1037
=== ep: 1038, time 30.02731204032898, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1038
=== ep: 1039, time 117.69781255722046, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1039
=== ep: 1040, time 28.502777814865112, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1040
=== ep: 1041, time 27.497853994369507, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1041
=== ep: 1042, time 28.756415367126465, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1042
=== ep: 1043, time 25.18923854827881, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1043
goal_identified
=== ep: 1044, time 27.999423265457153, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1044
goal_identified
=== ep: 1045, time 28.552735090255737, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1045
=== ep: 1046, time 28.100076913833618, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1046
=== ep: 1047, time 31.207796573638916, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1047
goal_identified
=== ep: 1048, time 27.135809183120728, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1048
=== ep: 1049, time 127.50777816772461, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1049
=== ep: 1050, time 26.747177839279175, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1050
=== ep: 1051, time 28.424493312835693, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1051
=== ep: 1052, time 27.9827299118042, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1052
goal_identified
=== ep: 1053, time 27.650961875915527, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1053
=== ep: 1054, time 29.19896960258484, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1054
=== ep: 1055, time 28.954089403152466, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1055
=== ep: 1056, time 28.215553283691406, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1056
=== ep: 1057, time 29.703822135925293, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1057
goal_identified
goal_identified
=== ep: 1058, time 30.023741006851196, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1058
=== ep: 1059, time 112.57093167304993, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1059
=== ep: 1060, time 29.58749532699585, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1060
=== ep: 1061, time 28.151287317276, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1061
goal_identified
=== ep: 1062, time 29.470799684524536, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1062
=== ep: 1063, time 27.454685926437378, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1063
=== ep: 1064, time 28.019004583358765, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1064
=== ep: 1065, time 29.626368045806885, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1065
=== ep: 1066, time 27.721987009048462, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1066
=== ep: 1067, time 27.654131650924683, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1067
goal_identified
=== ep: 1068, time 28.853249073028564, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1068
=== ep: 1069, time 112.6360194683075, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1069
=== ep: 1070, time 27.84927201271057, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1070
goal_identified
=== ep: 1071, time 27.64207625389099, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1071
goal_identified
goal_identified
goal_identified
=== ep: 1072, time 27.960691928863525, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 9
goal_identified
goal_identified
=== ep: 1073, time 26.58269238471985, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1073
=== ep: 1074, time 28.38642978668213, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1074
=== ep: 1075, time 28.354533433914185, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1075
=== ep: 1076, time 29.069690942764282, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1076
=== ep: 1077, time 26.313715934753418, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1077
goal_identified
=== ep: 1078, time 27.430826425552368, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1078
=== ep: 1079, time 134.19355535507202, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1079
=== ep: 1080, time 27.032445430755615, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1080
=== ep: 1081, time 26.130638360977173, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1081
=== ep: 1082, time 28.932095289230347, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1082
=== ep: 1083, time 29.77462363243103, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1083
=== ep: 1084, time 30.619883060455322, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1084
=== ep: 1085, time 27.30755090713501, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1085
=== ep: 1086, time 29.681896686553955, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1086
=== ep: 1087, time 24.987106800079346, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1087
=== ep: 1088, time 30.370054483413696, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1088
=== ep: 1089, time 135.84238958358765, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1089
=== ep: 1090, time 26.18738293647766, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1090
=== ep: 1091, time 28.564836025238037, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1091
=== ep: 1092, time 33.50028610229492, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1092
=== ep: 1093, time 28.047271251678467, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1093
=== ep: 1094, time 30.16610074043274, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1094
=== ep: 1095, time 26.944188117980957, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1095
=== ep: 1096, time 27.230682611465454, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1096
=== ep: 1097, time 25.519409894943237, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1097
=== ep: 1098, time 27.61067032814026, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1098
=== ep: 1099, time 130.35946488380432, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1099
=== ep: 1100, time 29.501665353775024, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1100
=== ep: 1101, time 29.70290184020996, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1101
=== ep: 1102, time 28.418241024017334, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1102
=== ep: 1103, time 27.23743724822998, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1103
=== ep: 1104, time 25.1834876537323, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1104
=== ep: 1105, time 26.78582191467285, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1105
goal_identified
=== ep: 1106, time 27.70275044441223, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1106
=== ep: 1107, time 28.6740939617157, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1107
=== ep: 1108, time 26.562499523162842, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1108
=== ep: 1109, time 144.74599719047546, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1109
=== ep: 1110, time 27.840070486068726, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1110
=== ep: 1111, time 28.041343450546265, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1111
=== ep: 1112, time 35.568278551101685, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1112
=== ep: 1113, time 26.851531744003296, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1113
=== ep: 1114, time 27.59847855567932, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1114
=== ep: 1115, time 31.383671045303345, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1115
=== ep: 1116, time 27.467697858810425, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1116
=== ep: 1117, time 27.117669105529785, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1117
=== ep: 1118, time 29.822620391845703, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1118
=== ep: 1119, time 141.18972325325012, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1119
goal_identified
=== ep: 1120, time 26.80530595779419, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1120
=== ep: 1121, time 26.410027265548706, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1121
=== ep: 1122, time 30.35758352279663, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1122
=== ep: 1123, time 25.71500563621521, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1123
=== ep: 1124, time 28.313755750656128, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1124
=== ep: 1125, time 27.508111000061035, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1125
=== ep: 1126, time 26.986319065093994, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1126
=== ep: 1127, time 27.27561926841736, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1127
=== ep: 1128, time 29.933780908584595, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1128
=== ep: 1129, time 125.57594156265259, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1129
goal_identified
=== ep: 1130, time 30.844093322753906, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1130
=== ep: 1131, time 28.90254020690918, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1131
=== ep: 1132, time 27.214003562927246, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1132
goal_identified
=== ep: 1133, time 29.203141689300537, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1133
=== ep: 1134, time 29.859851837158203, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1134
=== ep: 1135, time 28.023812532424927, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1135
=== ep: 1136, time 29.451205253601074, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1136
=== ep: 1137, time 27.88611650466919, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1137
=== ep: 1138, time 26.73476219177246, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1138
=== ep: 1139, time 120.74044108390808, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1139
=== ep: 1140, time 27.064979314804077, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1140
=== ep: 1141, time 26.860227584838867, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1141
=== ep: 1142, time 27.954964876174927, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1142
=== ep: 1143, time 25.45247745513916, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1143
goal_identified
=== ep: 1144, time 27.34371781349182, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1144
=== ep: 1145, time 30.662988662719727, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1145
=== ep: 1146, time 26.432634353637695, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1146
=== ep: 1147, time 26.48445749282837, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1147
=== ep: 1148, time 26.708764791488647, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1148
=== ep: 1149, time 111.13109540939331, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1149
=== ep: 1150, time 27.19741988182068, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1150
=== ep: 1151, time 25.97210669517517, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1151
=== ep: 1152, time 26.99357581138611, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1152
=== ep: 1153, time 31.087988138198853, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1153
=== ep: 1154, time 25.46293306350708, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1154
=== ep: 1155, time 26.25798797607422, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1155
=== ep: 1156, time 28.333976984024048, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1156
=== ep: 1157, time 29.03123140335083, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1157
=== ep: 1158, time 26.875198125839233, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1158
=== ep: 1159, time 124.05562782287598, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1159
=== ep: 1160, time 26.89773464202881, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1160
=== ep: 1161, time 28.883840084075928, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1161
=== ep: 1162, time 27.96004009246826, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1162
=== ep: 1163, time 28.167367696762085, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1163
goal_identified
=== ep: 1164, time 28.114251613616943, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1164
=== ep: 1165, time 24.700584411621094, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1165
=== ep: 1166, time 28.723750114440918, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1166
=== ep: 1167, time 27.29227089881897, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1167
=== ep: 1168, time 27.78555703163147, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1168
=== ep: 1169, time 120.95054459571838, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1169
=== ep: 1170, time 27.01331114768982, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1170
=== ep: 1171, time 25.880411386489868, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1171
=== ep: 1172, time 26.547629594802856, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1172
=== ep: 1173, time 26.980828046798706, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1173
goal_identified
=== ep: 1174, time 25.889193296432495, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1174
goal_identified
=== ep: 1175, time 28.76618790626526, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1175
=== ep: 1176, time 28.598560571670532, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1176
=== ep: 1177, time 27.369892358779907, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1177
=== ep: 1178, time 27.94796657562256, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1178
=== ep: 1179, time 128.58077883720398, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1179
=== ep: 1180, time 29.009482383728027, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1180
goal_identified
=== ep: 1181, time 28.035423755645752, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1181
=== ep: 1182, time 27.858466386795044, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1182
goal_identified
=== ep: 1183, time 30.073500871658325, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1183
=== ep: 1184, time 26.48646640777588, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1184
=== ep: 1185, time 28.103641510009766, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1185
=== ep: 1186, time 27.239670276641846, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1186
=== ep: 1187, time 26.110288619995117, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1187
=== ep: 1188, time 26.60753345489502, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1188
=== ep: 1189, time 131.10078525543213, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1189
=== ep: 1190, time 27.788982391357422, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1190
goal_identified
=== ep: 1191, time 28.579877376556396, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1191
=== ep: 1192, time 27.500586986541748, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1192
goal_identified
=== ep: 1193, time 28.25457501411438, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1193
=== ep: 1194, time 28.7371084690094, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1194
=== ep: 1195, time 25.723608016967773, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1195
=== ep: 1196, time 29.476454257965088, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1196
=== ep: 1197, time 25.87908697128296, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1197
goal_identified
=== ep: 1198, time 32.3025586605072, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1198
=== ep: 1199, time 126.38816928863525, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1199
=== ep: 1200, time 28.968408823013306, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1200
=== ep: 1201, time 27.962255239486694, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1201
=== ep: 1202, time 29.093240976333618, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1202
=== ep: 1203, time 29.31485605239868, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1203
=== ep: 1204, time 28.124584197998047, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1204
=== ep: 1205, time 27.578577995300293, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1205
=== ep: 1206, time 27.678779125213623, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1206
=== ep: 1207, time 26.167885065078735, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1207
=== ep: 1208, time 30.240779638290405, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1208
=== ep: 1209, time 116.1498863697052, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1209
goal_identified
=== ep: 1210, time 28.60923147201538, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1210
=== ep: 1211, time 30.328700304031372, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1211
=== ep: 1212, time 27.444536924362183, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1212
=== ep: 1213, time 27.189030170440674, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1213
=== ep: 1214, time 26.312669038772583, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1214
=== ep: 1215, time 28.96363091468811, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1215
goal_identified
=== ep: 1216, time 28.430169343948364, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1216
=== ep: 1217, time 27.448156595230103, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1217
=== ep: 1218, time 27.056220531463623, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1218
=== ep: 1219, time 130.89920854568481, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1219
=== ep: 1220, time 28.29418158531189, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1220
=== ep: 1221, time 26.36207675933838, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1221
=== ep: 1222, time 26.952375173568726, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1222
=== ep: 1223, time 28.500739812850952, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1223
=== ep: 1224, time 30.07104253768921, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1224
=== ep: 1225, time 28.91672706604004, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1225
=== ep: 1226, time 28.259589910507202, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1226
=== ep: 1227, time 26.544574737548828, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1227
=== ep: 1228, time 28.80216383934021, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1228
=== ep: 1229, time 119.30484437942505, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1229
=== ep: 1230, time 26.94188952445984, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1230
=== ep: 1231, time 30.943432569503784, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1231
=== ep: 1232, time 29.91740870475769, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1232
=== ep: 1233, time 28.30586051940918, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1233
=== ep: 1234, time 28.011340856552124, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1234
=== ep: 1235, time 26.233230352401733, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1235
=== ep: 1236, time 28.033818244934082, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1236
=== ep: 1237, time 27.364691257476807, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1237
=== ep: 1238, time 27.806612014770508, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1238
=== ep: 1239, time 133.97756433486938, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1239
=== ep: 1240, time 31.676680088043213, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1240
=== ep: 1241, time 27.811318159103394, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1241
goal_identified
=== ep: 1242, time 27.43355703353882, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1242
=== ep: 1243, time 27.728084087371826, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1243
=== ep: 1244, time 32.23292779922485, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1244
=== ep: 1245, time 28.859606742858887, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1245
goal_identified
goal_identified
=== ep: 1246, time 28.703205108642578, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1246
=== ep: 1247, time 26.133849382400513, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1247
goal_identified
=== ep: 1248, time 27.89065980911255, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1248
=== ep: 1249, time 140.59238362312317, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1249
=== ep: 1250, time 28.62331509590149, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1250
goal_identified
=== ep: 1251, time 27.34252429008484, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1251
goal_identified
=== ep: 1252, time 27.663779258728027, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1252
=== ep: 1253, time 29.133159399032593, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1253
=== ep: 1254, time 29.25395679473877, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1254
=== ep: 1255, time 29.03263211250305, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1255
=== ep: 1256, time 28.757645845413208, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1256
=== ep: 1257, time 27.506970643997192, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1257
=== ep: 1258, time 33.86184477806091, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1258
=== ep: 1259, time 115.41733956336975, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1259
=== ep: 1260, time 29.47579598426819, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1260
=== ep: 1261, time 27.26857280731201, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1261
=== ep: 1262, time 27.573397636413574, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1262
=== ep: 1263, time 27.23116970062256, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1263
=== ep: 1264, time 27.86809515953064, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1264
=== ep: 1265, time 29.56733775138855, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1265
=== ep: 1266, time 30.042616367340088, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1266
=== ep: 1267, time 28.409677743911743, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1267
goal_identified
=== ep: 1268, time 28.07942485809326, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1268
=== ep: 1269, time 133.77765083312988, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1269
=== ep: 1270, time 25.990723609924316, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1270
goal_identified
=== ep: 1271, time 27.708620309829712, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1271
=== ep: 1272, time 28.81355094909668, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1272
=== ep: 1273, time 29.56987738609314, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1273
goal_identified
goal_identified
=== ep: 1274, time 31.692137718200684, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1274
=== ep: 1275, time 27.898528575897217, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1275
=== ep: 1276, time 29.829269886016846, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1276
goal_identified
=== ep: 1277, time 28.921187162399292, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1277
=== ep: 1278, time 28.529213190078735, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1278
=== ep: 1279, time 121.260981798172, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1279
goal_identified
=== ep: 1280, time 28.575783014297485, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1280
goal_identified
=== ep: 1281, time 28.662237882614136, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1281
=== ep: 1282, time 30.10155439376831, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1282
=== ep: 1283, time 26.749277591705322, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1283
=== ep: 1284, time 27.2666916847229, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1284
=== ep: 1285, time 27.07924485206604, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1285
=== ep: 1286, time 28.650967597961426, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1286
goal_identified
=== ep: 1287, time 28.87690496444702, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1287
=== ep: 1288, time 24.61728835105896, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1288
goal_identified
=== ep: 1289, time 127.89704203605652, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1289
=== ep: 1290, time 29.831793069839478, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1290
=== ep: 1291, time 26.53034472465515, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1291
=== ep: 1292, time 31.309016227722168, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1292
goal_identified
goal_identified
goal_identified
=== ep: 1293, time 28.59439206123352, eps 0.001, sum reward: 3, score_diff 3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 30
=== ep: 1294, time 25.857143878936768, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1294
=== ep: 1295, time 26.25194811820984, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1295
=== ep: 1296, time 27.544325351715088, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1296
=== ep: 1297, time 26.02213406562805, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1297
=== ep: 1298, time 26.461950540542603, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1298
goal_identified
=== ep: 1299, time 140.51914262771606, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1299
=== ep: 1300, time 28.17101001739502, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1300
=== ep: 1301, time 27.09647822380066, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1301
=== ep: 1302, time 27.24789333343506, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1302
=== ep: 1303, time 31.000943422317505, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1303
goal_identified
=== ep: 1304, time 27.830554723739624, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1304
=== ep: 1305, time 27.455851316452026, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1305
=== ep: 1306, time 28.4091956615448, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1306
goal_identified
=== ep: 1307, time 28.54784870147705, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1307
goal_identified
=== ep: 1308, time 30.812612533569336, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1308
=== ep: 1309, time 136.40683960914612, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1309
=== ep: 1310, time 29.22998356819153, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1310
=== ep: 1311, time 29.114720106124878, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1311
=== ep: 1312, time 25.868149518966675, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1312
=== ep: 1313, time 26.97396469116211, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1313
=== ep: 1314, time 28.546149253845215, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1314
=== ep: 1315, time 26.695070505142212, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1315
=== ep: 1316, time 31.53279709815979, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1316
=== ep: 1317, time 29.565741062164307, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1317
=== ep: 1318, time 27.599024295806885, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1318
=== ep: 1319, time 139.5179603099823, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1319
=== ep: 1320, time 31.105729818344116, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1320
=== ep: 1321, time 30.192063570022583, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1321
=== ep: 1322, time 30.901775360107422, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1322
goal_identified
=== ep: 1323, time 31.393197774887085, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1323
=== ep: 1324, time 27.235796451568604, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1324
=== ep: 1325, time 28.452304124832153, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1325
=== ep: 1326, time 27.320306301116943, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1326
=== ep: 1327, time 27.243962049484253, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1327
=== ep: 1328, time 28.684287309646606, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1328
goal_identified
goal_identified
=== ep: 1329, time 122.20635342597961, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1329
=== ep: 1330, time 29.03908109664917, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1330
=== ep: 1331, time 29.08051824569702, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1331
goal_identified
=== ep: 1332, time 30.148356914520264, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1332
=== ep: 1333, time 28.08497381210327, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1333
=== ep: 1334, time 26.163631916046143, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1334
=== ep: 1335, time 26.94589138031006, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1335
=== ep: 1336, time 27.74315333366394, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1336
=== ep: 1337, time 27.044705152511597, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1337
goal_identified
=== ep: 1338, time 27.900386333465576, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1338
=== ep: 1339, time 159.05470848083496, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1339
=== ep: 1340, time 27.867862701416016, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1340
goal_identified
=== ep: 1341, time 28.808306455612183, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1341
=== ep: 1342, time 26.97870373725891, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1342
=== ep: 1343, time 30.03475046157837, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1343
=== ep: 1344, time 30.97343373298645, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1344
=== ep: 1345, time 26.103458881378174, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1345
=== ep: 1346, time 27.52018666267395, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1346
goal_identified
=== ep: 1347, time 25.39791965484619, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1347
=== ep: 1348, time 26.537705183029175, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1348
=== ep: 1349, time 125.75616145133972, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1349
=== ep: 1350, time 24.679301500320435, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1350
=== ep: 1351, time 26.519515991210938, eps 0.001, sum reward: 0, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1351
=== ep: 1352, time 28.546645402908325, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1352
goal_identified
goal_identified
=== ep: 1353, time 28.753021478652954, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1353
=== ep: 1354, time 27.30319333076477, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1354
=== ep: 1355, time 26.638582229614258, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1355
=== ep: 1356, time 29.25102734565735, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1356
=== ep: 1357, time 27.105655193328857, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1357
goal_identified
=== ep: 1358, time 26.388455629348755, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1358
=== ep: 1359, time 143.18982028961182, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1359
=== ep: 1360, time 28.40642285346985, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1360
=== ep: 1361, time 23.983814477920532, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1361
=== ep: 1362, time 26.883820056915283, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1362
=== ep: 1363, time 26.629571676254272, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1363
=== ep: 1364, time 26.007056713104248, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1364
=== ep: 1365, time 26.316286087036133, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1365
=== ep: 1366, time 25.75372290611267, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1366
=== ep: 1367, time 26.451440572738647, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1367
=== ep: 1368, time 27.432238340377808, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1368
=== ep: 1369, time 126.03052973747253, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1369
=== ep: 1370, time 26.850796937942505, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1370
=== ep: 1371, time 29.283287048339844, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1371
=== ep: 1372, time 26.86510467529297, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1372
goal_identified
=== ep: 1373, time 26.265183210372925, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1373
=== ep: 1374, time 27.30800724029541, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1374
=== ep: 1375, time 29.284460067749023, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1375
=== ep: 1376, time 34.222670793533325, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1376
=== ep: 1377, time 27.620774745941162, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1377
=== ep: 1378, time 27.892930269241333, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1378
=== ep: 1379, time 127.0384910106659, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1379
=== ep: 1380, time 27.76226544380188, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1380
goal_identified
goal_identified
=== ep: 1381, time 28.267319440841675, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1381
goal_identified
=== ep: 1382, time 26.956178903579712, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1382
goal_identified
=== ep: 1383, time 28.775409936904907, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1383
goal_identified
=== ep: 1384, time 30.83690071105957, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1384
=== ep: 1385, time 25.77251148223877, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1385
goal_identified
=== ep: 1386, time 26.26685905456543, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1386
=== ep: 1387, time 26.310213327407837, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1387
=== ep: 1388, time 28.022796869277954, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1388
=== ep: 1389, time 126.34531331062317, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1389
goal_identified
=== ep: 1390, time 25.415428161621094, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1390
=== ep: 1391, time 27.828535556793213, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1391
=== ep: 1392, time 28.70679545402527, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1392
=== ep: 1393, time 30.947824001312256, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1393
=== ep: 1394, time 25.704017400741577, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1394
=== ep: 1395, time 28.391536712646484, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1395
=== ep: 1396, time 27.350539684295654, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1396
=== ep: 1397, time 25.70016074180603, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1397
goal_identified
=== ep: 1398, time 29.30355143547058, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1398
=== ep: 1399, time 126.12214708328247, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1399
=== ep: 1400, time 29.380356550216675, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1400
=== ep: 1401, time 27.849318265914917, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1401
=== ep: 1402, time 28.46307063102722, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1402
=== ep: 1403, time 25.394876718521118, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1403
goal_identified
=== ep: 1404, time 28.243772745132446, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1404
=== ep: 1405, time 27.309460401535034, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1405
goal_identified
goal_identified
=== ep: 1406, time 26.677496194839478, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1406
=== ep: 1407, time 27.789202213287354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1407
goal_identified
=== ep: 1408, time 25.722870588302612, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1408
=== ep: 1409, time 160.2685227394104, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1409
=== ep: 1410, time 27.99483633041382, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1410
=== ep: 1411, time 24.40540099143982, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1411
goal_identified
=== ep: 1412, time 27.258216381072998, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1412
=== ep: 1413, time 29.34970211982727, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1413
=== ep: 1414, time 25.404027700424194, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1414
=== ep: 1415, time 27.2520694732666, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1415
=== ep: 1416, time 27.021437168121338, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1416
=== ep: 1417, time 25.29096817970276, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1417
goal_identified
=== ep: 1418, time 26.974072456359863, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1418
=== ep: 1419, time 154.48147821426392, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1419
=== ep: 1420, time 26.769158840179443, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1420
goal_identified
=== ep: 1421, time 27.30504083633423, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1421
=== ep: 1422, time 26.281156539916992, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1422
=== ep: 1423, time 25.568960189819336, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1423
=== ep: 1424, time 25.985013484954834, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1424
=== ep: 1425, time 26.19553303718567, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1425
=== ep: 1426, time 28.277624368667603, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1426
=== ep: 1427, time 28.761993885040283, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1427
=== ep: 1428, time 36.180095195770264, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1428
=== ep: 1429, time 127.62535095214844, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1429
=== ep: 1430, time 28.56527805328369, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1430
=== ep: 1431, time 26.420508861541748, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1431
=== ep: 1432, time 26.889968395233154, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1432
goal_identified
goal_identified
goal_identified
=== ep: 1433, time 29.882261753082275, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1433
=== ep: 1434, time 26.442453861236572, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1434
=== ep: 1435, time 30.520601272583008, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1435
=== ep: 1436, time 32.58920693397522, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1436
goal_identified
=== ep: 1437, time 28.11231231689453, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1437
goal_identified
=== ep: 1438, time 27.366822957992554, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1438
=== ep: 1439, time 133.17968344688416, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1439
=== ep: 1440, time 26.93142080307007, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1440
goal_identified
goal_identified
=== ep: 1441, time 27.410032510757446, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1441
goal_identified
=== ep: 1442, time 23.90598464012146, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1442
goal_identified
=== ep: 1443, time 25.97528386116028, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1443
=== ep: 1444, time 27.3503155708313, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1444
=== ep: 1445, time 28.45617413520813, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1445
goal_identified
=== ep: 1446, time 27.174915075302124, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1446
goal_identified
=== ep: 1447, time 28.49015974998474, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1447
=== ep: 1448, time 27.01769995689392, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1448
goal_identified
=== ep: 1449, time 131.15538454055786, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1449
=== ep: 1450, time 27.168317556381226, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1450
goal_identified
=== ep: 1451, time 27.061450004577637, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1451
=== ep: 1452, time 28.676164865493774, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1452
=== ep: 1453, time 28.915729522705078, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1453
=== ep: 1454, time 27.113829612731934, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1454
=== ep: 1455, time 28.255946397781372, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1455
goal_identified
=== ep: 1456, time 27.740558624267578, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1456
=== ep: 1457, time 26.44643235206604, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1457
=== ep: 1458, time 26.30762243270874, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1458
=== ep: 1459, time 129.44227385520935, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1459
=== ep: 1460, time 27.158634185791016, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1460
=== ep: 1461, time 27.51881217956543, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1461
=== ep: 1462, time 30.50400686264038, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1462
goal_identified
=== ep: 1463, time 26.031546354293823, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1463
=== ep: 1464, time 26.106664896011353, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1464
goal_identified
=== ep: 1465, time 27.130688428878784, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1465
=== ep: 1466, time 29.583390951156616, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1466
=== ep: 1467, time 25.522826671600342, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1467
=== ep: 1468, time 27.37944483757019, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1468
goal_identified
=== ep: 1469, time 126.43823289871216, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1469
=== ep: 1470, time 28.739818572998047, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1470
=== ep: 1471, time 28.350842237472534, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1471
=== ep: 1472, time 27.940008640289307, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1472
=== ep: 1473, time 28.540123224258423, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1473
=== ep: 1474, time 26.69664168357849, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1474
=== ep: 1475, time 25.8822021484375, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1475
=== ep: 1476, time 27.13628125190735, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1476
=== ep: 1477, time 25.9284884929657, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1477
=== ep: 1478, time 29.91588306427002, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1478
=== ep: 1479, time 135.26100659370422, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1479
=== ep: 1480, time 28.824679613113403, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1480
=== ep: 1481, time 27.861568927764893, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1481
=== ep: 1482, time 26.46500277519226, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1482
=== ep: 1483, time 27.591678380966187, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1483
=== ep: 1484, time 28.740442991256714, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1484
=== ep: 1485, time 22.881616830825806, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1485
=== ep: 1486, time 28.683233499526978, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1486
=== ep: 1487, time 25.51194405555725, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1487
=== ep: 1488, time 25.801716566085815, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1488
=== ep: 1489, time 150.14881658554077, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1489
goal_identified
goal_identified
=== ep: 1490, time 26.843524932861328, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1490
=== ep: 1491, time 26.522799253463745, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1491
=== ep: 1492, time 25.10384440422058, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1492
=== ep: 1493, time 26.34203004837036, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1493
=== ep: 1494, time 28.1488094329834, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1494
=== ep: 1495, time 26.22957968711853, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1495
goal_identified
=== ep: 1496, time 27.651553869247437, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1496
=== ep: 1497, time 28.857970476150513, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1497
=== ep: 1498, time 27.236515998840332, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1498
=== ep: 1499, time 136.27073526382446, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1499
goal_identified
=== ep: 1500, time 25.838782787322998, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1500
goal_identified
=== ep: 1501, time 27.35110354423523, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1501
=== ep: 1502, time 25.67926597595215, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1502
=== ep: 1503, time 32.101927518844604, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1503
=== ep: 1504, time 25.303278923034668, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1504
=== ep: 1505, time 33.13986158370972, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1505
=== ep: 1506, time 27.458580493927002, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1506
=== ep: 1507, time 31.195368766784668, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1507
=== ep: 1508, time 25.62278985977173, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1508
=== ep: 1509, time 122.23874926567078, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1509
=== ep: 1510, time 29.638339519500732, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1510
=== ep: 1511, time 27.83222198486328, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1511
=== ep: 1512, time 27.065389156341553, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1512
=== ep: 1513, time 25.583679914474487, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1513
=== ep: 1514, time 28.804274082183838, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1514
=== ep: 1515, time 31.514321088790894, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1515
=== ep: 1516, time 28.418739318847656, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1516
=== ep: 1517, time 29.77088451385498, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1517
=== ep: 1518, time 25.787478923797607, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1518
goal_identified
=== ep: 1519, time 141.10767722129822, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1519
=== ep: 1520, time 26.75122594833374, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1520
goal_identified
=== ep: 1521, time 26.873997449874878, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1521
=== ep: 1522, time 28.65115737915039, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1522
=== ep: 1523, time 27.67381978034973, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1523
=== ep: 1524, time 29.782206773757935, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1524
=== ep: 1525, time 26.182881355285645, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1525
=== ep: 1526, time 27.482765436172485, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1526
=== ep: 1527, time 28.314424514770508, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1527
=== ep: 1528, time 29.047779321670532, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1528
=== ep: 1529, time 128.27689123153687, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1529
goal_identified
=== ep: 1530, time 27.879512071609497, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1530
=== ep: 1531, time 28.4641695022583, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1531
=== ep: 1532, time 29.973308563232422, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1532
goal_identified
=== ep: 1533, time 30.913674354553223, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1533
=== ep: 1534, time 26.762484788894653, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1534
=== ep: 1535, time 26.382960081100464, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1535
goal_identified
=== ep: 1536, time 27.34062361717224, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1536
=== ep: 1537, time 27.632097005844116, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1537
=== ep: 1538, time 29.220895528793335, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1538
=== ep: 1539, time 118.70267176628113, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1539
=== ep: 1540, time 27.60614275932312, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1540
=== ep: 1541, time 28.96611738204956, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1541
=== ep: 1542, time 31.209031105041504, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1542
=== ep: 1543, time 34.66883087158203, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1543
goal_identified
=== ep: 1544, time 25.856850385665894, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1544
=== ep: 1545, time 26.096047163009644, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1545
=== ep: 1546, time 28.484301805496216, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1546
goal_identified
=== ep: 1547, time 29.06289768218994, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1547
=== ep: 1548, time 27.67417025566101, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1548
=== ep: 1549, time 150.844384431839, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1549
goal_identified
=== ep: 1550, time 28.615298986434937, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1550
=== ep: 1551, time 24.921314239501953, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1551
goal_identified
=== ep: 1552, time 26.405869960784912, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1552
goal_identified
=== ep: 1553, time 26.691754579544067, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1553
goal_identified
goal_identified
=== ep: 1554, time 27.000202655792236, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1554
=== ep: 1555, time 26.49724006652832, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1555
=== ep: 1556, time 28.146700382232666, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1556
goal_identified
goal_identified
=== ep: 1557, time 27.14490270614624, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1557
=== ep: 1558, time 26.5034396648407, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1558
=== ep: 1559, time 142.15243196487427, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1559
=== ep: 1560, time 27.063703536987305, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1560
goal_identified
=== ep: 1561, time 27.995834350585938, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1561
=== ep: 1562, time 28.184919595718384, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1562
=== ep: 1563, time 27.450732707977295, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1563
goal_identified
=== ep: 1564, time 27.243983507156372, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1564
=== ep: 1565, time 24.8521409034729, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1565
=== ep: 1566, time 28.194284677505493, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1566
goal_identified
=== ep: 1567, time 29.218794584274292, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1567
=== ep: 1568, time 28.962388038635254, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1568
=== ep: 1569, time 122.36683630943298, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1569
=== ep: 1570, time 29.25494074821472, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1570
=== ep: 1571, time 27.078208208084106, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1571
goal_identified
=== ep: 1572, time 28.501400470733643, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1572
=== ep: 1573, time 25.966981649398804, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1573
goal_identified
=== ep: 1574, time 25.896172761917114, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1574
=== ep: 1575, time 25.251540422439575, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1575
goal_identified
=== ep: 1576, time 25.979553699493408, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1576
=== ep: 1577, time 28.976943492889404, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1577
=== ep: 1578, time 29.2737877368927, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1578
goal_identified
=== ep: 1579, time 130.9772515296936, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1579
=== ep: 1580, time 28.404338359832764, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1580
goal_identified
goal_identified
=== ep: 1581, time 28.985142469406128, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1581
=== ep: 1582, time 26.03288197517395, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1582
=== ep: 1583, time 26.914339542388916, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1583
goal_identified
=== ep: 1584, time 27.766818046569824, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1584
=== ep: 1585, time 28.310251235961914, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1585
=== ep: 1586, time 28.04309916496277, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1586
=== ep: 1587, time 29.08165431022644, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1587
=== ep: 1588, time 28.522151947021484, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1588
=== ep: 1589, time 135.73557901382446, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1589
goal_identified
=== ep: 1590, time 27.175776958465576, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1590
=== ep: 1591, time 28.7953360080719, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1591
=== ep: 1592, time 29.050464153289795, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1592
=== ep: 1593, time 32.000079870224, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1593
=== ep: 1594, time 34.39297080039978, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1594
=== ep: 1595, time 26.863955974578857, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1595
=== ep: 1596, time 27.473647356033325, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1596
=== ep: 1597, time 28.976202964782715, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1597
=== ep: 1598, time 27.1596736907959, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1598
=== ep: 1599, time 121.29005980491638, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1599
=== ep: 1600, time 29.16917610168457, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1600
=== ep: 1601, time 27.635089635849, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1601
goal_identified
=== ep: 1602, time 29.234816789627075, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1602
goal_identified
=== ep: 1603, time 30.436442136764526, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1603
goal_identified
=== ep: 1604, time 26.72424054145813, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1604
=== ep: 1605, time 24.235499620437622, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1605
=== ep: 1606, time 26.481487274169922, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1606
goal_identified
=== ep: 1607, time 28.305482149124146, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1607
=== ep: 1608, time 24.80561113357544, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1608
=== ep: 1609, time 134.62723064422607, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1609
goal_identified
=== ep: 1610, time 27.64047384262085, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1610
=== ep: 1611, time 27.29088854789734, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1611
=== ep: 1612, time 31.533681631088257, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1612
=== ep: 1613, time 27.771770238876343, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1613
=== ep: 1614, time 27.794968128204346, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1614
=== ep: 1615, time 32.12988471984863, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1615
goal_identified
goal_identified
=== ep: 1616, time 27.808350324630737, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1616
=== ep: 1617, time 28.467681169509888, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1617
=== ep: 1618, time 27.154173612594604, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1618
goal_identified
=== ep: 1619, time 139.57737946510315, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1619
=== ep: 1620, time 30.39968967437744, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1620
=== ep: 1621, time 26.218517780303955, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1621
goal_identified
=== ep: 1622, time 28.89054584503174, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1622
goal_identified
=== ep: 1623, time 26.643882989883423, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1623
=== ep: 1624, time 28.522167682647705, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1624
=== ep: 1625, time 27.98676872253418, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1625
=== ep: 1626, time 24.20160222053528, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1626
=== ep: 1627, time 35.57709193229675, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1627
goal_identified
=== ep: 1628, time 28.858153581619263, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1628
=== ep: 1629, time 146.28908848762512, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1629
=== ep: 1630, time 30.03490161895752, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1630
goal_identified
=== ep: 1631, time 26.558401346206665, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1631
=== ep: 1632, time 26.71170139312744, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1632
goal_identified
=== ep: 1633, time 27.35910677909851, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1633
goal_identified
=== ep: 1634, time 26.6273033618927, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1634
goal_identified
=== ep: 1635, time 27.560336351394653, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1635
goal_identified
=== ep: 1636, time 28.403502464294434, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1636
=== ep: 1637, time 28.184747219085693, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1637
=== ep: 1638, time 28.0746910572052, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1638
=== ep: 1639, time 138.24294781684875, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1639
=== ep: 1640, time 26.824750423431396, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1640
=== ep: 1641, time 26.317703247070312, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1641
=== ep: 1642, time 26.379454374313354, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1642
=== ep: 1643, time 27.16155242919922, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1643
=== ep: 1644, time 26.424556493759155, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1644
=== ep: 1645, time 27.016889095306396, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1645
goal_identified
=== ep: 1646, time 28.7122061252594, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1646
=== ep: 1647, time 28.538297414779663, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1647
goal_identified
=== ep: 1648, time 28.39870810508728, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1648
goal_identified
=== ep: 1649, time 129.59246182441711, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1649
goal_identified
=== ep: 1650, time 26.475958824157715, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1650
=== ep: 1651, time 27.043511867523193, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1651
goal_identified
=== ep: 1652, time 29.18784737586975, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1652
=== ep: 1653, time 28.922444105148315, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1653
=== ep: 1654, time 32.09117245674133, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1654
=== ep: 1655, time 27.181954622268677, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1655
=== ep: 1656, time 30.71741819381714, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1656
goal_identified
=== ep: 1657, time 28.825442790985107, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1657
=== ep: 1658, time 27.06375241279602, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1658
=== ep: 1659, time 135.2630054950714, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1659
=== ep: 1660, time 30.60033869743347, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1660
=== ep: 1661, time 29.848377466201782, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1661
=== ep: 1662, time 28.652721405029297, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1662
=== ep: 1663, time 27.649573802947998, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1663
=== ep: 1664, time 30.411272764205933, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1664
=== ep: 1665, time 31.39599347114563, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1665
=== ep: 1666, time 27.251527547836304, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1666
=== ep: 1667, time 27.71013593673706, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1667
goal_identified
=== ep: 1668, time 28.312323570251465, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1668
=== ep: 1669, time 123.25555872917175, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1669
=== ep: 1670, time 27.16778540611267, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1670
=== ep: 1671, time 28.44608426094055, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1671
=== ep: 1672, time 30.221360445022583, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1672
=== ep: 1673, time 29.779338359832764, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1673
=== ep: 1674, time 27.213920831680298, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1674
=== ep: 1675, time 25.58102297782898, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1675
=== ep: 1676, time 26.526048183441162, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1676
=== ep: 1677, time 39.34377098083496, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1677
=== ep: 1678, time 29.640979051589966, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1678
=== ep: 1679, time 127.67963480949402, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1679
=== ep: 1680, time 25.63925313949585, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1680
goal_identified
goal_identified
=== ep: 1681, time 29.25156331062317, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1681
=== ep: 1682, time 28.153019905090332, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1682
goal_identified
goal_identified
=== ep: 1683, time 27.99445867538452, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1683
=== ep: 1684, time 28.031818389892578, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1684
=== ep: 1685, time 28.79991054534912, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1685
=== ep: 1686, time 28.3190860748291, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1686
=== ep: 1687, time 27.692014932632446, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1687
=== ep: 1688, time 25.657835960388184, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1688
=== ep: 1689, time 146.8187747001648, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1689
=== ep: 1690, time 32.137399435043335, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1690
=== ep: 1691, time 27.043107509613037, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1691
=== ep: 1692, time 28.69797420501709, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1692
=== ep: 1693, time 36.16099667549133, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1693
=== ep: 1694, time 28.329787254333496, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1694
=== ep: 1695, time 25.371381282806396, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1695
goal_identified
=== ep: 1696, time 30.232076406478882, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1696
=== ep: 1697, time 26.961750745773315, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1697
=== ep: 1698, time 28.534499883651733, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1698
=== ep: 1699, time 143.8258934020996, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1699
=== ep: 1700, time 28.166850090026855, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1700
=== ep: 1701, time 27.35063624382019, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1701
=== ep: 1702, time 27.014026165008545, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1702
=== ep: 1703, time 27.972134590148926, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1703
=== ep: 1704, time 28.17908763885498, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1704
goal_identified
=== ep: 1705, time 28.694302082061768, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1705
=== ep: 1706, time 30.213608741760254, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1706
=== ep: 1707, time 25.854981422424316, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1707
goal_identified
=== ep: 1708, time 27.580133199691772, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1708
goal_identified
=== ep: 1709, time 139.27569389343262, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1709
=== ep: 1710, time 27.86742329597473, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1710
goal_identified
goal_identified
=== ep: 1711, time 27.9931743144989, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1711
=== ep: 1712, time 27.889092922210693, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1712
=== ep: 1713, time 26.90622091293335, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1713
=== ep: 1714, time 26.833659172058105, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1714
=== ep: 1715, time 26.009904861450195, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1715
=== ep: 1716, time 28.4046630859375, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1716
goal_identified
=== ep: 1717, time 26.746427536010742, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1717
=== ep: 1718, time 28.55851697921753, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1718
=== ep: 1719, time 135.8796741962433, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1719
=== ep: 1720, time 27.472176551818848, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1720
goal_identified
=== ep: 1721, time 27.581875562667847, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1721
goal_identified
=== ep: 1722, time 27.769643306732178, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1722
=== ep: 1723, time 28.251665353775024, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1723
=== ep: 1724, time 28.88490056991577, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1724
=== ep: 1725, time 28.56952691078186, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1725
=== ep: 1726, time 31.542296409606934, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1726
=== ep: 1727, time 30.717978715896606, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1727
goal_identified
=== ep: 1728, time 27.385231018066406, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1728
=== ep: 1729, time 138.23473930358887, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1729
goal_identified
=== ep: 1730, time 27.545347452163696, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1730
=== ep: 1731, time 28.991640329360962, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1731
=== ep: 1732, time 38.88624954223633, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1732
=== ep: 1733, time 28.214048624038696, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1733
=== ep: 1734, time 28.237944841384888, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1734
=== ep: 1735, time 28.215320348739624, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1735
goal_identified
=== ep: 1736, time 26.476078271865845, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1736
goal_identified
goal_identified
goal_identified
=== ep: 1737, time 27.720099687576294, eps 0.001, sum reward: 3, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1737
goal_identified
=== ep: 1738, time 30.176270008087158, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1738
=== ep: 1739, time 127.1876437664032, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1739
goal_identified
goal_identified
=== ep: 1740, time 28.47775411605835, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1740
=== ep: 1741, time 28.307419538497925, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1741
goal_identified
=== ep: 1742, time 28.10172390937805, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1742
goal_identified
=== ep: 1743, time 28.760289669036865, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1743
=== ep: 1744, time 31.03732919692993, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1744
goal_identified
=== ep: 1745, time 29.695549964904785, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1745
=== ep: 1746, time 28.823540687561035, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1746
goal_identified
=== ep: 1747, time 27.75252652168274, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1747
=== ep: 1748, time 28.087362051010132, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1748
=== ep: 1749, time 143.36863684654236, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1749
=== ep: 1750, time 29.270696878433228, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1750
=== ep: 1751, time 29.281094551086426, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1751
=== ep: 1752, time 29.168514728546143, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1752
goal_identified
=== ep: 1753, time 28.31635069847107, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1753
=== ep: 1754, time 28.754876852035522, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1754
=== ep: 1755, time 26.594131231307983, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1755
=== ep: 1756, time 27.165102243423462, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1756
=== ep: 1757, time 26.455230474472046, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1757
=== ep: 1758, time 27.605237245559692, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1758
goal_identified
=== ep: 1759, time 126.381178855896, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1759
=== ep: 1760, time 29.84845733642578, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1760
goal_identified
=== ep: 1761, time 27.26360011100769, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1761
goal_identified
=== ep: 1762, time 27.668556928634644, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1762
=== ep: 1763, time 29.847259044647217, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1763
=== ep: 1764, time 27.43188786506653, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1764
goal_identified
goal_identified
=== ep: 1765, time 27.293293714523315, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1765
=== ep: 1766, time 27.758582592010498, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1766
=== ep: 1767, time 27.264022827148438, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1767
goal_identified
=== ep: 1768, time 27.192973852157593, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1768
=== ep: 1769, time 147.52049207687378, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1769
=== ep: 1770, time 28.766030311584473, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1770
=== ep: 1771, time 26.14240050315857, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1771
=== ep: 1772, time 27.05713701248169, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1772
=== ep: 1773, time 28.681859493255615, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1773
goal_identified
goal_identified
=== ep: 1774, time 28.059637308120728, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1774
goal_identified
=== ep: 1775, time 30.76740527153015, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1775
goal_identified
=== ep: 1776, time 27.788398504257202, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1776
goal_identified
=== ep: 1777, time 27.071210384368896, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1777
=== ep: 1778, time 27.02982759475708, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1778
goal_identified
=== ep: 1779, time 146.91872906684875, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1779
goal_identified
=== ep: 1780, time 28.21663784980774, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1780
=== ep: 1781, time 29.24468159675598, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1781
=== ep: 1782, time 28.646284580230713, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1782
=== ep: 1783, time 27.326087951660156, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1783
goal_identified
=== ep: 1784, time 26.934075117111206, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1784
=== ep: 1785, time 28.17993450164795, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1785
=== ep: 1786, time 31.00157618522644, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1786
=== ep: 1787, time 27.270705699920654, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1787
=== ep: 1788, time 27.616283178329468, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1788
goal_identified
=== ep: 1789, time 140.27564477920532, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1789
=== ep: 1790, time 27.6207013130188, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1790
=== ep: 1791, time 33.0176568031311, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1791
=== ep: 1792, time 30.271381378173828, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1792
=== ep: 1793, time 29.099022150039673, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1793
=== ep: 1794, time 29.65177869796753, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1794
=== ep: 1795, time 27.36215901374817, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1795
goal_identified
=== ep: 1796, time 27.37608289718628, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1796
=== ep: 1797, time 26.716289043426514, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1797
goal_identified
=== ep: 1798, time 30.466347694396973, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1798
=== ep: 1799, time 131.8493263721466, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1799
=== ep: 1800, time 31.09186840057373, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1800
=== ep: 1801, time 27.740275144577026, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1801
goal_identified
=== ep: 1802, time 28.971426725387573, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1802
goal_identified
=== ep: 1803, time 28.03888702392578, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1803
=== ep: 1804, time 24.74628233909607, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1804
goal_identified
=== ep: 1805, time 27.606510162353516, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1805
goal_identified
=== ep: 1806, time 30.177740812301636, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1806
=== ep: 1807, time 31.152153491973877, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1807
=== ep: 1808, time 30.436655282974243, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1808
=== ep: 1809, time 143.3674988746643, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1809
=== ep: 1810, time 27.235054969787598, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1810
=== ep: 1811, time 27.93962264060974, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1811
goal_identified
=== ep: 1812, time 27.77627658843994, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1812
goal_identified
=== ep: 1813, time 29.417683124542236, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1813
=== ep: 1814, time 28.756044626235962, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1814
=== ep: 1815, time 31.02878499031067, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1815
=== ep: 1816, time 29.42135190963745, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1816
=== ep: 1817, time 24.714929580688477, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1817
goal_identified
goal_identified
=== ep: 1818, time 27.279412031173706, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1818
goal_identified
=== ep: 1819, time 130.41425728797913, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1819
=== ep: 1820, time 28.905547380447388, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1820
=== ep: 1821, time 28.047235012054443, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1821
=== ep: 1822, time 30.622464179992676, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1822
=== ep: 1823, time 30.436272144317627, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1823
goal_identified
=== ep: 1824, time 30.213815212249756, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1824
=== ep: 1825, time 30.012492179870605, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1825
=== ep: 1826, time 28.51814842224121, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1826
=== ep: 1827, time 28.280067682266235, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1827
=== ep: 1828, time 29.435100078582764, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1828
=== ep: 1829, time 142.16904854774475, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1829
=== ep: 1830, time 29.207787036895752, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1830
=== ep: 1831, time 28.625237703323364, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1831
=== ep: 1832, time 27.758691787719727, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1832
=== ep: 1833, time 26.856885194778442, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1833
=== ep: 1834, time 27.11557626724243, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1834
=== ep: 1835, time 26.93042254447937, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1835
=== ep: 1836, time 26.8652560710907, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1836
=== ep: 1837, time 26.165348291397095, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1837
=== ep: 1838, time 27.62344789505005, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1838
goal_identified
goal_identified
=== ep: 1839, time 138.315584897995, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1839
=== ep: 1840, time 27.502843856811523, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1840
goal_identified
=== ep: 1841, time 29.751973628997803, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1841
=== ep: 1842, time 30.7955002784729, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1842
=== ep: 1843, time 31.73865580558777, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1843
goal_identified
goal_identified
=== ep: 1844, time 28.415135622024536, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1844
=== ep: 1845, time 29.187121629714966, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1845
=== ep: 1846, time 26.89123797416687, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1846
=== ep: 1847, time 29.178272008895874, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1847
=== ep: 1848, time 26.82525134086609, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1848
goal_identified
=== ep: 1849, time 124.47258138656616, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1849
=== ep: 1850, time 30.487544536590576, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1850
goal_identified
=== ep: 1851, time 29.11781334877014, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1851
=== ep: 1852, time 27.3343608379364, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1852
=== ep: 1853, time 26.713513374328613, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1853
=== ep: 1854, time 26.992473602294922, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1854
=== ep: 1855, time 29.233988523483276, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1855
goal_identified
goal_identified
=== ep: 1856, time 29.327309131622314, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1856
=== ep: 1857, time 27.189844131469727, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1857
=== ep: 1858, time 30.99670648574829, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1858
=== ep: 1859, time 143.18086791038513, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1859
=== ep: 1860, time 28.610573291778564, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1860
=== ep: 1861, time 31.56312108039856, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1861
goal_identified
=== ep: 1862, time 28.037832260131836, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1862
=== ep: 1863, time 31.187634706497192, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1863
=== ep: 1864, time 28.489240407943726, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1864
=== ep: 1865, time 27.234858512878418, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1865
goal_identified
goal_identified
=== ep: 1866, time 28.066004276275635, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1866
goal_identified
goal_identified
=== ep: 1867, time 30.91960048675537, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1867
=== ep: 1868, time 30.669228315353394, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1868
=== ep: 1869, time 144.1098189353943, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1869
goal_identified
=== ep: 1870, time 27.82850480079651, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1870
=== ep: 1871, time 27.45822763442993, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1871
=== ep: 1872, time 29.11730122566223, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1872
=== ep: 1873, time 28.774045944213867, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1873
goal_identified
goal_identified
=== ep: 1874, time 28.226720809936523, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1874
=== ep: 1875, time 28.991907358169556, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1875
=== ep: 1876, time 29.378187656402588, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1876
goal_identified
=== ep: 1877, time 28.68511176109314, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1877
=== ep: 1878, time 28.563578844070435, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1878
=== ep: 1879, time 141.352210521698, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1879
=== ep: 1880, time 27.10504150390625, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1880
=== ep: 1881, time 26.646738290786743, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1881
=== ep: 1882, time 28.36180281639099, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1882
=== ep: 1883, time 33.367329835891724, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1883
=== ep: 1884, time 28.55901527404785, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1884
=== ep: 1885, time 27.44449281692505, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1885
=== ep: 1886, time 32.390545129776, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1886
=== ep: 1887, time 29.01835322380066, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1887
=== ep: 1888, time 28.49573540687561, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1888
goal_identified
=== ep: 1889, time 124.4307279586792, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1889
=== ep: 1890, time 30.00848078727722, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1890
=== ep: 1891, time 30.163082122802734, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1891
=== ep: 1892, time 27.749700784683228, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1892
goal_identified
=== ep: 1893, time 27.950656175613403, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1893
=== ep: 1894, time 27.715137004852295, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1894
goal_identified
=== ep: 1895, time 30.535276651382446, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1895
=== ep: 1896, time 28.828580379486084, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1896
goal_identified
=== ep: 1897, time 28.098008155822754, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1897
goal_identified
=== ep: 1898, time 27.330285787582397, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1898
