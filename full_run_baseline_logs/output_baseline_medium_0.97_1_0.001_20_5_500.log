==> Playing in 11_vs_11_stochastic.
==>Level 1
==>OTs in this level are dict_keys(['win_game'])
==>Currently learning win_game
==>using device cuda
==>critic has 5 layers and 500 hidden units.
goal_identified
=== ep: 0, time 26.53839945793152, eps 0.9, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
=== ep: 1, time 26.556500911712646, eps 0.8561552526261419, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 2, time 27.41971182823181, eps 0.8144488388143276, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 3, time 28.258096933364868, eps 0.774776470806127, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
goal_identified
=== ep: 4, time 29.03299307823181, eps 0.7370389470171057, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 5, time 29.68439269065857, eps 0.701141903981193, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 6, time 31.376036167144775, eps 0.6669955803928644, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001)
=== ep: 7, time 32.30710220336914, eps 0.6345145926571234, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
=== ep: 8, time 37.18465709686279, eps 0.6036177213860398, sum reward: 0, score_diff -4, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 9, time 33.77241230010986, eps 0.5742277083079742, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
=== ep: 10, time 34.57087850570679, eps 0.5462710630816575, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 11, time 35.32978391647339, eps 0.5196778795320575, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 12, time 36.298272132873535, eps 0.49438166084852986, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
=== ep: 13, time 37.182939529418945, eps 0.47031915330815344, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
=== ep: 14, time 40.733811378479004, eps 0.4474301881084772, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 15, time 43.712655782699585, eps 0.42565753091417224, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 16, time 46.91714549064636, eps 0.4049467387413822, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
goal_identified
=== ep: 17, time 48.946619510650635, eps 0.3852460238219053, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
=== ep: 18, time 47.5510733127594, eps 0.3665061241067986, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
goal_identified
goal_identified
=== ep: 19, time 47.95079183578491, eps 0.3486801800855966, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 8
=== ep: 20, time 46.277369260787964, eps 0.3317236176131267, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 7
goal_identified
=== ep: 21, time 45.71226215362549, eps 0.31559403645092865, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 11
goal_identified
goal_identified
goal_identified
goal_identified
=== ep: 22, time 47.315269231796265, eps 0.3002511042445735, sum reward: 4, score_diff 4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 20
=== ep: 23, time 49.12404775619507, eps 0.2856564556717689, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 1
=== ep: 24, time 52.77126145362854, eps 0.27177359650906974, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 24
goal_identified
=== ep: 25, time 56.18844485282898, eps 0.2585678123773109, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 10
goal_identified
=== ep: 26, time 50.9433970451355, eps 0.24600608193757734, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 14
goal_identified
=== ep: 27, time 59.7782187461853, eps 0.23405699432065646, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 17
goal_identified
=== ep: 28, time 54.42082238197327, eps 0.22269067058350425, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 5
=== ep: 29, time 56.039592266082764, eps 0.2118786889963241, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 29
=== ep: 30, time 55.14566159248352, eps 0.2015940139734384, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 6
=== ep: 31, time 53.55660676956177, eps 0.191810928470242, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 31
=== ep: 32, time 51.2760283946991, eps 0.1825049696771952, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 13
=== ep: 33, time 48.13418126106262, eps 0.17365286785005798, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 33
goal_identified
=== ep: 34, time 48.16781759262085, eps 0.16523248812340846, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 34
goal_identified
=== ep: 35, time 51.64741349220276, eps 0.15722277516195018, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 18
goal_identified
=== ep: 36, time 54.211660385131836, eps 0.1496037005112063, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 21
=== ep: 37, time 58.749605655670166, eps 0.14235621251595124, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 37
goal_identified
=== ep: 38, time 66.58979153633118, eps 0.13546218868114893, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 23
=== ep: 39, time 57.98692512512207, eps 0.1289043903562757, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 39
goal_identified
goal_identified
=== ep: 40, time 60.8843731880188, eps 0.12266641962971482, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 30
goal_identified
goal_identified
=== ep: 41, time 61.188897132873535, eps 0.116732678325436, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 32
goal_identified
=== ep: 42, time 61.27421259880066, eps 0.11108832899943073, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 40
=== ep: 43, time 68.15072894096375, eps 0.10571925783837377, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 43
goal_identified
=== ep: 44, time 59.60065770149231, eps 0.10061203936773815, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 42
=== ep: 45, time 58.483158349990845, eps 0.09575390288111604, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 45
=== ep: 46, time 53.95591402053833, eps 0.09113270050680057, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 46
=== ep: 47, time 51.35999608039856, eps 0.08673687683177911, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 47
goal_identified
goal_identified
goal_identified
=== ep: 48, time 52.20582675933838, eps 0.08255544000718185, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 0
goal_identified
=== ep: 49, time 62.369030237197876, eps 0.07857793426293408, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 2
goal_identified
=== ep: 50, time 63.83320713043213, eps 0.07479441376288502, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 50
goal_identified
=== ep: 51, time 66.18414831161499, eps 0.0711954177350367, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 51
=== ep: 52, time 66.06667399406433, eps 0.06777194681468615, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 52
=== ep: 53, time 75.05525207519531, eps 0.06451544054132621, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 53
goal_identified
=== ep: 54, time 67.93061089515686, eps 0.06141775595303503, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 9
goal_identified
=== ep: 55, time 70.41128015518188, eps 0.05847114722483011, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 12
goal_identified
=== ep: 56, time 72.26105332374573, eps 0.05566824630007096, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 56
goal_identified
=== ep: 57, time 70.71855401992798, eps 0.05300204446647978, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 57
=== ep: 58, time 80.9222342967987, eps 0.050465874830710106, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 58
=== ep: 59, time 67.71815276145935, eps 0.04805339564764071, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 59
=== ep: 60, time 70.36173248291016, eps 0.045758574462709686, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 60
=== ep: 61, time 68.04570174217224, eps 0.043575673027635695, sum reward: 0, score_diff -4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 61
goal_identified
=== ep: 62, time 76.56954860687256, eps 0.04149923295180846, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 16
=== ep: 63, time 68.29292249679565, eps 0.03952406205346913, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 63
=== ep: 64, time 68.2221930027008, eps 0.03764522137655123, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 64
=== ep: 65, time 69.3419280052185, eps 0.03585801284071809, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 65
goal_identified
=== ep: 66, time 67.91793274879456, eps 0.034157967493714775, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 19
goal_identified
=== ep: 67, time 78.45574998855591, eps 0.03254083433665968, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 25
goal_identified
=== ep: 68, time 71.2528829574585, eps 0.031002569694333147, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 68
=== ep: 69, time 71.82762622833252, eps 0.02953932710388308, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 69
=== ep: 70, time 71.91601037979126, eps 0.028147447696664333, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 70
goal_identified
=== ep: 71, time 80.69585013389587, eps 0.026823451049161253, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 71
=== ep: 72, time 74.30248785018921, eps 0.025564026480116013, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 72
=== ep: 73, time 89.61509299278259, eps 0.02436602477210106, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 73
goal_identified
=== ep: 74, time 80.4742043018341, eps 0.02322645029683511, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 26
=== ep: 75, time 92.22228837013245, eps 0.02214245352455219, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 75
goal_identified
=== ep: 76, time 80.36688566207886, eps 0.02111132389869288, sum reward: 1, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 76
=== ep: 77, time 75.62785124778748, eps 0.020130483058101077, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 77
goal_identified
goal_identified
goal_identified
=== ep: 78, time 74.66759419441223, eps 0.019197478389778148, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 27
=== ep: 79, time 95.97818684577942, eps 0.018309976896072843, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 79
=== ep: 80, time 75.11237144470215, eps 0.017465759360972027, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 80
=== ep: 81, time 75.34667587280273, eps 0.01666271480090467, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 81
=== ep: 82, time 74.98658514022827, eps 0.015898835186183367, sum reward: 0, score_diff -4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 82
=== ep: 83, time 83.54721426963806, eps 0.015172210419884185, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 83
=== ep: 84, time 82.2464816570282, eps 0.014481023561609456, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 84
=== ep: 85, time 79.19349813461304, eps 0.01382354628419033, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 85
goal_identified
=== ep: 86, time 82.67308449745178, eps 0.013198134551968641, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 28
=== ep: 87, time 88.65382361412048, eps 0.012603224509851407, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 87
=== ep: 88, time 87.35044384002686, eps 0.012037328572858524, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 88
=== ep: 89, time 78.67624735832214, eps 0.011499031706385502, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 89
goal_identified
=== ep: 90, time 76.49846839904785, eps 0.010986987887879832, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 90
=== ep: 91, time 76.41322445869446, eps 0.010499916741083536, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 91
=== ep: 92, time 95.40803408622742, eps 0.010036600334425595, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 92
=== ep: 93, time 79.78475069999695, eps 0.00959588013555861, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 93
goal_identified
=== ep: 94, time 78.64167189598083, eps 0.009176654114424539, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 35
goal_identified
=== ep: 95, time 85.68068599700928, eps 0.00877787398760545, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 95
=== ep: 96, time 97.36519193649292, eps 0.008398542597069007, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 96
goal_identified
=== ep: 97, time 83.95361256599426, eps 0.008037711416753971, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 97
=== ep: 98, time 77.47482991218567, eps 0.00769447818076098, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 98
=== ep: 99, time 79.84251236915588, eps 0.007367984627217855, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 99
=== ep: 100, time 89.39162993431091, eps 0.007057414352177835, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 100
=== ep: 101, time 78.3588924407959, eps 0.006761990768184489, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 101
=== ep: 102, time 78.95324349403381, eps 0.006480975162398559, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 102
=== ep: 103, time 83.12837433815002, eps 0.006213664849431085, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 103
=== ep: 104, time 95.71403169631958, eps 0.005959391414263934, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 104
=== ep: 105, time 88.37564659118652, eps 0.005717519040864065, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 105
=== ep: 106, time 79.98068070411682, eps 0.005487442922312285, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 106
=== ep: 107, time 81.79240560531616, eps 0.005268587748470919, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 107
=== ep: 108, time 90.67415642738342, eps 0.005060406267408787, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 108
=== ep: 109, time 82.12584066390991, eps 0.004862377916986354, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 109
=== ep: 110, time 79.56779789924622, eps 0.004674007523179196, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 110
=== ep: 111, time 84.0603940486908, eps 0.004494824061885041, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 111
=== ep: 112, time 101.7794497013092, eps 0.0043243794811181555, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 112
=== ep: 113, time 84.22813963890076, eps 0.0041622475806460035, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 113
goal_identified
=== ep: 114, time 79.47545504570007, eps 0.0040080229462666735, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 36
=== ep: 115, time 82.62915277481079, eps 0.0038613199360621906, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 115
=== ep: 116, time 112.73909497261047, eps 0.003721771716092858, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 116
=== ep: 117, time 93.0894501209259, eps 0.0035890293431213305, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 117
=== ep: 118, time 85.79798555374146, eps 0.0034627608920727634, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 118
goal_identified
=== ep: 119, time 84.91754484176636, eps 0.00334265062604924, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 119
=== ep: 120, time 104.7988691329956, eps 0.0032283982068230565, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 120
goal_identified
=== ep: 121, time 84.58547902107239, eps 0.0031197179438347193, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 121
=== ep: 122, time 84.86099529266357, eps 0.0030163380798177374, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 122
goal_identified
=== ep: 123, time 81.30873489379883, eps 0.0029180001112638996, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 123
=== ep: 124, time 104.12474298477173, eps 0.002824458142029865, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 124
=== ep: 125, time 92.35734510421753, eps 0.0027354782684687108, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 125
=== ep: 126, time 90.08092594146729, eps 0.0026508379945489875, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 126
=== ep: 127, time 91.59646582603455, eps 0.0025703256754987464, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 127
=== ep: 128, time 92.74340510368347, eps 0.0024937399885833667, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 128
goal_identified
=== ep: 129, time 84.2467188835144, eps 0.0024208894296938593, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 38
=== ep: 130, time 90.85730814933777, eps 0.0023515918344868374, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 130
=== ep: 131, time 121.65916275978088, eps 0.002285673922878779, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 131
=== ep: 132, time 99.10023283958435, eps 0.0022229708657555565, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 132
=== ep: 133, time 98.18191409111023, eps 0.0021633258728137976, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 133
=== ep: 134, time 96.10113286972046, eps 0.0021065898005034594, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 134
goal_identified
=== ep: 135, time 105.34125661849976, eps 0.002052620779091266, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 135
=== ep: 136, time 94.00561666488647, eps 0.0020012838579124784, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 136
=== ep: 137, time 93.82702589035034, eps 0.0019524506679239415, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 137
=== ep: 138, time 99.29994010925293, eps 0.001905999100714611, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 138
=== ep: 139, time 96.2269880771637, eps 0.001861813003170924, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 139
=== ep: 140, time 98.75823259353638, eps 0.0018197818870335101, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 140
=== ep: 141, time 83.79048562049866, eps 0.0017798006526189953, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 141
=== ep: 142, time 118.0856466293335, eps 0.0017417693260160481, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 142
=== ep: 143, time 88.56379985809326, eps 0.0017055928090985275, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 143
goal_identified
=== ep: 144, time 86.42705774307251, eps 0.0016711806417306348, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 44
=== ep: 145, time 94.70620632171631, eps 0.0016384467755694515, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 145
goal_identified
=== ep: 146, time 111.86692214012146, eps 0.0016073093588992661, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 49
=== ep: 147, time 101.71382927894592, eps 0.0015776905319596466, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 147
=== ep: 148, time 100.71180939674377, eps 0.0015495162322554856, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 148
=== ep: 149, time 90.80762791633606, eps 0.0015227160093621863, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 149
=== ep: 150, time 117.97059154510498, eps 0.0014972228487629025, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 150
=== ep: 151, time 99.17299962043762, eps 0.0014729730042773413, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 151
=== ep: 152, time 88.0252251625061, eps 0.001449905838663109, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 152
goal_identified
=== ep: 153, time 92.79256463050842, eps 0.00142796367199102, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 54
=== ep: 154, time 102.30135917663574, eps 0.0014070916374152305, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 154
=== ep: 155, time 91.9022364616394, eps 0.001387237543977543, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 155
goal_identified
=== ep: 156, time 91.26671004295349, eps 0.0013683517461028282, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 156
=== ep: 157, time 104.5312807559967, eps 0.0013503870194592265, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 157
=== ep: 158, time 96.02115201950073, eps 0.0013332984428727204, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 158
=== ep: 159, time 95.40782976150513, eps 0.001317043286000802, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 159
=== ep: 160, time 90.52977228164673, eps 0.0013015809024843582, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 160
=== ep: 161, time 119.97096037864685, eps 0.0012868726283106018, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 161
=== ep: 162, time 109.23935866355896, eps 0.0012728816851329014, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 162
=== ep: 163, time 106.55497908592224, eps 0.0012595730883057546, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 163
=== ep: 164, time 118.50017046928406, eps 0.001246913559404956, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 164
=== ep: 165, time 95.42154169082642, eps 0.0012348714430141991, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 165
goal_identified
goal_identified
goal_identified
=== ep: 166, time 92.91581559181213, eps 0.0012234166275700486, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 55
=== ep: 167, time 100.56983637809753, eps 0.001212520470067348, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 167
=== ep: 168, time 108.58020877838135, eps 0.0012021557244367845, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 168
goal_identified
=== ep: 169, time 88.37347316741943, eps 0.0011922964734155277, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 169
=== ep: 170, time 101.18443155288696, eps 0.001182918063740569, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 170
=== ep: 171, time 113.78178024291992, eps 0.0011739970445027263, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 171
=== ep: 172, time 93.23848342895508, eps 0.0011655111085071537, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 172
=== ep: 173, time 96.16982507705688, eps 0.001157439036493735, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 173
=== ep: 174, time 92.13514304161072, eps 0.0011497606440778825, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 174
=== ep: 175, time 113.64400959014893, eps 0.0011424567312790603, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 175
goal_identified
=== ep: 176, time 91.71742177009583, eps 0.0011355090345108335, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 176
=== ep: 177, time 100.85882425308228, eps 0.0011289001809123877, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 177
=== ep: 178, time 132.95552611351013, eps 0.0011226136449073282, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 178
=== ep: 179, time 94.81478142738342, eps 0.001116633706881133, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 179
=== ep: 180, time 103.74343752861023, eps 0.001110945413873925, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 180
goal_identified
=== ep: 181, time 97.82941937446594, eps 0.001105534542190287, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 181
=== ep: 182, time 131.0956814289093, eps 0.0011003875618326132, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 182
=== ep: 183, time 95.67177724838257, eps 0.0010954916026690664, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 183
=== ep: 184, time 99.35373187065125, eps 0.001090834422251547, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 184
=== ep: 185, time 127.41113638877869, eps 0.0010864043752031938, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 185
=== ep: 186, time 95.86010670661926, eps 0.0010821903840988777, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 186
=== ep: 187, time 114.52055716514587, eps 0.0010781819117658682, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 187
=== ep: 188, time 115.15497636795044, eps 0.0010743689349354123, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 188
goal_identified
=== ep: 189, time 104.49448823928833, eps 0.0010707419191793434, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 62
=== ep: 190, time 108.38817381858826, eps 0.0010672917950690429, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 190
=== ep: 191, time 102.51673340797424, eps 0.0010640099354971456, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 191
goal_identified
=== ep: 192, time 106.37867379188538, eps 0.0010608881341052777, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 66
goal_identified
=== ep: 193, time 97.51369142532349, eps 0.0010579185847638855, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 193
=== ep: 194, time 111.38867163658142, eps 0.0010550938620528466, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 194
=== ep: 195, time 138.91318726539612, eps 0.001052406902694051, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 195
goal_identified
=== ep: 196, time 101.51019501686096, eps 0.001049850987889527, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 67
=== ep: 197, time 101.476327419281, eps 0.0010474197265209469, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 197
goal_identified
=== ep: 198, time 126.16292405128479, eps 0.0010451070391685015, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 198
goal_identified
=== ep: 199, time 127.36337399482727, eps 0.001042907142909185, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 199
=== ep: 200, time 133.32159090042114, eps 0.001040814536856474, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 200
=== ep: 201, time 129.42048168182373, eps 0.0010388239884052469, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 201
=== ep: 202, time 109.44130825996399, eps 0.0010369305201475454, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 202
=== ep: 203, time 121.35484194755554, eps 0.0010351293974264616, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 203
=== ep: 204, time 145.33456230163574, eps 0.00103341611649703, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 204
=== ep: 205, time 130.35864686965942, eps 0.0010317863932645186, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 205
=== ep: 206, time 129.92414021492004, eps 0.0010302361525719613, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 206
=== ep: 207, time 120.2482578754425, eps 0.0010287615180101426, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 207
=== ep: 208, time 130.35902452468872, eps 0.001027358802224555, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 208
goal_identified
=== ep: 209, time 139.6188199520111, eps 0.0010260244976950921, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 209
=== ep: 210, time 141.99658393859863, eps 0.0010247552679654227, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 210
=== ep: 211, time 130.27787470817566, eps 0.00102354793930011, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 211
=== ep: 212, time 156.96149945259094, eps 0.0010223994927486214, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 212
=== ep: 213, time 120.9416012763977, eps 0.001021307056596379, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 213
=== ep: 214, time 127.12599325180054, eps 0.0010202678991839778, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 214
goal_identified
goal_identified
=== ep: 215, time 139.3681185245514, eps 0.0010192794220766138, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 215
=== ep: 216, time 110.41336941719055, eps 0.0010183391535666436, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 216
=== ep: 217, time 117.58189558982849, eps 0.0010174447424930286, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 217
=== ep: 218, time 139.21974778175354, eps 0.0010165939523622068, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 218
=== ep: 219, time 123.03598284721375, eps 0.0010157846557556941, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 219
=== ep: 220, time 123.09370398521423, eps 0.001015014829010431, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 220
=== ep: 221, time 135.84630489349365, eps 0.0010142825471585687, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 221
=== ep: 222, time 128.36721873283386, eps 0.0010135859791140496, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 222
goal_identified
goal_identified
=== ep: 223, time 119.64483332633972, eps 0.0010129233830939361, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 74
=== ep: 224, time 134.4048204421997, eps 0.0010122931022630473, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 224
=== ep: 225, time 117.60872530937195, eps 0.001011693560591007, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 225
=== ep: 226, time 113.21751856803894, eps 0.0010111232589113477, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 226
=== ep: 227, time 124.50732350349426, eps 0.0010105807711728136, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 227
=== ep: 228, time 123.48904418945312, eps 0.0010100647408734893, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 228
=== ep: 229, time 128.00600457191467, eps 0.001009573877668838, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 229
=== ep: 230, time 142.3670847415924, eps 0.001009106954145169, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 230
goal_identified
=== ep: 231, time 116.77246713638306, eps 0.0010086628027504636, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 86
=== ep: 232, time 117.76532649993896, eps 0.0010082403128748867, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 232
goal_identified
=== ep: 233, time 135.24903106689453, eps 0.0010078384280736842, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 94
=== ep: 234, time 116.15796089172363, eps 0.001007456143425521, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 234
=== ep: 235, time 123.34215664863586, eps 0.001007092503019653, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 235
=== ep: 236, time 147.28193974494934, eps 0.001006746597565654, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 236
=== ep: 237, time 113.45868945121765, eps 0.001006417562119715, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 237
=== ep: 238, time 111.91107201576233, eps 0.0010061045739218342, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 238
=== ep: 239, time 138.794091463089, eps 0.0010058068503384884, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 239
=== ep: 240, time 117.79074358940125, eps 0.001005523646905642, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 240
=== ep: 241, time 123.38347172737122, eps 0.001005254255467199, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 241
goal_identified
=== ep: 242, time 119.80394124984741, eps 0.0010049980024042435, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 242
=== ep: 243, time 109.05190014839172, eps 0.0010047542469506416, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 243
goal_identified
=== ep: 244, time 113.18584895133972, eps 0.0010045223795907931, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 244
=== ep: 245, time 126.57962489128113, eps 0.001004301820535524, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 245
=== ep: 246, time 131.46049451828003, eps 0.0010040920182723119, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 246
goal_identified
goal_identified
=== ep: 247, time 117.99669432640076, eps 0.0010038924481862177, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 114
=== ep: 248, time 124.60254621505737, eps 0.0010037026112480747, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 248
=== ep: 249, time 140.54161596298218, eps 0.0010035220327666559, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 249
=== ep: 250, time 128.30169987678528, eps 0.0010033502612016988, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 250
=== ep: 251, time 144.43228363990784, eps 0.001003186867034819, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 251
=== ep: 252, time 133.40182638168335, eps 0.001003031441695491, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 252
=== ep: 253, time 129.1430459022522, eps 0.0010028835965394094, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 253
=== ep: 254, time 156.79112005233765, eps 0.0010027429618766747, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 254
=== ep: 255, time 125.49938225746155, eps 0.0010026091860473767, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 255
goal_identified
=== ep: 256, time 130.90004587173462, eps 0.0010024819345422614, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 129
=== ep: 257, time 132.83619046211243, eps 0.0010023608891662839, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 257
=== ep: 258, time 137.16450238227844, eps 0.001002245747242954, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 258
=== ep: 259, time 156.010009765625, eps 0.0010021362208574892, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 259
=== ep: 260, time 139.78068614006042, eps 0.001002032036136876, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 260
=== ep: 261, time 135.27563738822937, eps 0.0010019329325650452, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 261
=== ep: 262, time 139.12226510047913, eps 0.0010018386623314465, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 262
=== ep: 263, time 131.093608379364, eps 0.0010017489897113931, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 263
=== ep: 264, time 146.3610463142395, eps 0.0010016636904766263, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 264
=== ep: 265, time 130.25032138824463, eps 0.0010015825513346283, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 265
=== ep: 266, time 140.52799725532532, eps 0.0010015053693952815, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 266
=== ep: 267, time 172.39358520507812, eps 0.0010014319516635345, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 267
goal_identified
goal_identified
=== ep: 268, time 136.54678344726562, eps 0.0010013621145568167, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 268
=== ep: 269, time 134.05073809623718, eps 0.0010012956834459848, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 269
goal_identified
=== ep: 270, time 131.13526678085327, eps 0.0010012324922186594, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 270
=== ep: 271, time 133.53799676895142, eps 0.001001172382863857, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 271
=== ep: 272, time 155.65983390808105, eps 0.0010011152050768812, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 272
=== ep: 273, time 146.38586330413818, eps 0.0010010608158834819, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 273
=== ep: 274, time 136.20563340187073, eps 0.0010010090792823456, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 274
=== ep: 275, time 159.68925762176514, eps 0.0010009598659050213, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 275
goal_identified
=== ep: 276, time 143.26525592803955, eps 0.0010009130526924313, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 144
=== ep: 277, time 145.4944725036621, eps 0.0010008685225871602, sum reward: 0, score_diff -4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 277
=== ep: 278, time 133.68572974205017, eps 0.0010008261642407504, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 278
=== ep: 279, time 142.93823099136353, eps 0.001000785871735272, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 279
=== ep: 280, time 165.34136319160461, eps 0.0010007475443184742, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 280
=== ep: 281, time 141.61780309677124, eps 0.001000711086151851, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 281
=== ep: 282, time 133.26870894432068, eps 0.0010006764060709957, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 282
=== ep: 283, time 156.8677635192871, eps 0.001000643417357642, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 283
=== ep: 284, time 157.4389488697052, eps 0.0010006120375228235, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 284
=== ep: 285, time 169.0906481742859, eps 0.0010005821881006083, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 285
goal_identified
=== ep: 286, time 161.29727697372437, eps 0.0010005537944518927, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 286
=== ep: 287, time 185.127032995224, eps 0.0010005267855777657, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 287
=== ep: 288, time 180.15060997009277, eps 0.0010005010939419733, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 288
=== ep: 289, time 178.5316071510315, eps 0.001000476655302044, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 289
=== ep: 290, time 178.1129777431488, eps 0.0010004534085486486, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 290
goal_identified
=== ep: 291, time 174.61473059654236, eps 0.0010004312955527947, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 146
=== ep: 292, time 188.76024651527405, eps 0.0010004102610204745, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 292
=== ep: 293, time 164.94551014900208, eps 0.0010003902523544011, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 293
=== ep: 294, time 191.8947296142578, eps 0.0010003712195224871, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 294
goal_identified
=== ep: 295, time 169.66227316856384, eps 0.0010003531149327387, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 295
=== ep: 296, time 184.09624075889587, eps 0.0010003358933142518, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 296
=== ep: 297, time 167.87476873397827, eps 0.0010003195116040093, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 297
=== ep: 298, time 171.7193639278412, eps 0.0010003039288392032, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 298
=== ep: 299, time 177.8405647277832, eps 0.0010002891060548044, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 299
=== ep: 300, time 171.72948026657104, eps 0.0010002750061861312, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 300
=== ep: 301, time 186.34130907058716, eps 0.0010002615939761676, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 301
=== ep: 302, time 160.23003482818604, eps 0.001000248835887403, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 302
=== ep: 303, time 169.86859488487244, eps 0.0010002367000179694, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 303
=== ep: 304, time 161.78438925743103, eps 0.0010002251560218723, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 304
=== ep: 305, time 158.3093101978302, eps 0.0010002141750331084, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 305
=== ep: 306, time 201.66285181045532, eps 0.0010002037295934862, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 306
=== ep: 307, time 171.88998818397522, eps 0.0010001937935839656, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 307
=== ep: 308, time 176.09228897094727, eps 0.0010001843421593476, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 308
=== ep: 309, time 154.58042740821838, eps 0.0010001753516861473, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 309
=== ep: 310, time 175.81723761558533, eps 0.0010001667996834991, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 310
=== ep: 311, time 151.37094044685364, eps 0.001000158664766942, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 311
=== ep: 312, time 169.01913285255432, eps 0.0010001509265949466, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 312
=== ep: 313, time 188.13784456253052, eps 0.001000143565818053, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 313
=== ep: 314, time 155.72962760925293, eps 0.0010001365640304844, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 314
=== ep: 315, time 156.84475946426392, eps 0.0010001299037241253, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 315
=== ep: 316, time 151.3531093597412, eps 0.0010001235682447402, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 316
=== ep: 317, time 156.14706540107727, eps 0.0010001175417503308, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 317
=== ep: 318, time 178.7837827205658, eps 0.0010001118091715218, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 318
=== ep: 319, time 173.2160668373108, eps 0.0010001063561738807, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 319
=== ep: 320, time 190.1102578639984, eps 0.0010001011691220727, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 320
=== ep: 321, time 161.74383926391602, eps 0.0010000962350457665, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 321
=== ep: 322, time 155.15125131607056, eps 0.0010000915416072012, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 322
=== ep: 323, time 176.19859910011292, eps 0.0010000870770703358, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 323
=== ep: 324, time 154.52829480171204, eps 0.0010000828302715028, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 324
=== ep: 325, time 202.1697211265564, eps 0.0010000787905914928, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 325
=== ep: 326, time 165.2834770679474, eps 0.0010000749479290019, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 326
=== ep: 327, time 201.35813117027283, eps 0.001000071292675372, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 327
=== ep: 328, time 180.76344084739685, eps 0.001000067815690565, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 328
goal_identified
=== ep: 329, time 170.98167538642883, eps 0.0010000645082803084, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 329
=== ep: 330, time 177.48308157920837, eps 0.0010000613621743532, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 330
=== ep: 331, time 167.37486124038696, eps 0.0010000583695057963, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 331
=== ep: 332, time 193.83621907234192, eps 0.0010000555227914069, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 332
goal_identified
=== ep: 333, time 172.64246201515198, eps 0.0010000528149129166, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 153
goal_identified
=== ep: 334, time 177.17560076713562, eps 0.0010000502390992187, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 189
=== ep: 335, time 159.16863417625427, eps 0.0010000477889094373, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 335
=== ep: 336, time 171.70068311691284, eps 0.0010000454582168217, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 336
=== ep: 337, time 162.35754346847534, eps 0.001000043241193426, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 337
=== ep: 338, time 161.45385479927063, eps 0.0010000411322955373, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 338
goal_identified
=== ep: 339, time 193.83072137832642, eps 0.0010000391262498123, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 339
=== ep: 340, time 177.1588978767395, eps 0.001000037218040092, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 340
goal_identified
=== ep: 341, time 176.49338293075562, eps 0.0010000354028948577, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 341
=== ep: 342, time 167.09232258796692, eps 0.0010000336762753012, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 342
=== ep: 343, time 157.20244812965393, eps 0.001000032033863974, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 343
=== ep: 344, time 180.52589344978333, eps 0.0010000304715539925, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 344
=== ep: 345, time 159.70636749267578, eps 0.001000028985438768, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 345
goal_identified
=== ep: 346, time 168.95867395401, eps 0.001000027571802238, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 346
=== ep: 347, time 167.88328647613525, eps 0.0010000262271095755, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 347
=== ep: 348, time 172.99952864646912, eps 0.0010000249479983478, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 348
=== ep: 349, time 168.97881174087524, eps 0.0010000237312701107, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 349
=== ep: 350, time 167.85301089286804, eps 0.00100002257388241, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 350
=== ep: 351, time 191.68302869796753, eps 0.0010000214729411737, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 351
=== ep: 352, time 167.28615283966064, eps 0.0010000204256934752, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 352
=== ep: 353, time 177.2653148174286, eps 0.0010000194295206493, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 353
=== ep: 354, time 162.51504015922546, eps 0.0010000184819317455, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 354
=== ep: 355, time 184.05831170082092, eps 0.001000017580557298, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 355
goal_identified
=== ep: 356, time 149.72440767288208, eps 0.001000016723143401, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 356
goal_identified
=== ep: 357, time 160.78669548034668, eps 0.0010000159075460732, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 357
goal_identified
=== ep: 358, time 174.77936100959778, eps 0.0010000151317258964, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 358
=== ep: 359, time 166.5999722480774, eps 0.0010000143937429161, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 359
=== ep: 360, time 175.6611349582672, eps 0.0010000136917517905, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 360
=== ep: 361, time 159.21044039726257, eps 0.001000013023997176, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 361
=== ep: 362, time 115.89052391052246, eps 0.0010000123888093385, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 362
=== ep: 363, time 129.0583941936493, eps 0.0010000117845999773, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 363
=== ep: 364, time 124.00497722625732, eps 0.0010000112098582543, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 364
=== ep: 365, time 138.28812050819397, eps 0.001000010663147016, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 365
=== ep: 366, time 144.40411114692688, eps 0.0010000101430991996, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 366
=== ep: 367, time 123.1850368976593, eps 0.0010000096484144142, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 367
=== ep: 368, time 135.93199968338013, eps 0.0010000091778556905, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 368
=== ep: 369, time 117.34967255592346, eps 0.0010000087302463867, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 369
=== ep: 370, time 115.8986508846283, eps 0.001000008304467246, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 370
=== ep: 371, time 142.93520784378052, eps 0.0010000078994535993, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 371
=== ep: 372, time 114.25241804122925, eps 0.0010000075141927012, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 372
goal_identified
goal_identified
=== ep: 373, time 128.787531375885, eps 0.0010000071477211988, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 192
goal_identified
=== ep: 374, time 171.87420439720154, eps 0.0010000067991227223, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 374
goal_identified
goal_identified
=== ep: 375, time 162.80724477767944, eps 0.0010000064675255943, sum reward: 2, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 375
=== ep: 376, time 199.96592020988464, eps 0.001000006152100649, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 376
=== ep: 377, time 186.3649742603302, eps 0.0010000058520591598, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 377
=== ep: 378, time 194.20292592048645, eps 0.0010000055666508666, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 378
=== ep: 379, time 224.517644405365, eps 0.0010000052951621003, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 379
goal_identified
=== ep: 380, time 195.71087002754211, eps 0.0010000050369139975, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 380
=== ep: 381, time 232.16439843177795, eps 0.001000004791260803, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 381
=== ep: 382, time 200.8697292804718, eps 0.0010000045575882562, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 382
=== ep: 383, time 262.29650020599365, eps 0.001000004335312054, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 383
=== ep: 384, time 229.02349638938904, eps 0.0010000041238763903, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 384
=== ep: 385, time 256.63747096061707, eps 0.0010000039227525655, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 385
=== ep: 386, time 220.93168139457703, eps 0.0010000037314376652, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 386
=== ep: 387, time 267.6574881076813, eps 0.001000003549453303, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 387
=== ep: 388, time 252.57770371437073, eps 0.0010000033763444226, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 388
=== ep: 389, time 258.73242020606995, eps 0.001000003211678162, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 389
=== ep: 390, time 265.4430773258209, eps 0.0010000030550427698, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 390
=== ep: 391, time 335.9534420967102, eps 0.0010000029060465757, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 391
=== ep: 392, time 284.0307672023773, eps 0.0010000027643170119, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 392
=== ep: 393, time 343.9813144207001, eps 0.0010000026294996803, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 393
=== ep: 394, time 285.6567962169647, eps 0.0010000025012574677, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 394
goal_identified
=== ep: 395, time 323.8052968978882, eps 0.0010000023792697014, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 196
=== ep: 396, time 307.6606605052948, eps 0.0010000022632313489, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 396
=== ep: 397, time 291.46008372306824, eps 0.0010000021528522535, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 397
goal_identified
=== ep: 398, time 326.01271891593933, eps 0.00100000204785641, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 231
=== ep: 399, time 323.97804713249207, eps 0.0010000019479812744, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 399
goal_identified
=== ep: 400, time 335.59879446029663, eps 0.0010000018529771066, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 400
goal_identified
goal_identified
=== ep: 401, time 315.3590338230133, eps 0.0010000017626063467, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 233
goal_identified
=== ep: 402, time 300.73906993865967, eps 0.0010000016766430208, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 402
=== ep: 403, time 346.30766010284424, eps 0.0010000015948721758, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 403
=== ep: 404, time 290.23961329460144, eps 0.001000001517089342, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 404
=== ep: 405, time 348.5927925109863, eps 0.0010000014431000217, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 405
=== ep: 406, time 325.6541283130646, eps 0.001000001372719203, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 406
=== ep: 407, time 303.7244031429291, eps 0.0010000013057708975, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 407
=== ep: 408, time 312.27844309806824, eps 0.0010000012420876994, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 408
goal_identified
goal_identified
goal_identified
=== ep: 409, time 331.88939332962036, eps 0.0010000011815103674, sum reward: 3, score_diff 3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 247
goal_identified
=== ep: 410, time 288.0949993133545, eps 0.001000001123887427, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 256
=== ep: 411, time 339.25038051605225, eps 0.0010000010690747903, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 411
=== ep: 412, time 328.27029848098755, eps 0.0010000010169353975, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 412
=== ep: 413, time 279.3509347438812, eps 0.0010000009673388729, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 413
=== ep: 414, time 311.0086224079132, eps 0.0010000009201611994, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 414
=== ep: 415, time 299.61514687538147, eps 0.0010000008752844081, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 415
=== ep: 416, time 302.2113366127014, eps 0.0010000008325962838, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 416
=== ep: 417, time 301.29391980171204, eps 0.001000000791990084, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 417
=== ep: 418, time 311.4522497653961, eps 0.0010000007533642718, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 418
=== ep: 419, time 337.5008473396301, eps 0.0010000007166222626, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 419
=== ep: 420, time 306.04187512397766, eps 0.0010000006816721825, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 420
=== ep: 421, time 342.8358669281006, eps 0.001000000648426638, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 421
goal_identified
=== ep: 422, time 293.4303050041199, eps 0.0010000006168024976, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 422
=== ep: 423, time 315.54042196273804, eps 0.0010000005867206849, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 423
goal_identified
goal_identified
=== ep: 424, time 302.3312568664551, eps 0.0010000005581059794, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 276
=== ep: 425, time 284.3932144641876, eps 0.0010000005308868295, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 425
=== ep: 426, time 289.8310270309448, eps 0.0010000005049951733, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 426
=== ep: 427, time 261.54374384880066, eps 0.001000000480366268, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 427
goal_identified
goal_identified
=== ep: 428, time 284.71309089660645, eps 0.0010000004569385287, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 291
=== ep: 429, time 256.90658807754517, eps 0.0010000004346533736, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 429
=== ep: 430, time 298.80578422546387, eps 0.0010000004134550786, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 430
=== ep: 431, time 281.41021370887756, eps 0.0010000003932906364, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 431
=== ep: 432, time 278.83065915107727, eps 0.0010000003741096257, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 432
=== ep: 433, time 282.98346853256226, eps 0.001000000355864084, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 433
=== ep: 434, time 307.216472864151, eps 0.0010000003385083878, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 434
=== ep: 435, time 307.25277519226074, eps 0.001000000321999139, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 435
goal_identified
=== ep: 436, time 284.4576938152313, eps 0.0010000003062950555, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 333
=== ep: 437, time 299.9623188972473, eps 0.0010000002913568694, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 437
goal_identified
goal_identified
=== ep: 438, time 267.79296469688416, eps 0.0010000002771472273, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 334
goal_identified
goal_identified
=== ep: 439, time 286.3708076477051, eps 0.0010000002636305976, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 373
=== ep: 440, time 280.1971125602722, eps 0.0010000002507731815, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 440
goal_identified
=== ep: 441, time 284.0034701824188, eps 0.0010000002385428292, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 441
=== ep: 442, time 270.9407286643982, eps 0.0010000002269089582, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 442
=== ep: 443, time 287.157107591629, eps 0.0010000002158424776, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 443
=== ep: 444, time 292.5105848312378, eps 0.0010000002053157158, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 444
=== ep: 445, time 310.7628929615021, eps 0.0010000001953023503, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 445
=== ep: 446, time 328.0535316467285, eps 0.001000000185777342, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 446
=== ep: 447, time 295.06487703323364, eps 0.0010000001767168742, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 447
=== ep: 448, time 317.7481954097748, eps 0.0010000001680982905, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 448
=== ep: 449, time 281.8764419555664, eps 0.0010000001599000403, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 449
=== ep: 450, time 305.56587529182434, eps 0.0010000001521016232, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 450
=== ep: 451, time 317.20919728279114, eps 0.0010000001446835395, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 451
goal_identified
goal_identified
goal_identified
=== ep: 452, time 268.2264165878296, eps 0.0010000001376272401, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 395
goal_identified
=== ep: 453, time 285.75921988487244, eps 0.0010000001309150804, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 398
=== ep: 454, time 284.16765689849854, eps 0.0010000001245302765, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 454
=== ep: 455, time 281.94536089897156, eps 0.0010000001184568633, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 455
=== ep: 456, time 280.9209930896759, eps 0.0010000001126796538, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 456
=== ep: 457, time 293.00419425964355, eps 0.0010000001071842023, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 457
goal_identified
=== ep: 458, time 268.8328912258148, eps 0.001000000101956767, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 458
goal_identified
=== ep: 459, time 282.46179962158203, eps 0.001000000096984277, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 401
goal_identified
=== ep: 460, time 296.991574048996, eps 0.001000000092254298, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 410
=== ep: 461, time 309.54382848739624, eps 0.0010000000877550027, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 461
goal_identified
=== ep: 462, time 283.7175886631012, eps 0.0010000000834751407, sum reward: 1, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 462
=== ep: 463, time 287.31137776374817, eps 0.00100000007940401, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 463
=== ep: 464, time 328.82911348342896, eps 0.0010000000755314307, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 464
=== ep: 465, time 312.5057315826416, eps 0.0010000000718477194, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 465
=== ep: 466, time 297.2838468551636, eps 0.0010000000683436647, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 466
=== ep: 467, time 282.06106424331665, eps 0.001000000065010505, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 467
=== ep: 468, time 290.2083468437195, eps 0.0010000000618399052, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 468
=== ep: 469, time 333.50256085395813, eps 0.0010000000588239375, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 469
=== ep: 470, time 308.4531784057617, eps 0.0010000000559550603, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 470
=== ep: 471, time 313.4554121494293, eps 0.0010000000532260998, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 471
=== ep: 472, time 273.8732273578644, eps 0.0010000000506302322, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 472
=== ep: 473, time 280.46770310401917, eps 0.0010000000481609666, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 473
=== ep: 474, time 298.1710157394409, eps 0.0010000000458121286, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 474
=== ep: 475, time 302.90946769714355, eps 0.0010000000435778447, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 475
goal_identified
=== ep: 476, time 272.10637164115906, eps 0.001000000041452528, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 476
goal_identified
goal_identified
=== ep: 477, time 278.19252276420593, eps 0.0010000000394308644, sum reward: 2, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 424
=== ep: 478, time 287.8807909488678, eps 0.0010000000375077985, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 478
goal_identified
goal_identified
=== ep: 479, time 283.74009704589844, eps 0.0010000000356785216, sum reward: 2, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 436
goal_identified
=== ep: 480, time 274.242258310318, eps 0.0010000000339384595, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 480
goal_identified
=== ep: 481, time 269.8382976055145, eps 0.0010000000322832614, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 438
goal_identified
=== ep: 482, time 259.0555992126465, eps 0.0010000000307087882, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 482
goal_identified
=== ep: 483, time 281.85770177841187, eps 0.001000000029211103, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 453
=== ep: 484, time 305.8859233856201, eps 0.0010000000277864607, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 484
=== ep: 485, time 280.0833327770233, eps 0.0010000000264312988, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 485
=== ep: 486, time 297.57245421409607, eps 0.0010000000251422292, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 486
=== ep: 487, time 275.6211621761322, eps 0.0010000000239160282, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 487
=== ep: 488, time 252.65232014656067, eps 0.00100000002274963, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 488
=== ep: 489, time 290.69094038009644, eps 0.0010000000216401172, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 489
=== ep: 490, time 276.98775148391724, eps 0.0010000000205847162, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 490
=== ep: 491, time 287.03205156326294, eps 0.0010000000195807877, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 491
goal_identified
=== ep: 492, time 275.59591484069824, eps 0.0010000000186258216, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 492
=== ep: 493, time 280.8861746788025, eps 0.0010000000177174295, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 493
=== ep: 494, time 283.23489236831665, eps 0.0010000000168533404, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 494
=== ep: 495, time 286.16230368614197, eps 0.0010000000160313932, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 495
goal_identified
goal_identified
goal_identified
=== ep: 496, time 276.2973518371582, eps 0.001000000015249533, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 459
goal_identified
=== ep: 497, time 251.1506860256195, eps 0.0010000000145058043, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 460
=== ep: 498, time 256.4789688587189, eps 0.001000000013798348, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 498
goal_identified
=== ep: 499, time 257.9415068626404, eps 0.0010000000131253947, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 479
=== ep: 500, time 250.58351969718933, eps 0.0010000000124852615, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 500
=== ep: 501, time 244.90514588356018, eps 0.0010000000118763482, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 501
=== ep: 502, time 243.5370795726776, eps 0.0010000000112971319, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 502
goal_identified
=== ep: 503, time 247.06337070465088, eps 0.0010000000107461642, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 481
=== ep: 504, time 257.25124645233154, eps 0.0010000000102220676, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 504
=== ep: 505, time 240.1576704978943, eps 0.0010000000097235315, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 505
goal_identified
=== ep: 506, time 260.63580870628357, eps 0.0010000000092493092, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 483
=== ep: 507, time 278.6453936100006, eps 0.0010000000087982152, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 507
=== ep: 508, time 244.07794213294983, eps 0.0010000000083691212, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 508
=== ep: 509, time 233.0695731639862, eps 0.0010000000079609542, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 509
=== ep: 510, time 251.59626245498657, eps 0.001000000007572694, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 510
=== ep: 511, time 254.94225025177002, eps 0.0010000000072033692, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 511
goal_identified
goal_identified
goal_identified
=== ep: 512, time 236.79377675056458, eps 0.001000000006852057, sum reward: 3, score_diff 2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 497
=== ep: 513, time 278.3017404079437, eps 0.001000000006517878, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 513
=== ep: 514, time 289.92729139328003, eps 0.0010000000061999974, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 514
=== ep: 515, time 247.40951681137085, eps 0.0010000000058976199, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 515
=== ep: 516, time 250.94374251365662, eps 0.0010000000056099897, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 516
=== ep: 517, time 268.2106840610504, eps 0.0010000000053363872, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 21 > 20.0 and we are deleting ep 517
