==> Playing in 11_vs_11_hard_stochastic.
==>Level 1
==>OTs in this level are dict_keys(['win_game'])
==>Currently learning win_game
==>using device cuda
==>critic has 7 layers and 500 hidden units.
=== ep: 0, time 25.198254585266113, eps 0.9, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 1, time 24.890516757965088, eps 0.8561552526261419, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
=== ep: 2, time 24.91037082672119, eps 0.8144488388143276, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
=== ep: 3, time 25.102404356002808, eps 0.774776470806127, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
=== ep: 4, time 25.334455966949463, eps 0.7370389470171057, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
goal_identified
=== ep: 5, time 25.559189796447754, eps 0.701141903981193, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
=== ep: 6, time 25.470806121826172, eps 0.6669955803928644, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
=== ep: 7, time 25.580939292907715, eps 0.6345145926571234, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
=== ep: 8, time 26.043474197387695, eps 0.6036177213860398, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
=== ep: 9, time 40.57265830039978, eps 0.5742277083079742, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 7
goal_identified
goal_identified
=== ep: 10, time 25.744689226150513, eps 0.5462710630816575, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 4
goal_identified
=== ep: 11, time 25.929123401641846, eps 0.5196778795320575, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 8
=== ep: 12, time 25.576367139816284, eps 0.49438166084852986, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 0
=== ep: 13, time 26.205077648162842, eps 0.47031915330815344, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 2
=== ep: 14, time 25.87610411643982, eps 0.4474301881084772, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 9
goal_identified
=== ep: 15, time 26.111711740493774, eps 0.42565753091417224, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 13
=== ep: 16, time 26.220996141433716, eps 0.4049467387413822, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 16
goal_identified
=== ep: 17, time 26.605798482894897, eps 0.3852460238219053, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 14
=== ep: 18, time 25.775136709213257, eps 0.3665061241067986, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 17
=== ep: 19, time 43.95284938812256, eps 0.3486801800855966, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1
goal_identified
=== ep: 20, time 26.77364706993103, eps 0.3317236176131267, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
/home/ksridhar/GRF/scripts/policies_baseline.py:437: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
== current size of memory is eps 11 > 10.0 and we are deleting ep 20
=== ep: 21, time 26.25839591026306, eps 0.31559403645092865, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 21
=== ep: 22, time 26.59399104118347, eps 0.3002511042445735, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 3
=== ep: 23, time 26.187165021896362, eps 0.2856564556717689, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 23
=== ep: 24, time 26.840776920318604, eps 0.27177359650906974, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 24
goal_identified
=== ep: 25, time 25.851861238479614, eps 0.2585678123773109, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 5
=== ep: 26, time 26.824103355407715, eps 0.24600608193757734, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 26
=== ep: 27, time 24.234783411026, eps 0.23405699432065646, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 6
=== ep: 28, time 27.301891088485718, eps 0.22269067058350425, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 28
goal_identified
=== ep: 29, time 55.18602252006531, eps 0.2118786889963241, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 10
goal_identified
=== ep: 30, time 26.26935648918152, eps 0.2015940139734384, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 11
=== ep: 31, time 26.144695043563843, eps 0.191810928470242, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 12
=== ep: 32, time 26.340169191360474, eps 0.1825049696771952, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 15
=== ep: 33, time 26.869372606277466, eps 0.17365286785005798, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 33
goal_identified
goal_identified
=== ep: 34, time 26.507877826690674, eps 0.16523248812340846, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 18
goal_identified
=== ep: 35, time 26.29898452758789, eps 0.15722277516195018, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 19
=== ep: 36, time 26.27709937095642, eps 0.1496037005112063, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 36
=== ep: 37, time 26.59661054611206, eps 0.14235621251595124, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 37
=== ep: 38, time 26.845840454101562, eps 0.13546218868114893, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 38
=== ep: 39, time 51.90335416793823, eps 0.1289043903562757, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 39
goal_identified
=== ep: 40, time 26.807156801223755, eps 0.12266641962971482, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 22
=== ep: 41, time 26.808643579483032, eps 0.116732678325436, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 27
=== ep: 42, time 26.659403085708618, eps 0.11108832899943073, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 42
goal_identified
=== ep: 43, time 27.61930775642395, eps 0.10571925783837377, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 43
=== ep: 44, time 27.0245943069458, eps 0.10061203936773815, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 44
=== ep: 45, time 26.765393257141113, eps 0.09575390288111604, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 31
=== ep: 46, time 27.108357191085815, eps 0.09113270050680057, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 46
=== ep: 47, time 27.12718629837036, eps 0.08673687683177911, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 47
=== ep: 48, time 26.588061094284058, eps 0.08255544000718185, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 32
goal_identified
=== ep: 49, time 50.189260482788086, eps 0.07857793426293408, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 35
goal_identified
goal_identified
=== ep: 50, time 26.694622039794922, eps 0.07479441376288502, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 40
=== ep: 51, time 26.688322067260742, eps 0.0711954177350367, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 41
goal_identified
=== ep: 52, time 26.609344482421875, eps 0.06777194681468615, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 45
=== ep: 53, time 26.95325231552124, eps 0.06451544054132621, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 53
goal_identified
=== ep: 54, time 26.27585482597351, eps 0.06141775595303503, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 48
=== ep: 55, time 26.88129448890686, eps 0.05847114722483011, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 51
goal_identified
=== ep: 56, time 27.28736710548401, eps 0.05566824630007096, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 52
=== ep: 57, time 27.22352623939514, eps 0.05300204446647978, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 57
=== ep: 58, time 26.99036145210266, eps 0.050465874830710106, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 54
=== ep: 59, time 61.73720860481262, eps 0.04805339564764071, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 55
=== ep: 60, time 27.22881269454956, eps 0.045758574462709686, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 60
=== ep: 61, time 28.551854133605957, eps 0.043575673027635695, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 61
=== ep: 62, time 27.467968702316284, eps 0.04149923295180846, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 56
goal_identified
=== ep: 63, time 27.420690059661865, eps 0.03952406205346913, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 58
goal_identified
=== ep: 64, time 26.596148252487183, eps 0.03764522137655123, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 59
=== ep: 65, time 27.529388666152954, eps 0.03585801284071809, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 65
=== ep: 66, time 27.21438980102539, eps 0.034157967493714775, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 66
=== ep: 67, time 27.510287523269653, eps 0.03254083433665968, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 67
=== ep: 68, time 27.22711157798767, eps 0.031002569694333147, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 68
goal_identified
=== ep: 69, time 62.444839000701904, eps 0.02953932710388308, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 62
=== ep: 70, time 27.43020725250244, eps 0.028147447696664333, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 70
goal_identified
=== ep: 71, time 30.697280883789062, eps 0.026823451049161253, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 25
goal_identified
=== ep: 72, time 27.438703536987305, eps 0.025564026480116013, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 72
=== ep: 73, time 27.39337182044983, eps 0.02436602477210106, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 73
=== ep: 74, time 26.838340759277344, eps 0.02322645029683511, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 74
=== ep: 75, time 27.012463569641113, eps 0.02214245352455219, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 75
=== ep: 76, time 27.14215350151062, eps 0.02111132389869288, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 76
=== ep: 77, time 28.031272649765015, eps 0.020130483058101077, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 77
=== ep: 78, time 27.15874195098877, eps 0.019197478389778148, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 78
=== ep: 79, time 62.275105476379395, eps 0.018309976896072843, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 79
goal_identified
=== ep: 80, time 27.401601552963257, eps 0.017465759360972027, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 80
=== ep: 81, time 27.33059048652649, eps 0.01666271480090467, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 81
=== ep: 82, time 27.796143770217896, eps 0.015898835186183367, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 82
=== ep: 83, time 27.025471925735474, eps 0.015172210419884185, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 83
=== ep: 84, time 28.53941512107849, eps 0.014481023561609456, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 84
=== ep: 85, time 28.513288259506226, eps 0.01382354628419033, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 85
=== ep: 86, time 27.277512550354004, eps 0.013198134551968641, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 86
=== ep: 87, time 27.542863130569458, eps 0.012603224509851407, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 87
=== ep: 88, time 26.70774221420288, eps 0.012037328572858524, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 88
=== ep: 89, time 59.16615629196167, eps 0.011499031706385502, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 89
=== ep: 90, time 28.86231780052185, eps 0.010986987887879832, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 90
=== ep: 91, time 28.12514328956604, eps 0.010499916741083536, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 91
=== ep: 92, time 27.29260563850403, eps 0.010036600334425595, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 92
=== ep: 93, time 28.10115957260132, eps 0.00959588013555861, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 93
=== ep: 94, time 36.250629901885986, eps 0.009176654114424539, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 94
=== ep: 95, time 29.405155658721924, eps 0.00877787398760545, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 95
=== ep: 96, time 32.33810091018677, eps 0.008398542597069007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 96
=== ep: 97, time 27.55835199356079, eps 0.008037711416753971, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 97
=== ep: 98, time 27.386447429656982, eps 0.00769447818076098, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 98
=== ep: 99, time 67.70575547218323, eps 0.007367984627217855, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 99
=== ep: 100, time 27.045978307724, eps 0.007057414352177835, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 100
=== ep: 101, time 27.367645263671875, eps 0.006761990768184489, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 101
=== ep: 102, time 27.194820404052734, eps 0.006480975162398559, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 102
=== ep: 103, time 26.98703956604004, eps 0.006213664849431085, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 103
=== ep: 104, time 28.499248266220093, eps 0.005959391414263934, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 104
goal_identified
=== ep: 105, time 27.128834009170532, eps 0.005717519040864065, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 105
=== ep: 106, time 27.46646022796631, eps 0.005487442922312285, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 106
=== ep: 107, time 27.151569366455078, eps 0.005268587748470919, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 107
=== ep: 108, time 27.197322368621826, eps 0.005060406267408787, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 108
=== ep: 109, time 61.55129790306091, eps 0.004862377916986354, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 109
=== ep: 110, time 28.785709142684937, eps 0.004674007523179196, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 110
=== ep: 111, time 32.54976558685303, eps 0.004494824061885041, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 111
=== ep: 112, time 27.761378526687622, eps 0.0043243794811181555, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 112
=== ep: 113, time 27.94261074066162, eps 0.0041622475806460035, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 113
=== ep: 114, time 26.55885934829712, eps 0.0040080229462666735, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 114
=== ep: 115, time 28.092039823532104, eps 0.0038613199360621906, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 115
=== ep: 116, time 27.586413383483887, eps 0.003721771716092858, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 116
=== ep: 117, time 27.668546676635742, eps 0.0035890293431213305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 117
=== ep: 118, time 28.195890426635742, eps 0.0034627608920727634, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 118
=== ep: 119, time 69.6138825416565, eps 0.00334265062604924, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 119
=== ep: 120, time 27.713746786117554, eps 0.0032283982068230565, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 120
=== ep: 121, time 28.5088050365448, eps 0.0031197179438347193, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 121
goal_identified
=== ep: 122, time 28.529651880264282, eps 0.0030163380798177374, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 29
=== ep: 123, time 27.635374546051025, eps 0.0029180001112638996, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 123
=== ep: 124, time 27.71891736984253, eps 0.002824458142029865, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 124
=== ep: 125, time 28.485235691070557, eps 0.0027354782684687108, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 125
=== ep: 126, time 27.550273656845093, eps 0.0026508379945489875, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 126
goal_identified
=== ep: 127, time 31.326326608657837, eps 0.0025703256754987464, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 30
goal_identified
=== ep: 128, time 27.6051766872406, eps 0.0024937399885833667, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 128
=== ep: 129, time 75.24678301811218, eps 0.0024208894296938593, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 129
=== ep: 130, time 25.92565131187439, eps 0.0023515918344868374, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 130
goal_identified
=== ep: 131, time 25.88299870491028, eps 0.002285673922878779, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 131
=== ep: 132, time 29.622726678848267, eps 0.0022229708657555565, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 132
=== ep: 133, time 26.504653692245483, eps 0.0021633258728137976, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 133
=== ep: 134, time 27.35504674911499, eps 0.0021065898005034594, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 134
=== ep: 135, time 25.483762502670288, eps 0.002052620779091266, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 135
=== ep: 136, time 26.93068790435791, eps 0.0020012838579124784, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 136
goal_identified
=== ep: 137, time 26.523478031158447, eps 0.0019524506679239415, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 49
=== ep: 138, time 26.07768702507019, eps 0.001905999100714611, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 138
=== ep: 139, time 76.4973714351654, eps 0.001861813003170924, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 139
=== ep: 140, time 24.95740580558777, eps 0.0018197818870335101, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 140
=== ep: 141, time 27.90621280670166, eps 0.0017798006526189953, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 141
=== ep: 142, time 26.321995973587036, eps 0.0017417693260160481, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 142
=== ep: 143, time 26.5804500579834, eps 0.0017055928090985275, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 143
=== ep: 144, time 26.678966283798218, eps 0.0016711806417306348, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 144
=== ep: 145, time 28.373283863067627, eps 0.0016384467755694515, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 145
goal_identified
=== ep: 146, time 28.93873381614685, eps 0.0016073093588992661, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 50
=== ep: 147, time 27.973204851150513, eps 0.0015776905319596466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 147
=== ep: 148, time 25.964972019195557, eps 0.0015495162322554856, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 148
=== ep: 149, time 88.29792642593384, eps 0.0015227160093621863, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 149
=== ep: 150, time 26.881540060043335, eps 0.0014972228487629025, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 150
=== ep: 151, time 26.456586837768555, eps 0.0014729730042773413, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 151
=== ep: 152, time 37.51400113105774, eps 0.001449905838663109, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 152
=== ep: 153, time 26.492220401763916, eps 0.00142796367199102, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 153
goal_identified
=== ep: 154, time 26.374741554260254, eps 0.0014070916374152305, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 154
=== ep: 155, time 26.762311697006226, eps 0.001387237543977543, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 155
=== ep: 156, time 26.336792945861816, eps 0.0013683517461028282, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 156
=== ep: 157, time 26.494912147521973, eps 0.0013503870194592265, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 157
=== ep: 158, time 26.090710878372192, eps 0.0013332984428727204, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 158
=== ep: 159, time 84.0696804523468, eps 0.001317043286000802, sum reward: 0, score_diff -3, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 159
=== ep: 160, time 27.568246603012085, eps 0.0013015809024843582, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 160
=== ep: 161, time 27.148292779922485, eps 0.0012868726283106018, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 161
=== ep: 162, time 30.73248529434204, eps 0.0012728816851329014, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 162
=== ep: 163, time 26.889578580856323, eps 0.0012595730883057546, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 163
=== ep: 164, time 28.07160472869873, eps 0.001246913559404956, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 164
=== ep: 165, time 26.82363724708557, eps 0.0012348714430141991, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 165
=== ep: 166, time 28.593180179595947, eps 0.0012234166275700486, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 166
=== ep: 167, time 31.092655181884766, eps 0.001212520470067348, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 167
=== ep: 168, time 27.154552936553955, eps 0.0012021557244367845, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 168
goal_identified
=== ep: 169, time 88.0470643043518, eps 0.0011922964734155277, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 169
=== ep: 170, time 26.825115203857422, eps 0.001182918063740569, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 170
=== ep: 171, time 31.371955156326294, eps 0.0011739970445027263, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 171
=== ep: 172, time 26.79807209968567, eps 0.0011655111085071537, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 172
goal_identified
=== ep: 173, time 28.303115129470825, eps 0.001157439036493735, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 173
=== ep: 174, time 26.47330641746521, eps 0.0011497606440778825, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 174
=== ep: 175, time 26.800153732299805, eps 0.0011424567312790603, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 175
=== ep: 176, time 28.792877197265625, eps 0.0011355090345108335, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 176
=== ep: 177, time 34.86170530319214, eps 0.0011289001809123877, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 177
=== ep: 178, time 27.57585906982422, eps 0.0011226136449073282, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 178
=== ep: 179, time 84.02837872505188, eps 0.001116633706881133, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 179
=== ep: 180, time 26.836101293563843, eps 0.001110945413873925, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 180
=== ep: 181, time 26.570916414260864, eps 0.001105534542190287, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 181
=== ep: 182, time 27.298983573913574, eps 0.0011003875618326132, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 182
=== ep: 183, time 27.17930245399475, eps 0.0010954916026690664, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 183
=== ep: 184, time 26.208094120025635, eps 0.001090834422251547, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 184
=== ep: 185, time 27.680796146392822, eps 0.0010864043752031938, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 185
=== ep: 186, time 32.352484703063965, eps 0.0010821903840988777, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 186
=== ep: 187, time 24.61404061317444, eps 0.0010781819117658682, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 187
=== ep: 188, time 27.707202672958374, eps 0.0010743689349354123, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 188
=== ep: 189, time 83.31405472755432, eps 0.0010707419191793434, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 189
=== ep: 190, time 31.09334111213684, eps 0.0010672917950690429, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 190
goal_identified
=== ep: 191, time 26.277188062667847, eps 0.0010640099354971456, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 191
goal_identified
=== ep: 192, time 27.647321701049805, eps 0.0010608881341052777, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 63
=== ep: 193, time 26.6419038772583, eps 0.0010579185847638855, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 193
=== ep: 194, time 30.279560804367065, eps 0.0010550938620528466, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 194
=== ep: 195, time 27.864722967147827, eps 0.001052406902694051, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 195
=== ep: 196, time 27.074164390563965, eps 0.001049850987889527, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 196
=== ep: 197, time 26.408303260803223, eps 0.0010474197265209469, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 197
=== ep: 198, time 26.917810916900635, eps 0.0010451070391685015, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 198
goal_identified
=== ep: 199, time 86.05981492996216, eps 0.001042907142909185, sum reward: 1, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 199
=== ep: 200, time 26.883486032485962, eps 0.001040814536856474, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 200
=== ep: 201, time 26.166733741760254, eps 0.0010388239884052469, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 201
=== ep: 202, time 26.984465837478638, eps 0.0010369305201475454, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 202
=== ep: 203, time 26.550742387771606, eps 0.0010351293974264616, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 203
=== ep: 204, time 26.411489486694336, eps 0.00103341611649703, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 204
=== ep: 205, time 26.214141845703125, eps 0.0010317863932645186, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 205
=== ep: 206, time 31.987623691558838, eps 0.0010302361525719613, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 206
=== ep: 207, time 25.435270309448242, eps 0.0010287615180101426, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 207
=== ep: 208, time 27.422510385513306, eps 0.001027358802224555, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 208
=== ep: 209, time 86.35899543762207, eps 0.0010260244976950921, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 209
=== ep: 210, time 27.97275996208191, eps 0.0010247552679654227, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 210
=== ep: 211, time 26.858239889144897, eps 0.00102354793930011, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 211
=== ep: 212, time 27.637832641601562, eps 0.0010223994927486214, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 212
=== ep: 213, time 33.65341544151306, eps 0.001021307056596379, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 213
=== ep: 214, time 26.24630045890808, eps 0.0010202678991839778, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 214
goal_identified
=== ep: 215, time 27.191028833389282, eps 0.0010192794220766138, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 215
=== ep: 216, time 26.391178607940674, eps 0.0010183391535666436, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 216
goal_identified
goal_identified
=== ep: 217, time 26.813475847244263, eps 0.0010174447424930286, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 64
=== ep: 218, time 27.792036294937134, eps 0.0010165939523622068, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 218
=== ep: 219, time 97.98373413085938, eps 0.0010157846557556941, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 219
=== ep: 220, time 27.823608875274658, eps 0.001015014829010431, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 220
=== ep: 221, time 26.46213436126709, eps 0.0010142825471585687, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 221
=== ep: 222, time 30.558333158493042, eps 0.0010135859791140496, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 222
=== ep: 223, time 29.45820164680481, eps 0.0010129233830939361, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 223
=== ep: 224, time 26.101006984710693, eps 0.0010122931022630473, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 224
=== ep: 225, time 30.547835111618042, eps 0.001011693560591007, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 225
=== ep: 226, time 26.91735291481018, eps 0.0010111232589113477, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 226
=== ep: 227, time 31.3247971534729, eps 0.0010105807711728136, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 227
=== ep: 228, time 26.89654517173767, eps 0.0010100647408734893, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 228
=== ep: 229, time 103.05258655548096, eps 0.001009573877668838, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 229
=== ep: 230, time 28.324405670166016, eps 0.001009106954145169, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 230
=== ep: 231, time 26.808509588241577, eps 0.0010086628027504636, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 231
=== ep: 232, time 26.525417804718018, eps 0.0010082403128748867, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 232
=== ep: 233, time 26.26112127304077, eps 0.0010078384280736842, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 233
=== ep: 234, time 27.48321533203125, eps 0.001007456143425521, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 234
=== ep: 235, time 26.71823287010193, eps 0.001007092503019653, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 235
=== ep: 236, time 27.079785346984863, eps 0.001006746597565654, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 236
=== ep: 237, time 26.941983461380005, eps 0.001006417562119715, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 237
goal_identified
=== ep: 238, time 27.310314416885376, eps 0.0010061045739218342, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 69
=== ep: 239, time 120.45933103561401, eps 0.0010058068503384884, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 239
=== ep: 240, time 27.394749641418457, eps 0.001005523646905642, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 240
=== ep: 241, time 26.963659048080444, eps 0.001005254255467199, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 241
=== ep: 242, time 36.997891664505005, eps 0.0010049980024042435, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 242
=== ep: 243, time 30.387757778167725, eps 0.0010047542469506416, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 243
=== ep: 244, time 26.941460847854614, eps 0.0010045223795907931, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 244
=== ep: 245, time 26.947114944458008, eps 0.001004301820535524, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 245
goal_identified
=== ep: 246, time 26.10266089439392, eps 0.0010040920182723119, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 71
=== ep: 247, time 27.414408445358276, eps 0.0010038924481862177, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 247
=== ep: 248, time 27.131006240844727, eps 0.0010037026112480747, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 248
goal_identified
=== ep: 249, time 100.67894983291626, eps 0.0010035220327666559, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 249
=== ep: 250, time 26.787024974822998, eps 0.0010033502612016988, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 250
=== ep: 251, time 26.949910879135132, eps 0.001003186867034819, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 251
=== ep: 252, time 26.59267807006836, eps 0.001003031441695491, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 252
goal_identified
=== ep: 253, time 26.67678189277649, eps 0.0010028835965394094, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 122
=== ep: 254, time 26.76189374923706, eps 0.0010027429618766747, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 254
goal_identified
=== ep: 255, time 27.76079535484314, eps 0.0010026091860473767, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 127
=== ep: 256, time 27.144379377365112, eps 0.0010024819345422614, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 256
=== ep: 257, time 27.782777786254883, eps 0.0010023608891662839, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 257
=== ep: 258, time 26.664725303649902, eps 0.001002245747242954, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 258
=== ep: 259, time 99.11683297157288, eps 0.0010021362208574892, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 259
=== ep: 260, time 27.380524396896362, eps 0.001002032036136876, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 260
=== ep: 261, time 26.79330539703369, eps 0.0010019329325650452, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 261
goal_identified
=== ep: 262, time 26.47652268409729, eps 0.0010018386623314465, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 262
=== ep: 263, time 28.203274488449097, eps 0.0010017489897113931, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 263
=== ep: 264, time 27.223376274108887, eps 0.0010016636904766263, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 264
=== ep: 265, time 27.042548656463623, eps 0.0010015825513346283, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 265
=== ep: 266, time 27.592785358428955, eps 0.0010015053693952815, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 266
=== ep: 267, time 27.173798084259033, eps 0.0010014319516635345, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 267
=== ep: 268, time 25.34138536453247, eps 0.0010013621145568167, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 268
=== ep: 269, time 97.57285356521606, eps 0.0010012956834459848, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 269
=== ep: 270, time 31.348782539367676, eps 0.0010012324922186594, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 270
=== ep: 271, time 28.9058256149292, eps 0.001001172382863857, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 271
=== ep: 272, time 27.486982822418213, eps 0.0010011152050768812, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 272
=== ep: 273, time 26.813701152801514, eps 0.0010010608158834819, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 273
=== ep: 274, time 26.64667248725891, eps 0.0010010090792823456, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 274
=== ep: 275, time 27.63119387626648, eps 0.0010009598659050213, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 275
=== ep: 276, time 26.42127752304077, eps 0.0010009130526924313, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 276
=== ep: 277, time 27.219045639038086, eps 0.0010008685225871602, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 277
=== ep: 278, time 28.877435445785522, eps 0.0010008261642407504, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 278
=== ep: 279, time 108.87718462944031, eps 0.001000785871735272, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 279
=== ep: 280, time 26.298287868499756, eps 0.0010007475443184742, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 280
=== ep: 281, time 29.75205636024475, eps 0.001000711086151851, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 281
=== ep: 282, time 29.674928188323975, eps 0.0010006764060709957, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 282
=== ep: 283, time 25.838709592819214, eps 0.001000643417357642, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 283
=== ep: 284, time 28.840916395187378, eps 0.0010006120375228235, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 284
goal_identified
=== ep: 285, time 27.760070085525513, eps 0.0010005821881006083, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 285
=== ep: 286, time 32.17682862281799, eps 0.0010005537944518927, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 286
goal_identified
=== ep: 287, time 26.553627014160156, eps 0.0010005267855777657, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 137
=== ep: 288, time 28.313151597976685, eps 0.0010005010939419733, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 288
=== ep: 289, time 105.79850912094116, eps 0.001000476655302044, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 289
=== ep: 290, time 28.243098735809326, eps 0.0010004534085486486, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 290
=== ep: 291, time 32.75368285179138, eps 0.0010004312955527947, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 291
=== ep: 292, time 30.064565181732178, eps 0.0010004102610204745, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 292
=== ep: 293, time 26.876270532608032, eps 0.0010003902523544011, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 293
=== ep: 294, time 27.72561287879944, eps 0.0010003712195224871, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 294
=== ep: 295, time 27.108006954193115, eps 0.0010003531149327387, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 295
=== ep: 296, time 28.375385284423828, eps 0.0010003358933142518, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 296
=== ep: 297, time 26.880158185958862, eps 0.0010003195116040093, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 297
=== ep: 298, time 27.062264680862427, eps 0.0010003039288392032, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 298
=== ep: 299, time 104.84011888504028, eps 0.0010002891060548044, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 299
=== ep: 300, time 26.819032669067383, eps 0.0010002750061861312, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 300
=== ep: 301, time 33.4860200881958, eps 0.0010002615939761676, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 301
=== ep: 302, time 28.074655294418335, eps 0.001000248835887403, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 302
=== ep: 303, time 28.46857786178589, eps 0.0010002367000179694, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 303
=== ep: 304, time 26.3670392036438, eps 0.0010002251560218723, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 304
=== ep: 305, time 26.448638200759888, eps 0.0010002141750331084, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 305
=== ep: 306, time 26.77902317047119, eps 0.0010002037295934862, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 306
=== ep: 307, time 35.80738353729248, eps 0.0010001937935839656, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 307
=== ep: 308, time 29.360547065734863, eps 0.0010001843421593476, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 308
=== ep: 309, time 111.7664430141449, eps 0.0010001753516861473, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 309
=== ep: 310, time 33.03450679779053, eps 0.0010001667996834991, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 310
goal_identified
=== ep: 311, time 27.473950147628784, eps 0.001000158664766942, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 146
=== ep: 312, time 26.34264063835144, eps 0.0010001509265949466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 312
=== ep: 313, time 28.95893359184265, eps 0.001000143565818053, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 313
=== ep: 314, time 27.027105569839478, eps 0.0010001365640304844, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 314
=== ep: 315, time 26.490175247192383, eps 0.0010001299037241253, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 315
=== ep: 316, time 36.47062397003174, eps 0.0010001235682447402, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 316
=== ep: 317, time 27.260148525238037, eps 0.0010001175417503308, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 317
=== ep: 318, time 26.77554988861084, eps 0.0010001118091715218, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 318
=== ep: 319, time 96.50340342521667, eps 0.0010001063561738807, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 319
=== ep: 320, time 31.77807378768921, eps 0.0010001011691220727, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 320
=== ep: 321, time 29.529115200042725, eps 0.0010000962350457665, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 321
=== ep: 322, time 29.576569080352783, eps 0.0010000915416072012, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 322
=== ep: 323, time 31.499972820281982, eps 0.0010000870770703358, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 323
=== ep: 324, time 36.376845598220825, eps 0.0010000828302715028, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 324
=== ep: 325, time 29.007732629776, eps 0.0010000787905914928, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 325
=== ep: 326, time 27.424253463745117, eps 0.0010000749479290019, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 326
=== ep: 327, time 31.975366830825806, eps 0.001000071292675372, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 327
=== ep: 328, time 29.51797866821289, eps 0.001000067815690565, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 328
=== ep: 329, time 112.36040759086609, eps 0.0010000645082803084, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 329
=== ep: 330, time 27.842605113983154, eps 0.0010000613621743532, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 330
=== ep: 331, time 28.039756536483765, eps 0.0010000583695057963, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 331
=== ep: 332, time 26.941073179244995, eps 0.0010000555227914069, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 332
=== ep: 333, time 31.776862144470215, eps 0.0010000528149129166, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 333
goal_identified
=== ep: 334, time 31.62891459465027, eps 0.0010000502390992187, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 192
=== ep: 335, time 26.62792420387268, eps 0.0010000477889094373, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 335
=== ep: 336, time 29.136228561401367, eps 0.0010000454582168217, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 336
=== ep: 337, time 30.443507194519043, eps 0.001000043241193426, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 337
=== ep: 338, time 29.454763412475586, eps 0.0010000411322955373, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 338
=== ep: 339, time 99.75868320465088, eps 0.0010000391262498123, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 339
=== ep: 340, time 26.934206247329712, eps 0.001000037218040092, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 340
=== ep: 341, time 27.05101442337036, eps 0.0010000354028948577, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 341
=== ep: 342, time 31.608612775802612, eps 0.0010000336762753012, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 342
goal_identified
=== ep: 343, time 26.83277940750122, eps 0.001000032033863974, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 217
=== ep: 344, time 29.502187967300415, eps 0.0010000304715539925, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 344
=== ep: 345, time 28.18790102005005, eps 0.001000028985438768, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 345
=== ep: 346, time 29.103536367416382, eps 0.001000027571802238, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 346
=== ep: 347, time 27.25164222717285, eps 0.0010000262271095755, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 347
=== ep: 348, time 28.359513759613037, eps 0.0010000249479983478, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 348
=== ep: 349, time 92.99002647399902, eps 0.0010000237312701107, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 349
=== ep: 350, time 30.218179941177368, eps 0.00100002257388241, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 350
=== ep: 351, time 28.15623950958252, eps 0.0010000214729411737, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 351
=== ep: 352, time 31.467256784439087, eps 0.0010000204256934752, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 352
=== ep: 353, time 27.42494559288025, eps 0.0010000194295206493, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 353
goal_identified
=== ep: 354, time 30.96690273284912, eps 0.0010000184819317455, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 238
=== ep: 355, time 28.14807891845703, eps 0.001000017580557298, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 355
=== ep: 356, time 27.115232944488525, eps 0.001000016723143401, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 356
=== ep: 357, time 27.725088119506836, eps 0.0010000159075460732, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 357
=== ep: 358, time 31.669527769088745, eps 0.0010000151317258964, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 358
=== ep: 359, time 90.99777960777283, eps 0.0010000143937429161, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 359
=== ep: 360, time 28.47506594657898, eps 0.0010000136917517905, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 360
=== ep: 361, time 26.565709590911865, eps 0.001000013023997176, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 361
=== ep: 362, time 29.367512941360474, eps 0.0010000123888093385, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 362
=== ep: 363, time 27.75144386291504, eps 0.0010000117845999773, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 363
=== ep: 364, time 28.444271326065063, eps 0.0010000112098582543, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 364
=== ep: 365, time 26.742761373519897, eps 0.001000010663147016, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 365
=== ep: 366, time 27.42675805091858, eps 0.0010000101430991996, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 366
=== ep: 367, time 27.894927978515625, eps 0.0010000096484144142, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 367
=== ep: 368, time 29.23539447784424, eps 0.0010000091778556905, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 368
=== ep: 369, time 93.83650660514832, eps 0.0010000087302463867, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 369
goal_identified
=== ep: 370, time 26.82552981376648, eps 0.001000008304467246, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 246
=== ep: 371, time 27.51763916015625, eps 0.0010000078994535993, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 371
=== ep: 372, time 27.363295316696167, eps 0.0010000075141927012, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 372
=== ep: 373, time 28.00738024711609, eps 0.0010000071477211988, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 373
=== ep: 374, time 27.39482283592224, eps 0.0010000067991227223, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 374
=== ep: 375, time 32.503533124923706, eps 0.0010000064675255943, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 375
=== ep: 376, time 33.53918695449829, eps 0.001000006152100649, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 376
=== ep: 377, time 27.905978441238403, eps 0.0010000058520591598, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 377
=== ep: 378, time 27.101417779922485, eps 0.0010000055666508666, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 378
=== ep: 379, time 90.88503170013428, eps 0.0010000052951621003, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 379
=== ep: 380, time 34.99913501739502, eps 0.0010000050369139975, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 380
=== ep: 381, time 27.885595083236694, eps 0.001000004791260803, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 381
=== ep: 382, time 28.248054265975952, eps 0.0010000045575882562, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 382
=== ep: 383, time 27.24801731109619, eps 0.001000004335312054, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 383
=== ep: 384, time 27.583477020263672, eps 0.0010000041238763903, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 384
=== ep: 385, time 30.39481019973755, eps 0.0010000039227525655, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 385
=== ep: 386, time 28.00390386581421, eps 0.0010000037314376652, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 386
=== ep: 387, time 27.665590047836304, eps 0.001000003549453303, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 387
=== ep: 388, time 27.569659948349, eps 0.0010000033763444226, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 388
=== ep: 389, time 104.99576687812805, eps 0.001000003211678162, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 389
=== ep: 390, time 27.27560067176819, eps 0.0010000030550427698, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 390
=== ep: 391, time 28.000539302825928, eps 0.0010000029060465757, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 391
=== ep: 392, time 27.603403329849243, eps 0.0010000027643170119, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 392
=== ep: 393, time 34.16283202171326, eps 0.0010000026294996803, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 393
=== ep: 394, time 27.256374835968018, eps 0.0010000025012574677, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 394
goal_identified
=== ep: 395, time 26.243545293807983, eps 0.0010000023792697014, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 253
=== ep: 396, time 28.811520099639893, eps 0.0010000022632313489, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 396
=== ep: 397, time 27.558314561843872, eps 0.0010000021528522535, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 397
=== ep: 398, time 29.376132011413574, eps 0.00100000204785641, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 398
goal_identified
=== ep: 399, time 96.63134479522705, eps 0.0010000019479812744, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 255
goal_identified
=== ep: 400, time 31.07235026359558, eps 0.0010000018529771066, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 287
=== ep: 401, time 32.0337073802948, eps 0.0010000017626063467, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 401
=== ep: 402, time 28.447099447250366, eps 0.0010000016766430208, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 402
=== ep: 403, time 26.67703413963318, eps 0.0010000015948721758, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 403
=== ep: 404, time 32.772563457489014, eps 0.001000001517089342, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 404
=== ep: 405, time 28.589094877243042, eps 0.0010000014431000217, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 405
=== ep: 406, time 27.02815818786621, eps 0.001000001372719203, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 406
=== ep: 407, time 26.754987001419067, eps 0.0010000013057708975, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 407
=== ep: 408, time 26.903749465942383, eps 0.0010000012420876994, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 408
=== ep: 409, time 100.41772294044495, eps 0.0010000011815103674, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 409
=== ep: 410, time 26.995110034942627, eps 0.001000001123887427, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 410
=== ep: 411, time 30.428262948989868, eps 0.0010000010690747903, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 411
=== ep: 412, time 31.87757968902588, eps 0.0010000010169353975, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 412
=== ep: 413, time 34.607789516448975, eps 0.0010000009673388729, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 413
=== ep: 414, time 39.36070537567139, eps 0.0010000009201611994, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 414
=== ep: 415, time 33.4608998298645, eps 0.0010000008752844081, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 415
=== ep: 416, time 27.11897039413452, eps 0.0010000008325962838, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 416
=== ep: 417, time 30.56134033203125, eps 0.001000000791990084, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 417
=== ep: 418, time 28.668312549591064, eps 0.0010000007533642718, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 418
=== ep: 419, time 101.36165738105774, eps 0.0010000007166222626, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 419
=== ep: 420, time 27.699503183364868, eps 0.0010000006816721825, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 420
=== ep: 421, time 27.4011173248291, eps 0.001000000648426638, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 421
=== ep: 422, time 26.852607488632202, eps 0.0010000006168024976, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 422
=== ep: 423, time 27.696593046188354, eps 0.0010000005867206849, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 423
=== ep: 424, time 29.618131637573242, eps 0.0010000005581059794, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 424
=== ep: 425, time 27.91088318824768, eps 0.0010000005308868295, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 425
=== ep: 426, time 29.157469987869263, eps 0.0010000005049951733, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 426
=== ep: 427, time 32.82991671562195, eps 0.001000000480366268, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 427
=== ep: 428, time 26.618282318115234, eps 0.0010000004569385287, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 428
=== ep: 429, time 105.12648749351501, eps 0.0010000004346533736, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 429
=== ep: 430, time 27.68807578086853, eps 0.0010000004134550786, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 430
goal_identified
=== ep: 431, time 26.87179660797119, eps 0.0010000003932906364, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 431
=== ep: 432, time 27.05689787864685, eps 0.0010000003741096257, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 432
=== ep: 433, time 28.30025291442871, eps 0.001000000355864084, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 433
=== ep: 434, time 27.123146057128906, eps 0.0010000003385083878, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 434
=== ep: 435, time 26.799851894378662, eps 0.001000000321999139, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 435
=== ep: 436, time 29.318949460983276, eps 0.0010000003062950555, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 436
=== ep: 437, time 27.14544415473938, eps 0.0010000002913568694, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 437
=== ep: 438, time 26.349544763565063, eps 0.0010000002771472273, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 438
=== ep: 439, time 105.53757667541504, eps 0.0010000002636305976, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 439
=== ep: 440, time 27.497010946273804, eps 0.0010000002507731815, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 440
=== ep: 441, time 28.77220845222473, eps 0.0010000002385428292, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 441
=== ep: 442, time 27.803309679031372, eps 0.0010000002269089582, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 442
=== ep: 443, time 24.500702142715454, eps 0.0010000002158424776, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 443
=== ep: 444, time 29.193459272384644, eps 0.0010000002053157158, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 444
=== ep: 445, time 27.867368936538696, eps 0.0010000001953023503, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 445
=== ep: 446, time 27.35366916656494, eps 0.001000000185777342, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 446
=== ep: 447, time 24.63260579109192, eps 0.0010000001767168742, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 447
=== ep: 448, time 26.98732089996338, eps 0.0010000001680982905, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 448
=== ep: 449, time 97.31019425392151, eps 0.0010000001599000403, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 449
goal_identified
=== ep: 450, time 27.315674304962158, eps 0.0010000001521016232, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 450
=== ep: 451, time 27.163872003555298, eps 0.0010000001446835395, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 451
=== ep: 452, time 29.2453932762146, eps 0.0010000001376272401, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 452
=== ep: 453, time 27.56144118309021, eps 0.0010000001309150804, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 453
=== ep: 454, time 29.637176275253296, eps 0.0010000001245302765, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 454
=== ep: 455, time 26.894275903701782, eps 0.0010000001184568633, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 455
goal_identified
=== ep: 456, time 29.33899998664856, eps 0.0010000001126796538, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 456
=== ep: 457, time 27.73090171813965, eps 0.0010000001071842023, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 457
=== ep: 458, time 27.67215657234192, eps 0.001000000101956767, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 458
=== ep: 459, time 98.41960191726685, eps 0.001000000096984277, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 459
goal_identified
=== ep: 460, time 27.891939878463745, eps 0.001000000092254298, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 311
=== ep: 461, time 27.495567083358765, eps 0.0010000000877550027, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 461
=== ep: 462, time 33.84016966819763, eps 0.0010000000834751407, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 462
=== ep: 463, time 27.869662046432495, eps 0.00100000007940401, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 463
=== ep: 464, time 29.819031476974487, eps 0.0010000000755314307, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 464
=== ep: 465, time 27.465339183807373, eps 0.0010000000718477194, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 465
=== ep: 466, time 27.50994110107422, eps 0.0010000000683436647, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 466
=== ep: 467, time 30.350815296173096, eps 0.001000000065010505, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 467
goal_identified
=== ep: 468, time 27.72670865058899, eps 0.0010000000618399052, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 468
=== ep: 469, time 98.18209314346313, eps 0.0010000000588239375, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 469
=== ep: 470, time 27.81867241859436, eps 0.0010000000559550603, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 470
=== ep: 471, time 28.26350235939026, eps 0.0010000000532260998, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 471
=== ep: 472, time 30.934990644454956, eps 0.0010000000506302322, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 472
goal_identified
goal_identified
=== ep: 473, time 28.01826810836792, eps 0.0010000000481609666, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 334
=== ep: 474, time 36.18404006958008, eps 0.0010000000458121286, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 474
=== ep: 475, time 28.401262998580933, eps 0.0010000000435778447, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 475
=== ep: 476, time 27.57810664176941, eps 0.001000000041452528, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 476
=== ep: 477, time 30.994958877563477, eps 0.0010000000394308644, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 477
=== ep: 478, time 33.220595359802246, eps 0.0010000000375077985, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 478
=== ep: 479, time 91.4915099143982, eps 0.0010000000356785216, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 479
=== ep: 480, time 28.028209924697876, eps 0.0010000000339384595, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 480
=== ep: 481, time 27.12978196144104, eps 0.0010000000322832614, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 481
=== ep: 482, time 27.84590458869934, eps 0.0010000000307087882, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 482
=== ep: 483, time 27.860474109649658, eps 0.001000000029211103, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 483
=== ep: 484, time 28.10497736930847, eps 0.0010000000277864607, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 484
goal_identified
=== ep: 485, time 27.14519190788269, eps 0.0010000000264312988, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 485
=== ep: 486, time 27.53076696395874, eps 0.0010000000251422292, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 486
=== ep: 487, time 29.9731662273407, eps 0.0010000000239160282, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 487
goal_identified
goal_identified
=== ep: 488, time 27.47580361366272, eps 0.00100000002274963, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 343
=== ep: 489, time 96.21927762031555, eps 0.0010000000216401172, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 489
=== ep: 490, time 28.930230379104614, eps 0.0010000000205847162, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 490
=== ep: 491, time 28.184955596923828, eps 0.0010000000195807877, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 491
=== ep: 492, time 28.518565893173218, eps 0.0010000000186258216, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 492
=== ep: 493, time 29.622373819351196, eps 0.0010000000177174295, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 493
=== ep: 494, time 27.935876846313477, eps 0.0010000000168533404, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 494
=== ep: 495, time 28.470494031906128, eps 0.0010000000160313932, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 495
=== ep: 496, time 29.563685655593872, eps 0.001000000015249533, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 496
=== ep: 497, time 27.301023244857788, eps 0.0010000000145058043, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 497
=== ep: 498, time 35.95041799545288, eps 0.001000000013798348, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 498
=== ep: 499, time 98.72041964530945, eps 0.0010000000131253947, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 499
=== ep: 500, time 30.138968229293823, eps 0.0010000000124852615, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 500
=== ep: 501, time 30.065194845199585, eps 0.0010000000118763482, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 501
=== ep: 502, time 26.91126537322998, eps 0.0010000000112971319, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 502
=== ep: 503, time 33.416770219802856, eps 0.0010000000107461642, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 503
=== ep: 504, time 30.55205464363098, eps 0.0010000000102220676, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 504
=== ep: 505, time 30.868908166885376, eps 0.0010000000097235315, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 505
=== ep: 506, time 27.916899919509888, eps 0.0010000000092493092, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 506
=== ep: 507, time 27.394381284713745, eps 0.0010000000087982152, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 507
=== ep: 508, time 30.396880626678467, eps 0.0010000000083691212, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 508
=== ep: 509, time 103.33028316497803, eps 0.0010000000079609542, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 509
=== ep: 510, time 35.59210705757141, eps 0.001000000007572694, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 510
=== ep: 511, time 29.823232412338257, eps 0.0010000000072033692, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 511
=== ep: 512, time 31.05295753479004, eps 0.001000000006852057, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 512
=== ep: 513, time 39.188751459121704, eps 0.001000000006517878, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 513
=== ep: 514, time 28.062262296676636, eps 0.0010000000061999974, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 514
=== ep: 515, time 30.72911834716797, eps 0.0010000000058976199, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 515
=== ep: 516, time 28.757814645767212, eps 0.0010000000056099897, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 516
=== ep: 517, time 29.916280031204224, eps 0.0010000000053363872, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 517
=== ep: 518, time 30.63939118385315, eps 0.0010000000050761286, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 518
=== ep: 519, time 117.25552225112915, eps 0.001000000004828563, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 519
=== ep: 520, time 30.88758420944214, eps 0.001000000004593071, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 520
=== ep: 521, time 28.61729335784912, eps 0.0010000000043690644, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 521
=== ep: 522, time 28.9136483669281, eps 0.0010000000041559827, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 522
=== ep: 523, time 30.181365728378296, eps 0.0010000000039532928, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 523
=== ep: 524, time 33.14981031417847, eps 0.0010000000037604885, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 524
=== ep: 525, time 28.84819769859314, eps 0.0010000000035770874, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 525
=== ep: 526, time 28.91658878326416, eps 0.0010000000034026306, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 526
=== ep: 527, time 31.155871629714966, eps 0.0010000000032366824, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 527
goal_identified
=== ep: 528, time 27.452015161514282, eps 0.0010000000030788276, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 528
goal_identified
=== ep: 529, time 120.62631249427795, eps 0.0010000000029286714, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 354
=== ep: 530, time 27.71554470062256, eps 0.0010000000027858384, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 530
=== ep: 531, time 31.34526801109314, eps 0.0010000000026499714, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 531
=== ep: 532, time 32.20052194595337, eps 0.0010000000025207308, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 532
goal_identified
=== ep: 533, time 28.745924711227417, eps 0.0010000000023977934, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 533
=== ep: 534, time 32.035255908966064, eps 0.0010000000022808515, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 534
=== ep: 535, time 31.544284343719482, eps 0.0010000000021696133, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 535
=== ep: 536, time 30.579533338546753, eps 0.0010000000020637999, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 536
=== ep: 537, time 28.557562828063965, eps 0.0010000000019631471, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 537
goal_identified
goal_identified
=== ep: 538, time 27.767268419265747, eps 0.0010000000018674034, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 370
=== ep: 539, time 109.00898480415344, eps 0.001000000001776329, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 539
=== ep: 540, time 29.178184986114502, eps 0.0010000000016896964, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 540
=== ep: 541, time 31.758128881454468, eps 0.001000000001607289, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 541
=== ep: 542, time 27.86486315727234, eps 0.0010000000015289005, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 542
=== ep: 543, time 30.024260759353638, eps 0.0010000000014543352, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 543
=== ep: 544, time 27.806458234786987, eps 0.0010000000013834064, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 544
=== ep: 545, time 28.30791211128235, eps 0.001000000001315937, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 545
=== ep: 546, time 28.69901132583618, eps 0.0010000000012517578, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 546
=== ep: 547, time 31.181947946548462, eps 0.001000000001190709, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 547
=== ep: 548, time 28.063665628433228, eps 0.0010000000011326374, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 548
=== ep: 549, time 119.07430791854858, eps 0.001000000001077398, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 549
=== ep: 550, time 27.623612880706787, eps 0.0010000000010248527, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 550
=== ep: 551, time 28.10338044166565, eps 0.00100000000097487, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 551
=== ep: 552, time 28.938610315322876, eps 0.001000000000927325, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 552
=== ep: 553, time 29.718302488327026, eps 0.0010000000008820989, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 553
=== ep: 554, time 27.573273181915283, eps 0.0010000000008390784, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 554
=== ep: 555, time 33.3307147026062, eps 0.001000000000798156, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 555
=== ep: 556, time 31.36921715736389, eps 0.0010000000007592295, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 556
=== ep: 557, time 27.334758520126343, eps 0.0010000000007222014, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 557
=== ep: 558, time 36.60744857788086, eps 0.0010000000006869794, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 558
=== ep: 559, time 116.41278338432312, eps 0.001000000000653475, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 559
=== ep: 560, time 27.233092069625854, eps 0.0010000000006216046, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 560
=== ep: 561, time 27.98674702644348, eps 0.0010000000005912885, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 561
=== ep: 562, time 27.61485528945923, eps 0.0010000000005624511, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 562
=== ep: 563, time 30.492180109024048, eps 0.00100000000053502, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 563
=== ep: 564, time 36.158780336380005, eps 0.001000000000508927, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 564
=== ep: 565, time 32.59200668334961, eps 0.001000000000484106, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 565
=== ep: 566, time 28.03702998161316, eps 0.001000000000460496, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 566
=== ep: 567, time 35.70742750167847, eps 0.0010000000004380374, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 567
=== ep: 568, time 28.735679388046265, eps 0.001000000000416674, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 568
=== ep: 569, time 95.3088800907135, eps 0.0010000000003963527, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 569
=== ep: 570, time 27.953377962112427, eps 0.0010000000003770222, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 570
=== ep: 571, time 27.67422127723694, eps 0.0010000000003586346, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 571
goal_identified
=== ep: 572, time 28.220611572265625, eps 0.0010000000003411438, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 572
=== ep: 573, time 26.696283102035522, eps 0.001000000000324506, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 573
=== ep: 574, time 27.766497373580933, eps 0.0010000000003086798, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 574
=== ep: 575, time 27.357640743255615, eps 0.0010000000002936252, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 575
=== ep: 576, time 31.22204065322876, eps 0.001000000000279305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 576
=== ep: 577, time 27.882974863052368, eps 0.0010000000002656831, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 577
goal_identified
=== ep: 578, time 27.992493629455566, eps 0.0010000000002527256, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 578
=== ep: 579, time 102.83209252357483, eps 0.0010000000002404, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 579
=== ep: 580, time 27.564175605773926, eps 0.0010000000002286756, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 580
=== ep: 581, time 31.365159034729004, eps 0.0010000000002175229, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 581
=== ep: 582, time 27.804599046707153, eps 0.0010000000002069142, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 582
=== ep: 583, time 32.47768425941467, eps 0.0010000000001968228, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 583
=== ep: 584, time 27.099639415740967, eps 0.0010000000001872237, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 584
=== ep: 585, time 27.71804642677307, eps 0.0010000000001780928, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 585
goal_identified
=== ep: 586, time 27.41693091392517, eps 0.001000000000169407, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 586
=== ep: 587, time 27.657299280166626, eps 0.001000000000161145, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 587
goal_identified
goal_identified
=== ep: 588, time 28.22297167778015, eps 0.0010000000001532858, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 588
=== ep: 589, time 121.0146849155426, eps 0.00100000000014581, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 589
=== ep: 590, time 29.319705724716187, eps 0.0010000000001386988, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 590
=== ep: 591, time 29.835264682769775, eps 0.0010000000001319344, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 591
=== ep: 592, time 30.753878355026245, eps 0.0010000000001255, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 592
=== ep: 593, time 29.49542737007141, eps 0.0010000000001193791, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 593
=== ep: 594, time 30.829534769058228, eps 0.001000000000113557, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 594
=== ep: 595, time 31.914605379104614, eps 0.0010000000001080186, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 595
=== ep: 596, time 28.575807332992554, eps 0.0010000000001027505, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 596
=== ep: 597, time 26.837830543518066, eps 0.0010000000000977393, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 597
=== ep: 598, time 33.70042824745178, eps 0.0010000000000929725, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 598
=== ep: 599, time 108.47235989570618, eps 0.0010000000000884382, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 599
=== ep: 600, time 27.543644428253174, eps 0.001000000000084125, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 600
=== ep: 601, time 29.786460638046265, eps 0.0010000000000800222, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 601
=== ep: 602, time 27.81690812110901, eps 0.0010000000000761195, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 602
=== ep: 603, time 27.416864156723022, eps 0.0010000000000724072, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 603
=== ep: 604, time 28.792519330978394, eps 0.0010000000000688757, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 604
=== ep: 605, time 27.479286909103394, eps 0.0010000000000655166, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 605
=== ep: 606, time 27.99903392791748, eps 0.0010000000000623215, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 606
=== ep: 607, time 28.380269289016724, eps 0.001000000000059282, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 607
=== ep: 608, time 27.866938591003418, eps 0.0010000000000563907, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 608
=== ep: 609, time 97.37790775299072, eps 0.0010000000000536405, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 609
=== ep: 610, time 29.627768754959106, eps 0.0010000000000510245, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 610
=== ep: 611, time 29.074790954589844, eps 0.0010000000000485358, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 611
=== ep: 612, time 37.47136569023132, eps 0.0010000000000461688, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 612
=== ep: 613, time 28.35887837409973, eps 0.0010000000000439171, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 613
=== ep: 614, time 31.44747519493103, eps 0.0010000000000417752, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 614
goal_identified
=== ep: 615, time 28.551464796066284, eps 0.0010000000000397378, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 395
=== ep: 616, time 29.821425914764404, eps 0.0010000000000377999, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 616
goal_identified
=== ep: 617, time 28.158157348632812, eps 0.0010000000000359563, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 617
=== ep: 618, time 34.38126826286316, eps 0.0010000000000342027, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 618
=== ep: 619, time 107.81474924087524, eps 0.0010000000000325345, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 619
=== ep: 620, time 35.898539781570435, eps 0.001000000000030948, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 620
=== ep: 621, time 29.266163110733032, eps 0.0010000000000294385, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 621
=== ep: 622, time 37.96275186538696, eps 0.0010000000000280028, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 622
goal_identified
=== ep: 623, time 27.690487146377563, eps 0.0010000000000266371, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 623
goal_identified
=== ep: 624, time 28.51651382446289, eps 0.001000000000025338, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 624
=== ep: 625, time 29.405725240707397, eps 0.0010000000000241023, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 625
goal_identified
=== ep: 626, time 27.16854691505432, eps 0.0010000000000229268, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 399
=== ep: 627, time 27.547362565994263, eps 0.0010000000000218085, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 627
goal_identified
=== ep: 628, time 28.99772071838379, eps 0.001000000000020745, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 628
=== ep: 629, time 92.38353085517883, eps 0.0010000000000197332, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 629
goal_identified
=== ep: 630, time 29.103586673736572, eps 0.0010000000000187708, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 400
=== ep: 631, time 29.336405515670776, eps 0.0010000000000178553, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 631
=== ep: 632, time 38.497413635253906, eps 0.0010000000000169845, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 632
=== ep: 633, time 28.580498933792114, eps 0.0010000000000161562, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 633
=== ep: 634, time 28.245153427124023, eps 0.0010000000000153684, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 634
=== ep: 635, time 27.958494424819946, eps 0.0010000000000146188, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 635
=== ep: 636, time 29.85722064971924, eps 0.0010000000000139058, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 636
=== ep: 637, time 35.108325481414795, eps 0.0010000000000132275, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 637
=== ep: 638, time 27.428925037384033, eps 0.0010000000000125824, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 638
=== ep: 639, time 94.0003752708435, eps 0.0010000000000119687, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 639
=== ep: 640, time 27.44594931602478, eps 0.001000000000011385, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 640
=== ep: 641, time 27.976881742477417, eps 0.00100000000001083, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 641
=== ep: 642, time 27.245956659317017, eps 0.0010000000000103017, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 642
=== ep: 643, time 32.30030941963196, eps 0.0010000000000097993, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 643
=== ep: 644, time 27.511391401290894, eps 0.0010000000000093213, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 644
=== ep: 645, time 27.721669673919678, eps 0.0010000000000088666, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 645
goal_identified
=== ep: 646, time 28.74728560447693, eps 0.0010000000000084342, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 646
=== ep: 647, time 28.7485408782959, eps 0.001000000000008023, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 647
=== ep: 648, time 32.204999685287476, eps 0.0010000000000076317, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 648
=== ep: 649, time 107.44642806053162, eps 0.0010000000000072594, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 649
=== ep: 650, time 28.18561100959778, eps 0.0010000000000069055, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 650
goal_identified
=== ep: 651, time 30.750633716583252, eps 0.0010000000000065686, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 651
=== ep: 652, time 38.135488748550415, eps 0.0010000000000062483, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 652
=== ep: 653, time 29.214020252227783, eps 0.0010000000000059436, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 653
=== ep: 654, time 28.57641625404358, eps 0.0010000000000056537, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 654
goal_identified
=== ep: 655, time 27.08787441253662, eps 0.0010000000000053779, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 655
=== ep: 656, time 29.451231479644775, eps 0.0010000000000051157, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 656
goal_identified
=== ep: 657, time 28.32464575767517, eps 0.0010000000000048661, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 657
=== ep: 658, time 29.369028329849243, eps 0.001000000000004629, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 658
=== ep: 659, time 89.54666996002197, eps 0.0010000000000044032, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 659
=== ep: 660, time 28.407172441482544, eps 0.0010000000000041883, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 660
=== ep: 661, time 28.9052414894104, eps 0.001000000000003984, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 661
goal_identified
=== ep: 662, time 28.06306004524231, eps 0.0010000000000037897, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 662
=== ep: 663, time 27.96189022064209, eps 0.001000000000003605, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 663
=== ep: 664, time 34.266717195510864, eps 0.0010000000000034291, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 664
=== ep: 665, time 38.179431676864624, eps 0.001000000000003262, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 665
goal_identified
=== ep: 666, time 30.802578926086426, eps 0.0010000000000031028, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 666
=== ep: 667, time 28.304654598236084, eps 0.0010000000000029514, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 667
=== ep: 668, time 33.98432111740112, eps 0.0010000000000028075, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 668
=== ep: 669, time 92.4462194442749, eps 0.0010000000000026706, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 669
=== ep: 670, time 30.212984800338745, eps 0.0010000000000025403, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 670
=== ep: 671, time 28.507388830184937, eps 0.0010000000000024165, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 671
=== ep: 672, time 28.684235334396362, eps 0.0010000000000022985, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 672
=== ep: 673, time 31.406497478485107, eps 0.0010000000000021864, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 673
=== ep: 674, time 27.638214111328125, eps 0.00100000000000208, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 674
=== ep: 675, time 31.34414315223694, eps 0.0010000000000019785, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 675
=== ep: 676, time 28.357126235961914, eps 0.001000000000001882, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 676
=== ep: 677, time 28.95236039161682, eps 0.0010000000000017903, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 677
=== ep: 678, time 27.131176233291626, eps 0.0010000000000017029, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 678
=== ep: 679, time 103.62483191490173, eps 0.0010000000000016198, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 679
=== ep: 680, time 29.680492401123047, eps 0.0010000000000015409, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 680
=== ep: 681, time 26.877296209335327, eps 0.0010000000000014656, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 681
=== ep: 682, time 30.67641019821167, eps 0.0010000000000013943, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 682
=== ep: 683, time 37.615965604782104, eps 0.0010000000000013262, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 683
=== ep: 684, time 28.313560485839844, eps 0.0010000000000012616, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 684
=== ep: 685, time 29.762017965316772, eps 0.0010000000000012, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 685
=== ep: 686, time 28.014109134674072, eps 0.0010000000000011415, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 686
=== ep: 687, time 29.29728865623474, eps 0.0010000000000010857, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 687
=== ep: 688, time 29.23055386543274, eps 0.0010000000000010328, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 688
=== ep: 689, time 88.65541076660156, eps 0.0010000000000009825, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 689
=== ep: 690, time 27.006219387054443, eps 0.0010000000000009346, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 690
=== ep: 691, time 30.953011512756348, eps 0.001000000000000889, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 691
goal_identified
=== ep: 692, time 27.97041606903076, eps 0.0010000000000008457, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 460
=== ep: 693, time 29.2917959690094, eps 0.0010000000000008045, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 693
=== ep: 694, time 33.567368030548096, eps 0.0010000000000007653, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 694
=== ep: 695, time 28.354753971099854, eps 0.0010000000000007277, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 695
=== ep: 696, time 29.78253698348999, eps 0.0010000000000006924, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 696
=== ep: 697, time 28.200618982315063, eps 0.0010000000000006586, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 697
=== ep: 698, time 28.93066167831421, eps 0.0010000000000006265, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 698
=== ep: 699, time 97.34244084358215, eps 0.001000000000000596, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 699
=== ep: 700, time 27.681392192840576, eps 0.0010000000000005668, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 700
=== ep: 701, time 36.20042276382446, eps 0.0010000000000005393, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 701
=== ep: 702, time 29.35505509376526, eps 0.0010000000000005128, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 702
=== ep: 703, time 28.824013471603394, eps 0.001000000000000488, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 703
=== ep: 704, time 29.114351511001587, eps 0.001000000000000464, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 704
=== ep: 705, time 28.853139638900757, eps 0.0010000000000004415, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 705
goal_identified
goal_identified
=== ep: 706, time 29.821195125579834, eps 0.00100000000000042, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 488
=== ep: 707, time 29.610044956207275, eps 0.0010000000000003994, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 707
=== ep: 708, time 28.10222053527832, eps 0.00100000000000038, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 708
=== ep: 709, time 108.08240485191345, eps 0.0010000000000003615, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 709
=== ep: 710, time 28.59219789505005, eps 0.0010000000000003437, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 710
=== ep: 711, time 28.262787103652954, eps 0.001000000000000327, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 711
=== ep: 712, time 27.52806067466736, eps 0.0010000000000003112, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 712
=== ep: 713, time 28.403300762176514, eps 0.001000000000000296, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 713
=== ep: 714, time 39.40392756462097, eps 0.0010000000000002815, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 714
=== ep: 715, time 28.1919584274292, eps 0.0010000000000002678, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 715
=== ep: 716, time 30.196781635284424, eps 0.0010000000000002548, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 716
=== ep: 717, time 27.881134033203125, eps 0.0010000000000002422, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 717
=== ep: 718, time 36.93508982658386, eps 0.0010000000000002305, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 718
=== ep: 719, time 86.50296521186829, eps 0.0010000000000002192, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 719
=== ep: 720, time 38.840696811676025, eps 0.0010000000000002086, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 720
=== ep: 721, time 32.00550985336304, eps 0.0010000000000001984, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 721
=== ep: 722, time 31.173001050949097, eps 0.0010000000000001887, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 722
=== ep: 723, time 29.922375202178955, eps 0.0010000000000001796, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 723
=== ep: 724, time 28.67776894569397, eps 0.0010000000000001707, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 724
=== ep: 725, time 30.82309365272522, eps 0.0010000000000001624, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 725
=== ep: 726, time 33.872962951660156, eps 0.0010000000000001544, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 726
=== ep: 727, time 28.303564071655273, eps 0.001000000000000147, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 727
=== ep: 728, time 28.07009506225586, eps 0.0010000000000001399, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 728
=== ep: 729, time 106.66205501556396, eps 0.001000000000000133, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 729
=== ep: 730, time 27.614078283309937, eps 0.0010000000000001264, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 730
=== ep: 731, time 31.354952096939087, eps 0.0010000000000001204, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 731
=== ep: 732, time 29.582379817962646, eps 0.0010000000000001145, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 732
=== ep: 733, time 31.33712935447693, eps 0.0010000000000001089, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 733
=== ep: 734, time 28.039462089538574, eps 0.0010000000000001037, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 734
=== ep: 735, time 34.51823019981384, eps 0.0010000000000000985, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 735
=== ep: 736, time 28.652888774871826, eps 0.0010000000000000937, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 736
=== ep: 737, time 29.872962951660156, eps 0.0010000000000000891, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 737
=== ep: 738, time 28.619335174560547, eps 0.0010000000000000848, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 738
=== ep: 739, time 106.00924110412598, eps 0.0010000000000000807, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 739
=== ep: 740, time 35.28048038482666, eps 0.0010000000000000768, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 740
=== ep: 741, time 29.402531385421753, eps 0.001000000000000073, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 741
=== ep: 742, time 28.182507276535034, eps 0.0010000000000000694, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 742
=== ep: 743, time 31.189887285232544, eps 0.001000000000000066, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 743
=== ep: 744, time 27.857632875442505, eps 0.001000000000000063, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 744
=== ep: 745, time 32.136486530303955, eps 0.0010000000000000599, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 745
=== ep: 746, time 28.311424255371094, eps 0.0010000000000000568, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 746
=== ep: 747, time 29.737821102142334, eps 0.001000000000000054, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 747
=== ep: 748, time 28.539695978164673, eps 0.0010000000000000514, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 748
=== ep: 749, time 104.25710535049438, eps 0.001000000000000049, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 749
=== ep: 750, time 28.12653398513794, eps 0.0010000000000000466, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 750
=== ep: 751, time 31.055439949035645, eps 0.0010000000000000443, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 751
=== ep: 752, time 28.69237470626831, eps 0.001000000000000042, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 752
=== ep: 753, time 28.103059768676758, eps 0.0010000000000000401, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 753
=== ep: 754, time 40.46553635597229, eps 0.0010000000000000382, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 754
=== ep: 755, time 30.626149654388428, eps 0.0010000000000000362, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 755
goal_identified
=== ep: 756, time 30.252760648727417, eps 0.0010000000000000345, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 756
=== ep: 757, time 28.7409405708313, eps 0.0010000000000000328, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 757
=== ep: 758, time 27.941462993621826, eps 0.0010000000000000312, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 758
=== ep: 759, time 95.83802151679993, eps 0.0010000000000000297, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 759
=== ep: 760, time 33.819185972213745, eps 0.0010000000000000282, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 760
=== ep: 761, time 28.848835468292236, eps 0.001000000000000027, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 761
=== ep: 762, time 29.7351291179657, eps 0.0010000000000000256, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 762
=== ep: 763, time 28.133267164230347, eps 0.0010000000000000243, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 763
=== ep: 764, time 32.53744196891785, eps 0.0010000000000000232, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 764
=== ep: 765, time 36.42111897468567, eps 0.001000000000000022, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 765
=== ep: 766, time 30.51417851448059, eps 0.0010000000000000208, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 766
=== ep: 767, time 27.745178937911987, eps 0.00100000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 767
=== ep: 768, time 32.406158685684204, eps 0.0010000000000000189, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 768
=== ep: 769, time 88.01173853874207, eps 0.001000000000000018, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 769
=== ep: 770, time 29.37900424003601, eps 0.0010000000000000172, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 770
=== ep: 771, time 29.66808271408081, eps 0.0010000000000000163, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 771
=== ep: 772, time 30.098162174224854, eps 0.0010000000000000154, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 772
=== ep: 773, time 28.55373477935791, eps 0.0010000000000000148, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 773
=== ep: 774, time 38.32482695579529, eps 0.0010000000000000141, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 774
=== ep: 775, time 30.235381603240967, eps 0.0010000000000000132, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 775
=== ep: 776, time 28.73903250694275, eps 0.0010000000000000126, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 776
=== ep: 777, time 29.576987743377686, eps 0.0010000000000000122, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 777
=== ep: 778, time 29.424712419509888, eps 0.0010000000000000115, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 778
=== ep: 779, time 112.53328561782837, eps 0.0010000000000000109, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 779
=== ep: 780, time 34.00951528549194, eps 0.0010000000000000104, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 780
=== ep: 781, time 31.04107356071472, eps 0.00100000000000001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 781
goal_identified
=== ep: 782, time 28.432604789733887, eps 0.0010000000000000093, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 782
=== ep: 783, time 30.63270616531372, eps 0.001000000000000009, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 783
=== ep: 784, time 29.864283800125122, eps 0.0010000000000000085, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 784
=== ep: 785, time 28.841328859329224, eps 0.001000000000000008, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 785
=== ep: 786, time 29.771244764328003, eps 0.0010000000000000076, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 786
=== ep: 787, time 28.75068163871765, eps 0.0010000000000000074, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 787
=== ep: 788, time 35.63806414604187, eps 0.001000000000000007, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 788
=== ep: 789, time 112.61086511611938, eps 0.0010000000000000067, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 789
=== ep: 790, time 28.00143074989319, eps 0.0010000000000000063, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 790
=== ep: 791, time 37.382227659225464, eps 0.001000000000000006, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 791
=== ep: 792, time 30.325424671173096, eps 0.0010000000000000057, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 792
=== ep: 793, time 30.3037748336792, eps 0.0010000000000000054, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 793
=== ep: 794, time 34.050228118896484, eps 0.0010000000000000052, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 794
=== ep: 795, time 33.34629702568054, eps 0.001000000000000005, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 795
=== ep: 796, time 32.18818426132202, eps 0.0010000000000000048, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 796
=== ep: 797, time 30.141894340515137, eps 0.0010000000000000044, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 797
=== ep: 798, time 28.94833517074585, eps 0.0010000000000000041, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 798
=== ep: 799, time 108.7991452217102, eps 0.0010000000000000041, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 799
=== ep: 800, time 29.27920627593994, eps 0.001000000000000004, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 800
=== ep: 801, time 30.232775688171387, eps 0.0010000000000000037, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 801
=== ep: 802, time 43.06671905517578, eps 0.0010000000000000035, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 802
=== ep: 803, time 31.69272541999817, eps 0.0010000000000000033, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 803
=== ep: 804, time 28.24374294281006, eps 0.001000000000000003, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 804
=== ep: 805, time 34.08310413360596, eps 0.001000000000000003, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 805
=== ep: 806, time 29.96014142036438, eps 0.0010000000000000028, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 806
=== ep: 807, time 28.184011936187744, eps 0.0010000000000000026, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 807
=== ep: 808, time 38.43658208847046, eps 0.0010000000000000026, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 808
goal_identified
=== ep: 809, time 115.98483109474182, eps 0.0010000000000000024, sum reward: 1, score_diff 1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 529
=== ep: 810, time 33.19097375869751, eps 0.0010000000000000024, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 810
=== ep: 811, time 32.06338047981262, eps 0.0010000000000000022, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 811
=== ep: 812, time 29.028399229049683, eps 0.0010000000000000022, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 812
=== ep: 813, time 32.08559989929199, eps 0.001000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 813
=== ep: 814, time 30.607394695281982, eps 0.001000000000000002, sum reward: 0, score_diff -5, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 814
=== ep: 815, time 32.690552949905396, eps 0.0010000000000000018, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 815
=== ep: 816, time 27.4266676902771, eps 0.0010000000000000018, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 816
=== ep: 817, time 31.115092992782593, eps 0.0010000000000000018, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 817
=== ep: 818, time 29.38193988800049, eps 0.0010000000000000015, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 818
goal_identified
=== ep: 819, time 118.30603885650635, eps 0.0010000000000000015, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 819
=== ep: 820, time 29.535828113555908, eps 0.0010000000000000013, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 820
=== ep: 821, time 30.467276573181152, eps 0.0010000000000000013, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 821
=== ep: 822, time 29.312466859817505, eps 0.0010000000000000013, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 822
=== ep: 823, time 30.190462589263916, eps 0.0010000000000000013, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 823
=== ep: 824, time 34.589560985565186, eps 0.001000000000000001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 824
=== ep: 825, time 29.316709280014038, eps 0.001000000000000001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 825
=== ep: 826, time 34.56370401382446, eps 0.001000000000000001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 826
=== ep: 827, time 31.639371156692505, eps 0.001000000000000001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 827
goal_identified
=== ep: 828, time 29.112829208374023, eps 0.0010000000000000009, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 538
=== ep: 829, time 106.27897214889526, eps 0.0010000000000000009, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 829
goal_identified
=== ep: 830, time 35.6855092048645, eps 0.0010000000000000009, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 830
=== ep: 831, time 31.55717897415161, eps 0.0010000000000000009, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 831
goal_identified
=== ep: 832, time 32.50140881538391, eps 0.0010000000000000009, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 615
=== ep: 833, time 29.75295639038086, eps 0.0010000000000000007, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 833
=== ep: 834, time 30.004700183868408, eps 0.0010000000000000007, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 834
=== ep: 835, time 29.915443181991577, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 835
=== ep: 836, time 28.577275276184082, eps 0.0010000000000000007, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 836
=== ep: 837, time 38.36847448348999, eps 0.0010000000000000007, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 837
=== ep: 838, time 38.19885563850403, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 838
=== ep: 839, time 119.55087542533875, eps 0.0010000000000000007, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 839
=== ep: 840, time 33.76880383491516, eps 0.0010000000000000005, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 840
=== ep: 841, time 37.132561922073364, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 841
=== ep: 842, time 29.85427451133728, eps 0.0010000000000000005, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 842
=== ep: 843, time 31.057490348815918, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 843
=== ep: 844, time 39.72846865653992, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 844
=== ep: 845, time 35.102299213409424, eps 0.0010000000000000005, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 845
=== ep: 846, time 41.1109344959259, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 846
=== ep: 847, time 34.32210159301758, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 847
=== ep: 848, time 29.009530305862427, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 848
=== ep: 849, time 113.77031064033508, eps 0.0010000000000000005, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 849
=== ep: 850, time 37.18189454078674, eps 0.0010000000000000002, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 850
=== ep: 851, time 30.977201461791992, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 851
=== ep: 852, time 32.15761470794678, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 852
=== ep: 853, time 31.18718934059143, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 853
=== ep: 854, time 35.91972756385803, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 854
=== ep: 855, time 30.302493810653687, eps 0.0010000000000000002, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 855
=== ep: 856, time 30.54996919631958, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 856
=== ep: 857, time 31.43368697166443, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 857
=== ep: 858, time 28.997066497802734, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 858
=== ep: 859, time 127.30124926567078, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 859
=== ep: 860, time 30.302884101867676, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 860
=== ep: 861, time 31.477906942367554, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 861
=== ep: 862, time 31.87152910232544, eps 0.0010000000000000002, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 862
=== ep: 863, time 31.431097269058228, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 863
=== ep: 864, time 31.348926067352295, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 864
=== ep: 865, time 29.662130117416382, eps 0.0010000000000000002, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 865
=== ep: 866, time 29.718000888824463, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 866
=== ep: 867, time 31.690720796585083, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 867
=== ep: 868, time 30.7862811088562, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 868
=== ep: 869, time 111.40410876274109, eps 0.0010000000000000002, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 869
=== ep: 870, time 30.96089005470276, eps 0.0010000000000000002, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 870
goal_identified
=== ep: 871, time 31.896713733673096, eps 0.0010000000000000002, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 871
=== ep: 872, time 36.897603273391724, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 872
=== ep: 873, time 31.800329208374023, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 873
=== ep: 874, time 31.100950717926025, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 874
goal_identified
=== ep: 875, time 29.57266354560852, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 875
=== ep: 876, time 32.06817960739136, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 876
=== ep: 877, time 29.492568731307983, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 877
=== ep: 878, time 30.726142406463623, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 878
=== ep: 879, time 121.327139377594, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 879
=== ep: 880, time 29.87075901031494, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 880
=== ep: 881, time 32.99835181236267, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 881
=== ep: 882, time 29.79950189590454, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 882
=== ep: 883, time 29.344094038009644, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 883
=== ep: 884, time 44.22326445579529, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 884
=== ep: 885, time 31.578091621398926, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 885
=== ep: 886, time 29.36454153060913, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 886
=== ep: 887, time 33.524399280548096, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 887
=== ep: 888, time 31.149867296218872, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 888
=== ep: 889, time 113.05940890312195, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 889
=== ep: 890, time 30.948471546173096, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 890
=== ep: 891, time 39.216150760650635, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 891
goal_identified
=== ep: 892, time 32.58358454704285, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 626
=== ep: 893, time 31.041845560073853, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 893
=== ep: 894, time 34.0972683429718, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 894
=== ep: 895, time 30.173532247543335, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 895
=== ep: 896, time 32.578797817230225, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 896
=== ep: 897, time 42.3785560131073, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 897
=== ep: 898, time 28.641558408737183, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 898
goal_identified
=== ep: 899, time 133.99567222595215, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 899
=== ep: 900, time 29.38705086708069, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 900
=== ep: 901, time 29.13541054725647, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 901
=== ep: 902, time 37.95785474777222, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 902
=== ep: 903, time 32.25165271759033, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 903
=== ep: 904, time 31.021111488342285, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 904
goal_identified
=== ep: 905, time 29.51592755317688, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 630
=== ep: 906, time 33.143619775772095, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 906
=== ep: 907, time 29.508620262145996, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 907
=== ep: 908, time 29.996599435806274, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 908
=== ep: 909, time 112.33377146720886, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 909
=== ep: 910, time 31.198695182800293, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 910
=== ep: 911, time 29.187883138656616, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 911
=== ep: 912, time 29.806870937347412, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 912
=== ep: 913, time 33.21674132347107, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 913
=== ep: 914, time 33.59009099006653, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 914
goal_identified
goal_identified
=== ep: 915, time 29.676234006881714, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 692
=== ep: 916, time 29.980025053024292, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 916
=== ep: 917, time 30.73701047897339, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 917
=== ep: 918, time 29.170016765594482, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 918
=== ep: 919, time 128.2558093070984, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 919
=== ep: 920, time 29.430656909942627, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 920
=== ep: 921, time 29.93720841407776, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 921
=== ep: 922, time 30.225141286849976, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 922
=== ep: 923, time 29.438885927200317, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 923
=== ep: 924, time 29.07413411140442, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 924
=== ep: 925, time 32.92486095428467, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 925
=== ep: 926, time 34.65790319442749, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 926
=== ep: 927, time 29.252792835235596, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 927
=== ep: 928, time 35.37601590156555, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 928
=== ep: 929, time 111.41429352760315, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 929
=== ep: 930, time 28.566694259643555, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 930
=== ep: 931, time 30.573808670043945, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 931
=== ep: 932, time 29.146944046020508, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 932
=== ep: 933, time 32.72757053375244, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 933
=== ep: 934, time 33.0535945892334, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 934
=== ep: 935, time 31.20942497253418, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 935
=== ep: 936, time 30.408814430236816, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 936
=== ep: 937, time 32.3076229095459, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 937
=== ep: 938, time 34.45179080963135, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 938
=== ep: 939, time 124.28261613845825, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 939
=== ep: 940, time 30.299824476242065, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 940
=== ep: 941, time 29.410507202148438, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 941
=== ep: 942, time 29.05916929244995, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 942
goal_identified
=== ep: 943, time 28.806177139282227, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 943
=== ep: 944, time 35.89514088630676, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 944
=== ep: 945, time 31.994922161102295, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 945
=== ep: 946, time 28.814903020858765, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 946
=== ep: 947, time 31.29858088493347, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 947
=== ep: 948, time 28.541435956954956, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 948
=== ep: 949, time 124.69979786872864, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 949
=== ep: 950, time 29.586761474609375, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 950
=== ep: 951, time 31.32977056503296, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 951
=== ep: 952, time 30.738492488861084, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 952
=== ep: 953, time 32.318279504776, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 953
=== ep: 954, time 29.019076824188232, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 954
=== ep: 955, time 37.42945098876953, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 955
=== ep: 956, time 29.248289585113525, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 956
=== ep: 957, time 29.331737756729126, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 957
=== ep: 958, time 30.415160655975342, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 958
goal_identified
=== ep: 959, time 118.14544558525085, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 959
=== ep: 960, time 28.4978244304657, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 960
=== ep: 961, time 30.761554479599, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 961
=== ep: 962, time 29.91469645500183, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 962
goal_identified
=== ep: 963, time 28.989562034606934, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 963
=== ep: 964, time 30.821613788604736, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 964
=== ep: 965, time 33.289186000823975, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 965
=== ep: 966, time 29.88515591621399, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 966
=== ep: 967, time 28.943068027496338, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 967
=== ep: 968, time 30.282032251358032, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 968
=== ep: 969, time 119.93179440498352, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 969
=== ep: 970, time 34.19088625907898, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 970
=== ep: 971, time 34.541258335113525, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 971
=== ep: 972, time 33.99261951446533, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 972
=== ep: 973, time 32.35903000831604, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 973
=== ep: 974, time 28.30550980567932, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 974
=== ep: 975, time 30.093209743499756, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 975
=== ep: 976, time 36.0441997051239, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 976
=== ep: 977, time 32.45742082595825, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 977
=== ep: 978, time 31.45016312599182, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 978
=== ep: 979, time 135.16152262687683, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 979
=== ep: 980, time 33.442076683044434, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 980
=== ep: 981, time 32.03972339630127, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 981
=== ep: 982, time 33.117398738861084, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 982
=== ep: 983, time 28.67190670967102, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 983
=== ep: 984, time 41.23640584945679, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 984
=== ep: 985, time 31.38578224182129, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 985
=== ep: 986, time 30.311512231826782, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 986
=== ep: 987, time 33.32173991203308, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 987
=== ep: 988, time 29.25876545906067, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 988
=== ep: 989, time 120.02563858032227, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 989
=== ep: 990, time 28.0929172039032, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 990
goal_identified
=== ep: 991, time 31.26196575164795, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 706
=== ep: 992, time 28.87377953529358, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 992
=== ep: 993, time 28.885637998580933, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 993
=== ep: 994, time 30.09675168991089, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 994
=== ep: 995, time 28.073378086090088, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 995
=== ep: 996, time 29.081960201263428, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 996
=== ep: 997, time 31.51967215538025, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 997
=== ep: 998, time 29.89516592025757, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 998
=== ep: 999, time 122.02555418014526, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 999
=== ep: 1000, time 30.394168615341187, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1000
=== ep: 1001, time 30.092041015625, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1001
goal_identified
=== ep: 1002, time 31.58360505104065, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 809
=== ep: 1003, time 36.28346037864685, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1003
=== ep: 1004, time 31.73140239715576, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1004
goal_identified
=== ep: 1005, time 29.18839168548584, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 828
=== ep: 1006, time 36.192936420440674, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1006
=== ep: 1007, time 35.41695308685303, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1007
=== ep: 1008, time 28.104163646697998, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1008
=== ep: 1009, time 163.6635880470276, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1009
=== ep: 1010, time 29.41994833946228, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1010
goal_identified
=== ep: 1011, time 27.208452224731445, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1011
=== ep: 1012, time 31.221668243408203, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1012
=== ep: 1013, time 28.04620337486267, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1013
goal_identified
=== ep: 1014, time 30.471527338027954, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 832
=== ep: 1015, time 30.284823179244995, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1015
=== ep: 1016, time 28.450056314468384, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1016
=== ep: 1017, time 29.222965240478516, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1017
=== ep: 1018, time 30.780986785888672, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1018
goal_identified
=== ep: 1019, time 124.57524728775024, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1019
=== ep: 1020, time 35.88320469856262, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1020
=== ep: 1021, time 32.83271932601929, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1021
=== ep: 1022, time 27.879631996154785, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1022
=== ep: 1023, time 31.40632915496826, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1023
=== ep: 1024, time 28.21456003189087, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1024
=== ep: 1025, time 34.70808935165405, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1025
=== ep: 1026, time 26.992875337600708, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1026
=== ep: 1027, time 27.68747329711914, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1027
=== ep: 1028, time 33.334640979766846, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1028
=== ep: 1029, time 129.2491466999054, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1029
=== ep: 1030, time 27.414472818374634, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1030
=== ep: 1031, time 27.337544202804565, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1031
=== ep: 1032, time 27.386826515197754, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1032
=== ep: 1033, time 28.13236355781555, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1033
=== ep: 1034, time 30.219666719436646, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1034
=== ep: 1035, time 29.659915924072266, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1035
=== ep: 1036, time 27.93605327606201, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1036
=== ep: 1037, time 34.2968487739563, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1037
=== ep: 1038, time 34.956692934036255, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1038
=== ep: 1039, time 124.13782334327698, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1039
=== ep: 1040, time 26.56939959526062, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1040
goal_identified
goal_identified
=== ep: 1041, time 28.09339165687561, eps 0.001, sum reward: 2, score_diff 2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 892
=== ep: 1042, time 32.320005893707275, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1042
=== ep: 1043, time 30.574624061584473, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1043
=== ep: 1044, time 42.589165925979614, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1044
goal_identified
=== ep: 1045, time 27.564945697784424, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1045
=== ep: 1046, time 27.23805069923401, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1046
=== ep: 1047, time 30.55775260925293, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1047
=== ep: 1048, time 32.18438649177551, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1048
=== ep: 1049, time 112.72913980484009, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1049
=== ep: 1050, time 37.8407142162323, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1050
=== ep: 1051, time 28.964985132217407, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1051
=== ep: 1052, time 37.82870936393738, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1052
=== ep: 1053, time 33.18091559410095, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1053
=== ep: 1054, time 27.891623973846436, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1054
=== ep: 1055, time 29.735849618911743, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1055
=== ep: 1056, time 27.855668306350708, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1056
=== ep: 1057, time 28.97277283668518, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1057
=== ep: 1058, time 31.203282117843628, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1058
=== ep: 1059, time 111.34300231933594, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1059
goal_identified
goal_identified
=== ep: 1060, time 28.325950860977173, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1060
goal_identified
goal_identified
=== ep: 1061, time 28.786502599716187, eps 0.001, sum reward: 2, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1061
=== ep: 1062, time 28.48129916191101, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1062
=== ep: 1063, time 28.861743450164795, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1063
=== ep: 1064, time 33.134647846221924, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1064
=== ep: 1065, time 29.51350998878479, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1065
=== ep: 1066, time 35.99002385139465, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1066
=== ep: 1067, time 31.63153839111328, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1067
=== ep: 1068, time 28.44830584526062, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1068
=== ep: 1069, time 113.31008768081665, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1069
=== ep: 1070, time 36.02852988243103, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1070
goal_identified
=== ep: 1071, time 29.885070085525513, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1071
=== ep: 1072, time 30.924920558929443, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1072
=== ep: 1073, time 27.493488073349, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1073
=== ep: 1074, time 29.066720485687256, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1074
=== ep: 1075, time 30.75350594520569, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1075
=== ep: 1076, time 27.765974521636963, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1076
=== ep: 1077, time 27.184308767318726, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1077
goal_identified
=== ep: 1078, time 28.706541538238525, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1078
=== ep: 1079, time 121.90178608894348, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1079
=== ep: 1080, time 31.20064401626587, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1080
=== ep: 1081, time 38.78274059295654, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1081
=== ep: 1082, time 29.435807466506958, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1082
=== ep: 1083, time 30.149213314056396, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1083
=== ep: 1084, time 28.197223901748657, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1084
=== ep: 1085, time 27.841766834259033, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1085
=== ep: 1086, time 28.12931251525879, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1086
=== ep: 1087, time 29.880948543548584, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1087
=== ep: 1088, time 30.31660509109497, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1088
=== ep: 1089, time 132.08434176445007, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1089
goal_identified
=== ep: 1090, time 31.724422693252563, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 905
=== ep: 1091, time 33.76621747016907, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1091
=== ep: 1092, time 27.64452075958252, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1092
=== ep: 1093, time 33.18843960762024, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1093
goal_identified
=== ep: 1094, time 36.11631226539612, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 991
=== ep: 1095, time 31.936242818832397, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1095
=== ep: 1096, time 29.02532720565796, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1096
=== ep: 1097, time 28.936557292938232, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1097
goal_identified
=== ep: 1098, time 27.697310209274292, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1098
=== ep: 1099, time 134.36946988105774, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1099
=== ep: 1100, time 30.66748046875, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1100
=== ep: 1101, time 30.280165433883667, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1101
=== ep: 1102, time 27.53543496131897, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1102
=== ep: 1103, time 27.678189754486084, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1103
=== ep: 1104, time 28.574589252471924, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1104
=== ep: 1105, time 27.668299198150635, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1105
=== ep: 1106, time 26.958393812179565, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1106
=== ep: 1107, time 26.728939533233643, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1107
=== ep: 1108, time 27.475106477737427, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1108
=== ep: 1109, time 116.96581029891968, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1109
=== ep: 1110, time 27.208543300628662, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1110
goal_identified
=== ep: 1111, time 26.49044942855835, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1002
=== ep: 1112, time 26.705246448516846, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1112
=== ep: 1113, time 27.103912830352783, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1113
=== ep: 1114, time 26.67085313796997, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1114
=== ep: 1115, time 28.979572296142578, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1115
=== ep: 1116, time 31.252304315567017, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1116
=== ep: 1117, time 27.353877067565918, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1117
=== ep: 1118, time 27.342206716537476, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1118
=== ep: 1119, time 104.77001285552979, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1119
=== ep: 1120, time 26.3834285736084, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1120
=== ep: 1121, time 28.67640781402588, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1121
=== ep: 1122, time 26.677072763442993, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1122
goal_identified
=== ep: 1123, time 26.185016632080078, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1123
=== ep: 1124, time 27.139152765274048, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1124
=== ep: 1125, time 34.02794170379639, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1125
=== ep: 1126, time 27.26900291442871, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1126
=== ep: 1127, time 26.75527048110962, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1127
=== ep: 1128, time 26.150330543518066, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1128
=== ep: 1129, time 102.19338870048523, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1129
=== ep: 1130, time 26.991187572479248, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1130
=== ep: 1131, time 29.943306922912598, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1131
=== ep: 1132, time 28.841435432434082, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1132
goal_identified
=== ep: 1133, time 26.70915699005127, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1133
=== ep: 1134, time 31.900214195251465, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1134
=== ep: 1135, time 31.61347723007202, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1135
=== ep: 1136, time 28.194777488708496, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1136
=== ep: 1137, time 30.388172388076782, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1137
=== ep: 1138, time 32.937692642211914, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1138
=== ep: 1139, time 113.63319253921509, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1139
=== ep: 1140, time 27.821096658706665, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1140
=== ep: 1141, time 26.569491624832153, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1141
=== ep: 1142, time 27.236806392669678, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1142
=== ep: 1143, time 26.99865460395813, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1143
=== ep: 1144, time 27.496760606765747, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1144
goal_identified
=== ep: 1145, time 27.47036337852478, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1145
=== ep: 1146, time 32.728676319122314, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1146
=== ep: 1147, time 28.221447229385376, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1147
=== ep: 1148, time 27.140145778656006, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1148
=== ep: 1149, time 135.25102400779724, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1149
=== ep: 1150, time 27.977020740509033, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1150
=== ep: 1151, time 29.14172911643982, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1151
=== ep: 1152, time 27.459643602371216, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1152
=== ep: 1153, time 34.45450401306152, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1153
=== ep: 1154, time 30.721871614456177, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1154
=== ep: 1155, time 27.699207544326782, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1155
=== ep: 1156, time 26.51214909553528, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1156
=== ep: 1157, time 31.147154569625854, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1157
=== ep: 1158, time 34.170106649398804, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1158
=== ep: 1159, time 143.70496940612793, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1159
goal_identified
=== ep: 1160, time 30.500283002853394, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1005
=== ep: 1161, time 32.88134455680847, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1161
=== ep: 1162, time 31.597996711730957, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1162
=== ep: 1163, time 29.920284748077393, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1163
=== ep: 1164, time 28.49408769607544, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1164
=== ep: 1165, time 28.50357723236084, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1165
=== ep: 1166, time 36.27203345298767, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1166
=== ep: 1167, time 27.515150547027588, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1167
goal_identified
=== ep: 1168, time 34.02166032791138, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1014
=== ep: 1169, time 199.43947577476501, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1169
=== ep: 1170, time 35.91411209106445, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1170
=== ep: 1171, time 28.639960050582886, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1171
=== ep: 1172, time 30.974697828292847, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1172
=== ep: 1173, time 38.28150677680969, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1173
=== ep: 1174, time 28.738043785095215, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1174
=== ep: 1175, time 41.07446885108948, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1175
=== ep: 1176, time 30.17389965057373, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1176
=== ep: 1177, time 29.615367650985718, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1177
=== ep: 1178, time 28.21522068977356, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1178
=== ep: 1179, time 184.2151575088501, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1179
=== ep: 1180, time 27.800055027008057, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1180
goal_identified
=== ep: 1181, time 31.926450729370117, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1090
=== ep: 1182, time 29.26591157913208, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1182
=== ep: 1183, time 29.447864294052124, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1183
=== ep: 1184, time 35.077556133270264, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1184
goal_identified
=== ep: 1185, time 33.75005102157593, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1094
=== ep: 1186, time 35.33994388580322, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1186
=== ep: 1187, time 37.16654133796692, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1187
=== ep: 1188, time 29.100751399993896, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1188
goal_identified
=== ep: 1189, time 193.6749746799469, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1189
=== ep: 1190, time 34.439976930618286, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1190
=== ep: 1191, time 32.921703815460205, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1191
=== ep: 1192, time 38.53766751289368, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1192
=== ep: 1193, time 29.7016122341156, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1193
=== ep: 1194, time 30.52942156791687, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1194
goal_identified
=== ep: 1195, time 41.205646276474, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1111
=== ep: 1196, time 46.820788860321045, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1196
=== ep: 1197, time 28.94955325126648, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1197
=== ep: 1198, time 28.603384017944336, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1198
=== ep: 1199, time 211.73126316070557, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1199
=== ep: 1200, time 30.959733247756958, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1200
=== ep: 1201, time 28.355355262756348, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1201
=== ep: 1202, time 28.549365758895874, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1202
=== ep: 1203, time 35.50112175941467, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1203
=== ep: 1204, time 47.241899728775024, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1204
=== ep: 1205, time 28.513474464416504, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1205
=== ep: 1206, time 30.31775450706482, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1206
=== ep: 1207, time 28.197864770889282, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1207
=== ep: 1208, time 31.89371371269226, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1208
=== ep: 1209, time 238.050883769989, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1209
=== ep: 1210, time 29.382960319519043, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1210
=== ep: 1211, time 30.43748450279236, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1211
=== ep: 1212, time 36.230382442474365, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1212
=== ep: 1213, time 34.55921983718872, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1213
=== ep: 1214, time 30.208009958267212, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1214
=== ep: 1215, time 29.4541335105896, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1215
=== ep: 1216, time 31.26087713241577, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1216
=== ep: 1217, time 29.365314960479736, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1217
goal_identified
=== ep: 1218, time 39.80058312416077, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1160
=== ep: 1219, time 212.2658233642578, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1219
=== ep: 1220, time 35.4573016166687, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1220
=== ep: 1221, time 29.912199020385742, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1221
=== ep: 1222, time 38.11966943740845, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1222
goal_identified
=== ep: 1223, time 29.094202041625977, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1223
=== ep: 1224, time 29.075474739074707, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1224
=== ep: 1225, time 33.28223657608032, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1225
=== ep: 1226, time 30.20772671699524, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1226
=== ep: 1227, time 30.55325222015381, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1227
=== ep: 1228, time 28.231054306030273, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1228
=== ep: 1229, time 277.7227563858032, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1229
=== ep: 1230, time 27.753492832183838, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1230
=== ep: 1231, time 31.150328159332275, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1231
goal_identified
=== ep: 1232, time 30.94409418106079, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1168
=== ep: 1233, time 38.05201029777527, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1233
=== ep: 1234, time 32.52983355522156, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1234
=== ep: 1235, time 33.572715520858765, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1235
=== ep: 1236, time 30.179150819778442, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1236
goal_identified
=== ep: 1237, time 30.953590631484985, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1237
=== ep: 1238, time 32.398906230926514, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1238
=== ep: 1239, time 259.09337091445923, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1239
=== ep: 1240, time 30.171652793884277, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1240
goal_identified
=== ep: 1241, time 28.870746612548828, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1241
=== ep: 1242, time 30.527695655822754, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1242
=== ep: 1243, time 28.449012517929077, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1243
=== ep: 1244, time 37.10966372489929, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1244
=== ep: 1245, time 36.400179386138916, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1245
=== ep: 1246, time 43.169265270233154, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1246
goal_identified
=== ep: 1247, time 29.357916593551636, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1247
=== ep: 1248, time 35.93173933029175, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1248
=== ep: 1249, time 257.19260597229004, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1249
=== ep: 1250, time 29.865715503692627, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1250
=== ep: 1251, time 30.891554832458496, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1251
=== ep: 1252, time 32.858299016952515, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1252
=== ep: 1253, time 34.372841596603394, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1253
goal_identified
=== ep: 1254, time 29.8108811378479, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1181
=== ep: 1255, time 30.77474808692932, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1255
=== ep: 1256, time 36.67930483818054, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1256
=== ep: 1257, time 42.44569516181946, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1257
=== ep: 1258, time 31.48202109336853, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1258
=== ep: 1259, time 206.23654079437256, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1259
=== ep: 1260, time 29.190622329711914, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1260
=== ep: 1261, time 44.793222427368164, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1261
=== ep: 1262, time 30.962555646896362, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1262
=== ep: 1263, time 48.976364612579346, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1263
=== ep: 1264, time 32.19211506843567, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1264
=== ep: 1265, time 28.38206171989441, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1265
=== ep: 1266, time 30.07224678993225, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1266
=== ep: 1267, time 29.682000637054443, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1267
=== ep: 1268, time 32.49075651168823, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1268
=== ep: 1269, time 222.76501059532166, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1269
=== ep: 1270, time 35.66170525550842, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1270
goal_identified
goal_identified
=== ep: 1271, time 33.38914442062378, eps 0.001, sum reward: 2, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1271
goal_identified
=== ep: 1272, time 34.873913288116455, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1185
=== ep: 1273, time 28.897438287734985, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1273
=== ep: 1274, time 25.50912046432495, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1274
=== ep: 1275, time 37.297805070877075, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1275
=== ep: 1276, time 33.56209850311279, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1276
goal_identified
=== ep: 1277, time 29.56272530555725, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1277
=== ep: 1278, time 31.269804000854492, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1278
goal_identified
=== ep: 1279, time 228.74744415283203, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1279
=== ep: 1280, time 31.57781720161438, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1280
=== ep: 1281, time 42.691720962524414, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1281
=== ep: 1282, time 31.773789167404175, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1282
=== ep: 1283, time 30.77280044555664, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1283
=== ep: 1284, time 30.31095242500305, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1284
=== ep: 1285, time 38.322914600372314, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1285
=== ep: 1286, time 34.36349296569824, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1286
=== ep: 1287, time 30.401663303375244, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1287
goal_identified
=== ep: 1288, time 32.667996883392334, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1288
=== ep: 1289, time 216.3854856491089, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1289
=== ep: 1290, time 47.941486120224, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1290
=== ep: 1291, time 28.85127902030945, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1291
=== ep: 1292, time 36.79949164390564, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1292
=== ep: 1293, time 34.759047746658325, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1293
=== ep: 1294, time 33.25762224197388, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1294
=== ep: 1295, time 45.924582719802856, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1295
=== ep: 1296, time 43.999762296676636, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1296
=== ep: 1297, time 30.034331798553467, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1297
=== ep: 1298, time 31.809452533721924, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1298
=== ep: 1299, time 214.02276039123535, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1299
=== ep: 1300, time 34.57103967666626, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1300
=== ep: 1301, time 30.073078632354736, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1301
=== ep: 1302, time 31.742257118225098, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1302
goal_identified
=== ep: 1303, time 32.939000844955444, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1195
=== ep: 1304, time 35.82957124710083, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1304
=== ep: 1305, time 29.204858541488647, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1305
=== ep: 1306, time 36.32704734802246, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1306
=== ep: 1307, time 34.78201746940613, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1307
=== ep: 1308, time 36.0276882648468, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1308
=== ep: 1309, time 214.6764054298401, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1309
=== ep: 1310, time 30.54424023628235, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1310
=== ep: 1311, time 29.33750009536743, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1311
=== ep: 1312, time 29.684746742248535, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1312
=== ep: 1313, time 39.51375651359558, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1313
=== ep: 1314, time 30.788102865219116, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1314
=== ep: 1315, time 27.275463342666626, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1315
=== ep: 1316, time 34.738330602645874, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1316
=== ep: 1317, time 30.045539379119873, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1317
=== ep: 1318, time 35.49123573303223, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1318
=== ep: 1319, time 189.147123336792, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1319
=== ep: 1320, time 39.357186794281006, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1320
=== ep: 1321, time 32.28486084938049, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1321
=== ep: 1322, time 29.026208639144897, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1322
goal_identified
=== ep: 1323, time 33.689438581466675, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1323
goal_identified
=== ep: 1324, time 29.960238695144653, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1324
=== ep: 1325, time 28.76026725769043, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1325
goal_identified
=== ep: 1326, time 29.846420288085938, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1218
=== ep: 1327, time 34.1642370223999, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1327
=== ep: 1328, time 30.774299144744873, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1328
=== ep: 1329, time 170.71281814575195, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1329
=== ep: 1330, time 33.866387367248535, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1330
=== ep: 1331, time 38.521148443222046, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1331
goal_identified
=== ep: 1332, time 37.00752329826355, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1232
=== ep: 1333, time 37.57024598121643, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1333
=== ep: 1334, time 28.966437101364136, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1334
=== ep: 1335, time 28.903985261917114, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1335
=== ep: 1336, time 32.139105796813965, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1336
=== ep: 1337, time 30.773799657821655, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1337
=== ep: 1338, time 30.001882076263428, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1338
=== ep: 1339, time 204.92918181419373, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1339
=== ep: 1340, time 29.601444244384766, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1340
goal_identified
=== ep: 1341, time 34.98742890357971, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1341
goal_identified
=== ep: 1342, time 32.94520926475525, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1342
=== ep: 1343, time 30.434592247009277, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1343
=== ep: 1344, time 34.9863817691803, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1344
goal_identified
=== ep: 1345, time 42.61327862739563, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1254
=== ep: 1346, time 35.80505704879761, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1346
goal_identified
=== ep: 1347, time 29.063040256500244, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1347
=== ep: 1348, time 35.58755111694336, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1348
=== ep: 1349, time 191.95932340621948, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1349
=== ep: 1350, time 32.45469617843628, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1350
=== ep: 1351, time 34.64260768890381, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1351
goal_identified
=== ep: 1352, time 30.378363609313965, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1352
=== ep: 1353, time 38.87870478630066, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1353
=== ep: 1354, time 33.07595729827881, eps 0.001, sum reward: 0, score_diff -6, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1354
=== ep: 1355, time 30.237412929534912, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1355
=== ep: 1356, time 41.92592406272888, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1356
=== ep: 1357, time 30.901392698287964, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1357
=== ep: 1358, time 30.33634901046753, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1358
=== ep: 1359, time 222.44990921020508, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1359
=== ep: 1360, time 34.39980459213257, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1360
=== ep: 1361, time 42.769646406173706, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1361
=== ep: 1362, time 39.24709630012512, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1362
=== ep: 1363, time 40.05009341239929, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1363
=== ep: 1364, time 30.021417379379272, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1364
=== ep: 1365, time 40.7269766330719, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1365
=== ep: 1366, time 31.321213006973267, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1366
=== ep: 1367, time 33.17620873451233, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1367
=== ep: 1368, time 30.66185164451599, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1368
=== ep: 1369, time 189.14924788475037, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1369
=== ep: 1370, time 34.94683003425598, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1370
=== ep: 1371, time 41.290766954422, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1371
=== ep: 1372, time 31.18822145462036, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1372
=== ep: 1373, time 39.290929079055786, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1373
goal_identified
=== ep: 1374, time 29.860223054885864, eps 0.001, sum reward: 1, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1374
=== ep: 1375, time 41.66473317146301, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1375
=== ep: 1376, time 35.87540674209595, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1376
=== ep: 1377, time 33.7641863822937, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1377
=== ep: 1378, time 30.70958113670349, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1378
=== ep: 1379, time 191.79528665542603, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1379
=== ep: 1380, time 46.83956027030945, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1380
=== ep: 1381, time 29.078137636184692, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1381
=== ep: 1382, time 38.594263792037964, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1382
=== ep: 1383, time 31.474414110183716, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1383
=== ep: 1384, time 30.09060502052307, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1384
=== ep: 1385, time 36.77129364013672, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1385
=== ep: 1386, time 37.12653374671936, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1386
=== ep: 1387, time 39.895395040512085, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1387
=== ep: 1388, time 36.751678228378296, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1388
goal_identified
=== ep: 1389, time 192.58486151695251, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1389
=== ep: 1390, time 30.98481512069702, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1390
=== ep: 1391, time 32.57880258560181, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1391
=== ep: 1392, time 45.609405755996704, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1392
=== ep: 1393, time 39.5031213760376, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1393
=== ep: 1394, time 44.01367378234863, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1394
=== ep: 1395, time 43.487526655197144, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1395
=== ep: 1396, time 37.771565198898315, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1396
=== ep: 1397, time 43.09607195854187, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1397
=== ep: 1398, time 38.58008861541748, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1398
=== ep: 1399, time 209.25037050247192, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1399
=== ep: 1400, time 39.01251006126404, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1400
=== ep: 1401, time 32.41626238822937, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1401
=== ep: 1402, time 33.82244801521301, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1402
=== ep: 1403, time 50.19680953025818, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1403
=== ep: 1404, time 30.681352376937866, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1404
goal_identified
=== ep: 1405, time 30.501124382019043, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1405
=== ep: 1406, time 41.03091907501221, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1406
=== ep: 1407, time 33.464953660964966, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1407
=== ep: 1408, time 41.714271068573, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1408
=== ep: 1409, time 221.15277814865112, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1409
=== ep: 1410, time 29.560248851776123, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1410
=== ep: 1411, time 37.815688133239746, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1411
=== ep: 1412, time 37.758811950683594, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1412
=== ep: 1413, time 31.32934331893921, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1413
=== ep: 1414, time 31.700448513031006, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1414
=== ep: 1415, time 29.654089212417603, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1415
=== ep: 1416, time 36.20292258262634, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1416
=== ep: 1417, time 31.82375407218933, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1417
=== ep: 1418, time 48.26972031593323, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1418
=== ep: 1419, time 203.53909373283386, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1419
=== ep: 1420, time 31.681134462356567, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1420
=== ep: 1421, time 29.35331416130066, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1421
=== ep: 1422, time 40.94084286689758, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1422
=== ep: 1423, time 39.98525834083557, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1423
=== ep: 1424, time 31.49421525001526, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1424
=== ep: 1425, time 41.27404594421387, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1425
=== ep: 1426, time 42.31977987289429, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1426
=== ep: 1427, time 30.763458728790283, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1427
=== ep: 1428, time 39.338303089141846, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1428
=== ep: 1429, time 202.94464254379272, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1429
=== ep: 1430, time 30.503349542617798, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1430
=== ep: 1431, time 33.79851460456848, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1431
=== ep: 1432, time 40.29352593421936, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1432
=== ep: 1433, time 31.260498523712158, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1433
=== ep: 1434, time 44.16086196899414, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1434
=== ep: 1435, time 41.600102186203, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1435
goal_identified
=== ep: 1436, time 32.67393255233765, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1436
=== ep: 1437, time 30.890138864517212, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1437
=== ep: 1438, time 34.22152066230774, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1438
=== ep: 1439, time 217.03844952583313, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1439
goal_identified
=== ep: 1440, time 35.6532301902771, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1440
=== ep: 1441, time 39.880621910095215, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1441
=== ep: 1442, time 36.25550556182861, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1442
=== ep: 1443, time 33.124836921691895, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1443
=== ep: 1444, time 34.65620160102844, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1444
goal_identified
=== ep: 1445, time 28.576369285583496, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1272
=== ep: 1446, time 35.46601390838623, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1446
=== ep: 1447, time 43.45086073875427, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1447
=== ep: 1448, time 30.50464630126953, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1448
=== ep: 1449, time 188.8617103099823, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1449
=== ep: 1450, time 41.231058835983276, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1450
=== ep: 1451, time 37.42301344871521, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1451
=== ep: 1452, time 39.58250069618225, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1452
=== ep: 1453, time 31.497606992721558, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1453
=== ep: 1454, time 44.09505009651184, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1454
=== ep: 1455, time 31.362406015396118, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1455
=== ep: 1456, time 38.69355654716492, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1456
goal_identified
=== ep: 1457, time 31.182098150253296, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1457
=== ep: 1458, time 35.79937696456909, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1458
=== ep: 1459, time 223.19179677963257, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1459
=== ep: 1460, time 36.70774531364441, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1460
=== ep: 1461, time 39.53094792366028, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1461
goal_identified
=== ep: 1462, time 30.31510615348816, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1462
=== ep: 1463, time 34.611074924468994, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1463
=== ep: 1464, time 32.85946202278137, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1464
=== ep: 1465, time 33.49547719955444, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1465
=== ep: 1466, time 28.508142232894897, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1466
=== ep: 1467, time 35.16647219657898, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1467
=== ep: 1468, time 34.01073384284973, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1468
=== ep: 1469, time 183.5644817352295, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1469
=== ep: 1470, time 33.25835990905762, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1470
=== ep: 1471, time 33.97476005554199, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1471
goal_identified
=== ep: 1472, time 30.581804275512695, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1472
=== ep: 1473, time 30.193340063095093, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1473
=== ep: 1474, time 43.12826895713806, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1474
=== ep: 1475, time 30.27168345451355, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1475
=== ep: 1476, time 35.955554485321045, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1476
=== ep: 1477, time 42.86690807342529, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1477
=== ep: 1478, time 34.36402893066406, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1478
=== ep: 1479, time 196.57241201400757, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1479
=== ep: 1480, time 41.9189670085907, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1480
=== ep: 1481, time 38.400192975997925, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1481
goal_identified
=== ep: 1482, time 29.300585985183716, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1303
=== ep: 1483, time 46.72736835479736, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1483
=== ep: 1484, time 30.908730030059814, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1484
goal_identified
=== ep: 1485, time 33.22140288352966, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1485
=== ep: 1486, time 33.37399101257324, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1486
=== ep: 1487, time 29.769611597061157, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1487
=== ep: 1488, time 36.381141662597656, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1488
=== ep: 1489, time 201.76095843315125, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1489
=== ep: 1490, time 42.12779784202576, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1490
=== ep: 1491, time 32.40892148017883, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1491
=== ep: 1492, time 35.28443694114685, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1492
goal_identified
=== ep: 1493, time 33.949060916900635, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1493
=== ep: 1494, time 30.64912748336792, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1494
=== ep: 1495, time 34.35181450843811, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1495
=== ep: 1496, time 29.466010332107544, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1496
=== ep: 1497, time 38.915358543395996, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1497
=== ep: 1498, time 31.41140079498291, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1498
=== ep: 1499, time 191.62598991394043, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1499
=== ep: 1500, time 38.114397287368774, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1500
goal_identified
=== ep: 1501, time 36.642900705337524, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1501
=== ep: 1502, time 31.47578501701355, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1502
=== ep: 1503, time 42.321592807769775, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1503
=== ep: 1504, time 36.464195013046265, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1504
goal_identified
=== ep: 1505, time 39.3940007686615, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1505
=== ep: 1506, time 34.89149785041809, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1506
goal_identified
=== ep: 1507, time 29.32319474220276, eps 0.001, sum reward: 1, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1507
=== ep: 1508, time 43.04241347312927, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1508
=== ep: 1509, time 189.82490825653076, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1509
=== ep: 1510, time 29.523930311203003, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1510
=== ep: 1511, time 36.588202476501465, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1511
=== ep: 1512, time 34.43071508407593, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1512
=== ep: 1513, time 30.041179895401, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1513
=== ep: 1514, time 33.25235962867737, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1514
=== ep: 1515, time 38.20454239845276, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1515
=== ep: 1516, time 32.093026876449585, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1516
=== ep: 1517, time 33.47170662879944, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1517
=== ep: 1518, time 47.669198989868164, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1518
=== ep: 1519, time 213.8128147125244, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1519
=== ep: 1520, time 30.044252634048462, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1520
=== ep: 1521, time 29.546669721603394, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1521
=== ep: 1522, time 36.809454679489136, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1522
=== ep: 1523, time 38.51396942138672, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1523
=== ep: 1524, time 29.464414834976196, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1524
=== ep: 1525, time 35.61411094665527, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1525
=== ep: 1526, time 57.232450008392334, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1526
goal_identified
=== ep: 1527, time 35.30141305923462, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1326
=== ep: 1528, time 41.80752635002136, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1528
goal_identified
=== ep: 1529, time 177.2122414112091, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1529
=== ep: 1530, time 46.158724308013916, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1530
=== ep: 1531, time 30.314388036727905, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1531
=== ep: 1532, time 43.492815017700195, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1532
=== ep: 1533, time 41.5549635887146, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1533
=== ep: 1534, time 31.850764989852905, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1534
=== ep: 1535, time 37.68379330635071, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1535
=== ep: 1536, time 30.258161544799805, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1536
=== ep: 1537, time 35.09379172325134, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1537
=== ep: 1538, time 31.346415281295776, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1538
=== ep: 1539, time 191.1692132949829, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1539
=== ep: 1540, time 46.81322908401489, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1540
=== ep: 1541, time 38.55127787590027, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1541
=== ep: 1542, time 41.61787390708923, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1542
=== ep: 1543, time 39.05886888504028, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1543
=== ep: 1544, time 30.720287561416626, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1544
=== ep: 1545, time 42.0326886177063, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1545
=== ep: 1546, time 49.71870422363281, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1546
goal_identified
goal_identified
=== ep: 1547, time 28.149327754974365, eps 0.001, sum reward: 2, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1332
=== ep: 1548, time 36.33170747756958, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1548
=== ep: 1549, time 175.90021181106567, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1549
goal_identified
=== ep: 1550, time 30.778929233551025, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1550
=== ep: 1551, time 40.766133308410645, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1551
=== ep: 1552, time 30.832783937454224, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1552
=== ep: 1553, time 35.209885120391846, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1553
=== ep: 1554, time 41.73509955406189, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1554
=== ep: 1555, time 37.23653721809387, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1555
=== ep: 1556, time 36.92080307006836, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1556
=== ep: 1557, time 36.18861222267151, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1557
=== ep: 1558, time 31.609359979629517, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1558
=== ep: 1559, time 196.23894786834717, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1559
=== ep: 1560, time 42.663479804992676, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1560
=== ep: 1561, time 32.832955837249756, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1561
=== ep: 1562, time 42.52556872367859, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1562
=== ep: 1563, time 30.39388132095337, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1563
=== ep: 1564, time 29.74827766418457, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1564
=== ep: 1565, time 32.982508182525635, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1565
=== ep: 1566, time 41.01010775566101, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1566
=== ep: 1567, time 37.257585763931274, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1567
=== ep: 1568, time 38.644951581954956, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1568
=== ep: 1569, time 185.9481918811798, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1569
=== ep: 1570, time 38.81025743484497, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1570
=== ep: 1571, time 29.088541746139526, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1571
=== ep: 1572, time 29.933147192001343, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1572
=== ep: 1573, time 36.63461089134216, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1573
=== ep: 1574, time 35.67979860305786, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1574
=== ep: 1575, time 45.446754932403564, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1575
=== ep: 1576, time 38.02345323562622, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1576
=== ep: 1577, time 32.98133826255798, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1577
=== ep: 1578, time 35.65325450897217, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1578
=== ep: 1579, time 173.78727769851685, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1579
=== ep: 1580, time 42.24311566352844, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1580
=== ep: 1581, time 34.90034508705139, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1581
=== ep: 1582, time 38.447691202163696, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1582
=== ep: 1583, time 45.01473140716553, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1583
goal_identified
=== ep: 1584, time 32.22236490249634, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1345
=== ep: 1585, time 36.17761421203613, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1585
=== ep: 1586, time 31.290306568145752, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1586
=== ep: 1587, time 30.2931911945343, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1587
=== ep: 1588, time 31.07231330871582, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1588
=== ep: 1589, time 191.4089057445526, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1589
=== ep: 1590, time 32.26347732543945, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1590
=== ep: 1591, time 29.895212650299072, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1591
goal_identified
=== ep: 1592, time 33.173644065856934, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1592
=== ep: 1593, time 36.442790031433105, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1593
=== ep: 1594, time 31.008245944976807, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1594
=== ep: 1595, time 29.5293025970459, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1595
=== ep: 1596, time 34.1041796207428, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1596
=== ep: 1597, time 33.57023096084595, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1597
=== ep: 1598, time 35.75533843040466, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1598
=== ep: 1599, time 155.53407549858093, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1599
goal_identified
=== ep: 1600, time 29.016894578933716, eps 0.001, sum reward: 1, score_diff 1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1445
=== ep: 1601, time 30.280222415924072, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1601
=== ep: 1602, time 34.7729709148407, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1602
=== ep: 1603, time 31.02579402923584, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1603
goal_identified
=== ep: 1604, time 32.141013860702515, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1604
=== ep: 1605, time 31.715964317321777, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1605
=== ep: 1606, time 29.985153198242188, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1606
=== ep: 1607, time 32.08192420005798, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1607
=== ep: 1608, time 34.76395630836487, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1608
=== ep: 1609, time 134.06617736816406, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1609
=== ep: 1610, time 32.294283390045166, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1610
=== ep: 1611, time 35.54715180397034, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1611
=== ep: 1612, time 33.09933543205261, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1612
=== ep: 1613, time 43.63449573516846, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1613
=== ep: 1614, time 35.28491759300232, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1614
=== ep: 1615, time 30.54452633857727, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1615
=== ep: 1616, time 36.65815806388855, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1616
=== ep: 1617, time 41.010884523391724, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1617
goal_identified
=== ep: 1618, time 31.03968644142151, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1618
=== ep: 1619, time 156.52542328834534, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1619
goal_identified
=== ep: 1620, time 33.53978729248047, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1620
=== ep: 1621, time 30.184165477752686, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1621
=== ep: 1622, time 43.598543643951416, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1622
=== ep: 1623, time 43.40713548660278, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1623
=== ep: 1624, time 34.20810532569885, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1624
goal_identified
=== ep: 1625, time 32.07857346534729, eps 0.001, sum reward: 1, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1625
=== ep: 1626, time 35.30896353721619, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1626
=== ep: 1627, time 29.62679362297058, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1627
=== ep: 1628, time 29.172769784927368, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1628
=== ep: 1629, time 140.4642207622528, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1629
=== ep: 1630, time 36.57767844200134, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1630
=== ep: 1631, time 31.705074787139893, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1631
=== ep: 1632, time 32.337236642837524, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1632
=== ep: 1633, time 33.150558948516846, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1633
=== ep: 1634, time 30.75351119041443, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1634
=== ep: 1635, time 31.29984998703003, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1635
=== ep: 1636, time 32.594768047332764, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1636
=== ep: 1637, time 46.956780433654785, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1637
=== ep: 1638, time 48.321410179138184, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1638
=== ep: 1639, time 164.31546449661255, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1639
=== ep: 1640, time 28.844968795776367, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1640
=== ep: 1641, time 41.90713977813721, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1641
=== ep: 1642, time 35.969781160354614, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1642
=== ep: 1643, time 33.153414726257324, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1643
=== ep: 1644, time 43.73050498962402, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1644
=== ep: 1645, time 35.43499541282654, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1645
=== ep: 1646, time 33.877880573272705, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1646
=== ep: 1647, time 31.059930562973022, eps 0.001, sum reward: 0, score_diff -5, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1647
=== ep: 1648, time 43.68099045753479, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1648
=== ep: 1649, time 136.97480845451355, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1649
=== ep: 1650, time 38.91938233375549, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1650
=== ep: 1651, time 34.05978560447693, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1651
=== ep: 1652, time 40.161887407302856, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1652
=== ep: 1653, time 29.902766704559326, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1653
=== ep: 1654, time 37.69382977485657, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1654
=== ep: 1655, time 38.383755922317505, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1655
=== ep: 1656, time 32.87530326843262, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1656
=== ep: 1657, time 35.93845582008362, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1657
=== ep: 1658, time 34.87954330444336, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1658
=== ep: 1659, time 146.77176713943481, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1659
=== ep: 1660, time 29.819468021392822, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1660
goal_identified
=== ep: 1661, time 40.13298559188843, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1661
=== ep: 1662, time 30.218825340270996, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1662
=== ep: 1663, time 30.045253038406372, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1663
=== ep: 1664, time 44.497397661209106, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1664
=== ep: 1665, time 41.303701400756836, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1665
=== ep: 1666, time 39.99198865890503, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1666
=== ep: 1667, time 35.146169900894165, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1667
=== ep: 1668, time 29.977925777435303, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1668
=== ep: 1669, time 145.1009223461151, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1669
=== ep: 1670, time 39.48782706260681, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1670
=== ep: 1671, time 37.88706922531128, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1671
=== ep: 1672, time 30.96931767463684, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1672
=== ep: 1673, time 31.216843605041504, eps 0.001, sum reward: 0, score_diff -3, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1673
=== ep: 1674, time 30.85316014289856, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1674
=== ep: 1675, time 28.9056613445282, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1675
=== ep: 1676, time 49.395057916641235, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1676
=== ep: 1677, time 44.391361236572266, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1677
goal_identified
=== ep: 1678, time 32.11622452735901, eps 0.001, sum reward: 1, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1678
=== ep: 1679, time 132.85575914382935, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1679
=== ep: 1680, time 44.05950951576233, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1680
=== ep: 1681, time 29.48931050300598, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1681
=== ep: 1682, time 37.57669639587402, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1682
=== ep: 1683, time 32.98969316482544, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1683
=== ep: 1684, time 39.1115460395813, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1684
=== ep: 1685, time 43.31449270248413, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1685
=== ep: 1686, time 29.441887140274048, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1686
=== ep: 1687, time 30.03594398498535, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1687
=== ep: 1688, time 33.864503622055054, eps 0.001, sum reward: 0, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1688
=== ep: 1689, time 191.75026392936707, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1689
=== ep: 1690, time 40.66351652145386, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1690
=== ep: 1691, time 31.39842414855957, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1691
=== ep: 1692, time 31.29766583442688, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1692
=== ep: 1693, time 42.63285493850708, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1693
=== ep: 1694, time 31.00890326499939, eps 0.001, sum reward: 0, score_diff -4, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1694
=== ep: 1695, time 40.69507932662964, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1695
=== ep: 1696, time 32.843878746032715, eps 0.001, sum reward: 0, score_diff -2, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1696
=== ep: 1697, time 39.617995738983154, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1697
goal_identified
=== ep: 1698, time 31.668153285980225, eps 0.001, sum reward: 1, score_diff -1, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1698
=== ep: 1699, time 142.25820326805115, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 10 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1699
=== ep: 1700, time 41.93851399421692, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1700
=== ep: 1701, time 36.090839862823486, eps 0.001, sum reward: 0, score_diff 0, tot learning steps 0 (total env steps 3001)
== current size of memory is eps 11 > 10.0 and we are deleting ep 1701
